{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import words\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cv = CountVectorizer(binary=True)\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"sa-emotions/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tes):\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_U_NAME = re.compile(\"@[\\S]+\")\n",
    "    REPLACE_DIGITS = re.compile(\"\\d\")\n",
    "    REPLACE_W_SPACE = re.compile(\"_\")\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_NO_SPACE, '')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_U_NAME,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_DIGITS,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_W_SPACE,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.lower()\n",
    "    return tes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = cleaner(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   i know  i was listenin to bad habit earlier a...\n",
       "1     sadness  layin n bed with a headache  ughhhhwaitin on y...\n",
       "2     sadness                      funeral ceremonygloomy friday\n",
       "3  enthusiasm                wants to hang out with friends soon\n",
       "4     neutral   we want to trade with someone who has houston..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df_train.loc[(df_train['sentiment'] == 'sadness') | (df_train['sentiment'] == 'happiness')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sadness</td>\n",
       "      <td>i should be sleep but im not thinking about an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sadness</td>\n",
       "      <td>charlene my love i miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sadness</td>\n",
       "      <td>im sorry  at least its friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                            content\n",
       "1   sadness  layin n bed with a headache  ughhhhwaitin on y...\n",
       "2   sadness                      funeral ceremonygloomy friday\n",
       "6   sadness  i should be sleep but im not thinking about an...\n",
       "8   sadness                        charlene my love i miss you\n",
       "9   sadness                      im sorry  at least its friday"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sadness      4828\n",
       "happiness    2986\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_bin['sentiment'].value_counts().plot('bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the above plot we can note that the classes are imbalanced, We'll be using SMOTE to rectify this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(train_data,test_data,train_labels,test_labels):\n",
    "    lsvc = LinearSVC(random_state=0, tol=1e-5)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    et = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "    xgb = XGBClassifier()\n",
    "    adb = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    models = {'LinearSVC':lsvc,'RandomForest':rf,'ExtraTrees':et,'XGBoost':xgb,'AdaBoost':adb}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(train_data, train_labels)\n",
    "        print(\"Accuracy for {} is {}\".format(model_name,accuracy_score(test_labels, model.predict(test_data))))\n",
    "        predicted = model.predict(test_data)\n",
    "        print(classification_report(test_labels,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(df_bin[\"content\"])\n",
    "count_vec = cv.transform(df_bin[\"content\"])\n",
    "# X_test_vec = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sadness      4828\n",
       "happiness    2986\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(count_vec, df_bin['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_SMOTE.value_counts().plot('bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_enc(values):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    return(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_labels = int_enc(np.array(df_bin['sentiment']))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_X_train, cv_X_test, cv_y_train, cv_y_test = train_test_split(count_vec, cv_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_search(train_features,y_train):\n",
    "    parameters = {'C': np.linspace(1, 0.0001, 100, 20)}\n",
    "    grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "    grid_search.fit(train_features, y_train)\n",
    "\n",
    "    print('best parameters: ', grid_search.best_params_)\n",
    "    print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 0.6162000000000001}\n",
      "best scrores:  0.7902578796561605\n"
     ]
    }
   ],
   "source": [
    "# param_search(cv_X_train, cv_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.192: 0.7758821248545948\n"
     ]
    }
   ],
   "source": [
    "#non-SMOTE\n",
    "c = 0.1920 #value from grid search\n",
    "lr = LogisticRegression(C=c)\n",
    "lr.fit(cv_X_train, cv_y_train)\n",
    "print (\"Accuracy for C=%s: %s\" \n",
    "   % (c, accuracy_score(cv_y_test, lr.predict(cv_X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.59      0.66       954\n",
      "           1       0.79      0.89      0.83      1625\n",
      "\n",
      "    accuracy                           0.78      2579\n",
      "   macro avg       0.77      0.74      0.75      2579\n",
      "weighted avg       0.77      0.78      0.77      2579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(cv_X_test)\n",
    "print(classification_report(cv_y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 0.8081}\n",
      "best scrores:  0.8097076040168736\n"
     ]
    }
   ],
   "source": [
    "# param_search(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.9798: 0.8107938500156887\n"
     ]
    }
   ],
   "source": [
    "c = 0.9798 #value from gridsearch\n",
    "lr = LogisticRegression(C=c)\n",
    "lr.fit(X_train, y_train)\n",
    "print (\"Accuracy for C=%s: %s\" \n",
    "   % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81      1541\n",
      "           1       0.84      0.78      0.81      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(X_test)\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7638619620007755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.68       954\n",
      "           1       0.81      0.82      0.81      1625\n",
      "\n",
      "    accuracy                           0.76      2579\n",
      "   macro avg       0.75      0.74      0.75      2579\n",
      "weighted avg       0.76      0.76      0.76      2579\n",
      "\n",
      "Accuracy for RandomForest is 0.7564947654129508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.49      0.60       954\n",
      "           1       0.75      0.92      0.83      1625\n",
      "\n",
      "    accuracy                           0.76      2579\n",
      "   macro avg       0.76      0.70      0.71      2579\n",
      "weighted avg       0.76      0.76      0.74      2579\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7681271810779372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.54      0.63       954\n",
      "           1       0.77      0.90      0.83      1625\n",
      "\n",
      "    accuracy                           0.77      2579\n",
      "   macro avg       0.77      0.72      0.73      2579\n",
      "weighted avg       0.77      0.77      0.76      2579\n",
      "\n",
      "Accuracy for XGBoost is 0.7258627374951532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.36      0.49       954\n",
      "           1       0.71      0.94      0.81      1625\n",
      "\n",
      "    accuracy                           0.73      2579\n",
      "   macro avg       0.75      0.65      0.65      2579\n",
      "weighted avg       0.74      0.73      0.69      2579\n",
      "\n",
      "Accuracy for AdaBoost is 0.759208995734781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62       954\n",
      "           1       0.77      0.89      0.82      1625\n",
      "\n",
      "    accuracy                           0.76      2579\n",
      "   macro avg       0.75      0.71      0.72      2579\n",
      "weighted avg       0.76      0.76      0.75      2579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(cv_X_train, cv_X_test, cv_y_train, cv_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7973015374960778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      1541\n",
      "           1       0.83      0.76      0.79      1646\n",
      "\n",
      "    accuracy                           0.80      3187\n",
      "   macro avg       0.80      0.80      0.80      3187\n",
      "weighted avg       0.80      0.80      0.80      3187\n",
      "\n",
      "Accuracy for RandomForest is 0.807028553498588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      1541\n",
      "           1       0.81      0.82      0.81      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n",
      "Accuracy for ExtraTrees is 0.8076561029181047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80      1541\n",
      "           1       0.80      0.84      0.82      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n",
      "Accuracy for XGBoost is 0.7769061813617822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.65      0.74      1541\n",
      "           1       0.73      0.89      0.81      1646\n",
      "\n",
      "    accuracy                           0.78      3187\n",
      "   macro avg       0.79      0.77      0.77      3187\n",
      "weighted avg       0.79      0.78      0.77      3187\n",
      "\n",
      "Accuracy for AdaBoost is 0.7944775651082523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80      1541\n",
      "           1       0.85      0.73      0.79      1646\n",
      "\n",
      "    accuracy                           0.79      3187\n",
      "   macro avg       0.80      0.80      0.79      3187\n",
      "weighted avg       0.80      0.79      0.79      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69616874\n",
      "Iteration 2, loss = 0.69445304\n",
      "Iteration 3, loss = 0.69314041\n",
      "Iteration 4, loss = 0.69212137\n",
      "Iteration 5, loss = 0.69135756\n",
      "Iteration 6, loss = 0.69067760\n",
      "Iteration 7, loss = 0.69005792\n",
      "Iteration 8, loss = 0.68947866\n",
      "Iteration 9, loss = 0.68891598\n",
      "Iteration 10, loss = 0.68834973\n",
      "Iteration 11, loss = 0.68781632\n",
      "Iteration 12, loss = 0.68721640\n",
      "Iteration 13, loss = 0.68664448\n",
      "Iteration 14, loss = 0.68604589\n",
      "Iteration 15, loss = 0.68542078\n",
      "Iteration 16, loss = 0.68478037\n",
      "Iteration 17, loss = 0.68407651\n",
      "Iteration 18, loss = 0.68334300\n",
      "Iteration 19, loss = 0.68258196\n",
      "Iteration 20, loss = 0.68179012\n",
      "Iteration 21, loss = 0.68093321\n",
      "Iteration 22, loss = 0.68004995\n",
      "Iteration 23, loss = 0.67907634\n",
      "Iteration 24, loss = 0.67807759\n",
      "Iteration 25, loss = 0.67699626\n",
      "Iteration 26, loss = 0.67587303\n",
      "Iteration 27, loss = 0.67467159\n",
      "Iteration 28, loss = 0.67339932\n",
      "Iteration 29, loss = 0.67201492\n",
      "Iteration 30, loss = 0.67057969\n",
      "Iteration 31, loss = 0.66902742\n",
      "Iteration 32, loss = 0.66738983\n",
      "Iteration 33, loss = 0.66565789\n",
      "Iteration 34, loss = 0.66381189\n",
      "Iteration 35, loss = 0.66183617\n",
      "Iteration 36, loss = 0.65975090\n",
      "Iteration 37, loss = 0.65754834\n",
      "Iteration 38, loss = 0.65521242\n",
      "Iteration 39, loss = 0.65273022\n",
      "Iteration 40, loss = 0.65009895\n",
      "Iteration 41, loss = 0.64732424\n",
      "Iteration 42, loss = 0.64438542\n",
      "Iteration 43, loss = 0.64128355\n",
      "Iteration 44, loss = 0.63799793\n",
      "Iteration 45, loss = 0.63453822\n",
      "Iteration 46, loss = 0.63098313\n",
      "Iteration 47, loss = 0.62724121\n",
      "Iteration 48, loss = 0.62335466\n",
      "Iteration 49, loss = 0.61927805\n",
      "Iteration 50, loss = 0.61513919\n",
      "Iteration 51, loss = 0.61079003\n",
      "Iteration 52, loss = 0.60638477\n",
      "Iteration 53, loss = 0.60171474\n",
      "Iteration 54, loss = 0.59695941\n",
      "Iteration 55, loss = 0.59209120\n",
      "Iteration 56, loss = 0.58705345\n",
      "Iteration 57, loss = 0.58195784\n",
      "Iteration 58, loss = 0.57675814\n",
      "Iteration 59, loss = 0.57141104\n",
      "Iteration 60, loss = 0.56598297\n",
      "Iteration 61, loss = 0.56074991\n",
      "Iteration 62, loss = 0.55505143\n",
      "Iteration 63, loss = 0.54948359\n",
      "Iteration 64, loss = 0.54381178\n",
      "Iteration 65, loss = 0.53805094\n",
      "Iteration 66, loss = 0.53238906\n",
      "Iteration 67, loss = 0.52659143\n",
      "Iteration 68, loss = 0.52078221\n",
      "Iteration 69, loss = 0.51492199\n",
      "Iteration 70, loss = 0.50909980\n",
      "Iteration 71, loss = 0.50322976\n",
      "Iteration 72, loss = 0.49727760\n",
      "Iteration 73, loss = 0.49146720\n",
      "Iteration 74, loss = 0.48564664\n",
      "Iteration 75, loss = 0.47981621\n",
      "Iteration 76, loss = 0.47406984\n",
      "Iteration 77, loss = 0.46828538\n",
      "Iteration 78, loss = 0.46263968\n",
      "Iteration 79, loss = 0.45697946\n",
      "Iteration 80, loss = 0.45142969\n",
      "Iteration 81, loss = 0.44585432\n",
      "Iteration 82, loss = 0.44037179\n",
      "Iteration 83, loss = 0.43511580\n",
      "Iteration 84, loss = 0.42952426\n",
      "Iteration 85, loss = 0.42422661\n",
      "Iteration 86, loss = 0.41901962\n",
      "Iteration 87, loss = 0.41374398\n",
      "Iteration 88, loss = 0.40873007\n",
      "Iteration 89, loss = 0.40366642\n",
      "Iteration 90, loss = 0.39857880\n",
      "Iteration 91, loss = 0.39370675\n",
      "Iteration 92, loss = 0.38879225\n",
      "Iteration 93, loss = 0.38402061\n",
      "Iteration 94, loss = 0.37918214\n",
      "Iteration 95, loss = 0.37446400\n",
      "Iteration 96, loss = 0.36975411\n",
      "Iteration 97, loss = 0.36512175\n",
      "Iteration 98, loss = 0.36074053\n",
      "Iteration 99, loss = 0.35600199\n",
      "Iteration 100, loss = 0.35150788\n",
      "Iteration 101, loss = 0.34737231\n",
      "Iteration 102, loss = 0.34263893\n",
      "Iteration 103, loss = 0.33806461\n",
      "Iteration 104, loss = 0.33375486\n",
      "Iteration 105, loss = 0.32933264\n",
      "Iteration 106, loss = 0.32487792\n",
      "Iteration 107, loss = 0.32057536\n",
      "Iteration 108, loss = 0.31617527\n",
      "Iteration 109, loss = 0.31185689\n",
      "Iteration 110, loss = 0.30784536\n",
      "Iteration 111, loss = 0.30321572\n",
      "Iteration 112, loss = 0.29900261\n",
      "Iteration 113, loss = 0.29488733\n",
      "Iteration 114, loss = 0.29053425\n",
      "Iteration 115, loss = 0.28622401\n",
      "Iteration 116, loss = 0.28201503\n",
      "Iteration 117, loss = 0.27816600\n",
      "Iteration 118, loss = 0.27377315\n",
      "Iteration 119, loss = 0.26963775\n",
      "Iteration 120, loss = 0.26549729\n",
      "Iteration 121, loss = 0.26143857\n",
      "Iteration 122, loss = 0.25741399\n",
      "Iteration 123, loss = 0.25306142\n",
      "Iteration 124, loss = 0.24907646\n",
      "Iteration 125, loss = 0.24508858\n",
      "Iteration 126, loss = 0.24113041\n",
      "Iteration 127, loss = 0.23702083\n",
      "Iteration 128, loss = 0.23313695\n",
      "Iteration 129, loss = 0.22922000\n",
      "Iteration 130, loss = 0.22534139\n",
      "Iteration 131, loss = 0.22153823\n",
      "Iteration 132, loss = 0.21743227\n",
      "Iteration 133, loss = 0.21360626\n",
      "Iteration 134, loss = 0.21014085\n",
      "Iteration 135, loss = 0.20614076\n",
      "Iteration 136, loss = 0.20241322\n",
      "Iteration 137, loss = 0.19887034\n",
      "Iteration 138, loss = 0.19536487\n",
      "Iteration 139, loss = 0.19143342\n",
      "Iteration 140, loss = 0.18777837\n",
      "Iteration 141, loss = 0.18447695\n",
      "Iteration 142, loss = 0.18060552\n",
      "Iteration 143, loss = 0.17713288\n",
      "Iteration 144, loss = 0.17371868\n",
      "Iteration 145, loss = 0.17037711\n",
      "Iteration 146, loss = 0.16694904\n",
      "Iteration 147, loss = 0.16364203\n",
      "Iteration 148, loss = 0.16030251\n",
      "Iteration 149, loss = 0.15700374\n",
      "Iteration 150, loss = 0.15369151\n",
      "Iteration 151, loss = 0.15072204\n",
      "Iteration 152, loss = 0.14790956\n",
      "Iteration 153, loss = 0.14458330\n",
      "Iteration 154, loss = 0.14147883\n",
      "Iteration 155, loss = 0.13863129\n",
      "Iteration 156, loss = 0.13577765\n",
      "Iteration 157, loss = 0.13290080\n",
      "Iteration 158, loss = 0.13012782\n",
      "Iteration 159, loss = 0.12727766\n",
      "Iteration 160, loss = 0.12472524\n",
      "Iteration 161, loss = 0.12201750\n",
      "Iteration 162, loss = 0.11937832\n",
      "Iteration 163, loss = 0.11696184\n",
      "Iteration 164, loss = 0.11433060\n",
      "Iteration 165, loss = 0.11179738\n",
      "Iteration 166, loss = 0.10952615\n",
      "Iteration 167, loss = 0.10751528\n",
      "Iteration 168, loss = 0.10485278\n",
      "Iteration 169, loss = 0.10269066\n",
      "Iteration 170, loss = 0.10039472\n",
      "Iteration 171, loss = 0.09842621\n",
      "Iteration 172, loss = 0.09632654\n",
      "Iteration 173, loss = 0.09431530\n",
      "Iteration 174, loss = 0.09249495\n",
      "Iteration 175, loss = 0.09055312\n",
      "Iteration 176, loss = 0.08875278\n",
      "Iteration 177, loss = 0.08673556\n",
      "Iteration 178, loss = 0.08486764\n",
      "Iteration 179, loss = 0.08307788\n",
      "Iteration 180, loss = 0.08130305\n",
      "Iteration 181, loss = 0.07972171\n",
      "Iteration 182, loss = 0.07807682\n",
      "Iteration 183, loss = 0.07630339\n",
      "Iteration 184, loss = 0.07476739\n",
      "Iteration 185, loss = 0.07325298\n",
      "Iteration 186, loss = 0.07179151\n",
      "Iteration 187, loss = 0.07020809\n",
      "Iteration 188, loss = 0.06885785\n",
      "Iteration 189, loss = 0.06732113\n",
      "Iteration 190, loss = 0.06591886\n",
      "Iteration 191, loss = 0.06451101\n",
      "Iteration 192, loss = 0.06329507\n",
      "Iteration 193, loss = 0.06205454\n",
      "Iteration 194, loss = 0.06073962\n",
      "Iteration 195, loss = 0.05957191\n",
      "Iteration 196, loss = 0.05829267\n",
      "Iteration 197, loss = 0.05729237\n",
      "Iteration 198, loss = 0.05600412\n",
      "Iteration 199, loss = 0.05497398\n",
      "Iteration 200, loss = 0.05387412\n",
      "Iteration 201, loss = 0.05278851\n",
      "Iteration 202, loss = 0.05153631\n",
      "Iteration 203, loss = 0.05048618\n",
      "Iteration 204, loss = 0.04936459\n",
      "Iteration 205, loss = 0.04854764\n",
      "Iteration 206, loss = 0.04745562\n",
      "Iteration 207, loss = 0.04663207\n",
      "Iteration 208, loss = 0.04569016\n",
      "Iteration 209, loss = 0.04474734\n",
      "Iteration 210, loss = 0.04392627\n",
      "Iteration 211, loss = 0.04316115\n",
      "Iteration 212, loss = 0.04226644\n",
      "Iteration 213, loss = 0.04171519\n",
      "Iteration 214, loss = 0.04090573\n",
      "Iteration 215, loss = 0.04019528\n",
      "Iteration 216, loss = 0.03947714\n",
      "Iteration 217, loss = 0.03876816\n",
      "Iteration 218, loss = 0.03805483\n",
      "Iteration 219, loss = 0.03741000\n",
      "Iteration 220, loss = 0.03680454\n",
      "Iteration 221, loss = 0.03613758\n",
      "Iteration 222, loss = 0.03550659\n",
      "Iteration 223, loss = 0.03497103\n",
      "Iteration 224, loss = 0.03434553\n",
      "Iteration 225, loss = 0.03389158\n",
      "Iteration 226, loss = 0.03332833\n",
      "Iteration 227, loss = 0.03279492\n",
      "Iteration 228, loss = 0.03229277\n",
      "Iteration 229, loss = 0.03179191\n",
      "Iteration 230, loss = 0.03129014\n",
      "Iteration 231, loss = 0.03084906\n",
      "Iteration 232, loss = 0.03048118\n",
      "Iteration 233, loss = 0.02989548\n",
      "Iteration 234, loss = 0.02949557\n",
      "Iteration 235, loss = 0.02901331\n",
      "Iteration 236, loss = 0.02871414\n",
      "Iteration 237, loss = 0.02834526\n",
      "Iteration 238, loss = 0.02793707\n",
      "Iteration 239, loss = 0.02748359\n",
      "Iteration 240, loss = 0.02707872\n",
      "Iteration 241, loss = 0.02673104\n",
      "Iteration 242, loss = 0.02649667\n",
      "Iteration 243, loss = 0.02597756\n",
      "Iteration 244, loss = 0.02569515\n",
      "Iteration 245, loss = 0.02537969\n",
      "Iteration 246, loss = 0.02503652\n",
      "Iteration 247, loss = 0.02465785\n",
      "Iteration 248, loss = 0.02447192\n",
      "Iteration 249, loss = 0.02414508\n",
      "Iteration 250, loss = 0.02384805\n",
      "Iteration 251, loss = 0.02363258\n",
      "Iteration 252, loss = 0.02325188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.02300509\n",
      "Iteration 254, loss = 0.02272089\n",
      "Iteration 255, loss = 0.02239488\n",
      "Iteration 256, loss = 0.02211424\n",
      "Iteration 257, loss = 0.02188193\n",
      "Iteration 258, loss = 0.02169211\n",
      "Iteration 259, loss = 0.02140890\n",
      "Iteration 260, loss = 0.02117262\n",
      "Iteration 261, loss = 0.02096357\n",
      "Iteration 262, loss = 0.02068740\n",
      "Iteration 263, loss = 0.02056109\n",
      "Iteration 264, loss = 0.02023277\n",
      "Iteration 265, loss = 0.02000897\n",
      "Iteration 266, loss = 0.01981411\n",
      "Iteration 267, loss = 0.01959989\n",
      "Iteration 268, loss = 0.01948134\n",
      "Iteration 269, loss = 0.01920499\n",
      "Iteration 270, loss = 0.01904659\n",
      "Iteration 271, loss = 0.01882977\n",
      "Iteration 272, loss = 0.01861657\n",
      "Iteration 273, loss = 0.01846359\n",
      "Iteration 274, loss = 0.01826818\n",
      "Iteration 275, loss = 0.01807335\n",
      "Iteration 276, loss = 0.01798504\n",
      "Iteration 277, loss = 0.01777617\n",
      "Iteration 278, loss = 0.01760350\n",
      "Iteration 279, loss = 0.01742056\n",
      "Iteration 280, loss = 0.01730010\n",
      "Iteration 281, loss = 0.01713430\n",
      "Iteration 282, loss = 0.01695957\n",
      "Iteration 283, loss = 0.01689264\n",
      "Iteration 284, loss = 0.01662405\n",
      "Iteration 285, loss = 0.01659577\n",
      "Iteration 286, loss = 0.01635461\n",
      "Iteration 287, loss = 0.01618553\n",
      "Iteration 288, loss = 0.01616762\n",
      "Iteration 289, loss = 0.01587672\n",
      "Iteration 290, loss = 0.01587855\n",
      "Iteration 291, loss = 0.01565206\n",
      "Iteration 292, loss = 0.01551951\n",
      "Iteration 293, loss = 0.01542838\n",
      "Iteration 294, loss = 0.01528469\n",
      "Iteration 295, loss = 0.01513213\n",
      "Iteration 296, loss = 0.01504311\n",
      "Iteration 297, loss = 0.01492711\n",
      "Iteration 298, loss = 0.01481723\n",
      "Iteration 299, loss = 0.01468011\n",
      "Iteration 300, loss = 0.01456301\n",
      "Iteration 301, loss = 0.01443900\n",
      "Iteration 302, loss = 0.01431419\n",
      "Iteration 303, loss = 0.01422170\n",
      "Iteration 304, loss = 0.01411464\n",
      "Iteration 305, loss = 0.01399590\n",
      "Iteration 306, loss = 0.01388756\n",
      "Iteration 307, loss = 0.01387944\n",
      "Iteration 308, loss = 0.01369958\n",
      "Iteration 309, loss = 0.01361524\n",
      "Iteration 310, loss = 0.01351755\n",
      "Iteration 311, loss = 0.01339607\n",
      "Iteration 312, loss = 0.01332438\n",
      "Iteration 313, loss = 0.01319783\n",
      "Iteration 314, loss = 0.01309884\n",
      "Iteration 315, loss = 0.01300085\n",
      "Iteration 316, loss = 0.01290207\n",
      "Iteration 317, loss = 0.01287005\n",
      "Iteration 318, loss = 0.01275690\n",
      "Iteration 319, loss = 0.01266599\n",
      "Iteration 320, loss = 0.01255636\n",
      "Iteration 321, loss = 0.01258072\n",
      "Iteration 322, loss = 0.01246756\n",
      "Iteration 323, loss = 0.01232231\n",
      "Iteration 324, loss = 0.01222781\n",
      "Iteration 325, loss = 0.01222307\n",
      "Iteration 326, loss = 0.01210128\n",
      "Iteration 327, loss = 0.01206615\n",
      "Iteration 328, loss = 0.01198664\n",
      "Iteration 329, loss = 0.01184751\n",
      "Iteration 330, loss = 0.01179283\n",
      "Iteration 331, loss = 0.01176751\n",
      "Iteration 332, loss = 0.01168474\n",
      "Iteration 333, loss = 0.01161310\n",
      "Iteration 334, loss = 0.01150927\n",
      "Iteration 335, loss = 0.01153424\n",
      "Iteration 336, loss = 0.01135352\n",
      "Iteration 337, loss = 0.01137453\n",
      "Iteration 338, loss = 0.01124484\n",
      "Iteration 339, loss = 0.01126341\n",
      "Iteration 340, loss = 0.01112164\n",
      "Iteration 341, loss = 0.01104802\n",
      "Iteration 342, loss = 0.01102567\n",
      "Iteration 343, loss = 0.01098075\n",
      "Iteration 344, loss = 0.01093885\n",
      "Iteration 345, loss = 0.01088761\n",
      "Iteration 346, loss = 0.01081559\n",
      "Iteration 347, loss = 0.01067845\n",
      "Iteration 348, loss = 0.01065572\n",
      "Iteration 349, loss = 0.01066916\n",
      "Iteration 350, loss = 0.01057035\n",
      "Iteration 351, loss = 0.01050206\n",
      "Iteration 352, loss = 0.01041976\n",
      "Iteration 353, loss = 0.01033465\n",
      "Iteration 354, loss = 0.01029830\n",
      "Iteration 355, loss = 0.01027571\n",
      "Iteration 356, loss = 0.01018230\n",
      "Iteration 357, loss = 0.01015524\n",
      "Iteration 358, loss = 0.01014264\n",
      "Iteration 359, loss = 0.01005894\n",
      "Iteration 360, loss = 0.00999138\n",
      "Iteration 361, loss = 0.00995338\n",
      "Iteration 362, loss = 0.00990189\n",
      "Iteration 363, loss = 0.00989488\n",
      "Iteration 364, loss = 0.00983663\n",
      "Iteration 365, loss = 0.00974930\n",
      "Iteration 366, loss = 0.00975985\n",
      "Iteration 367, loss = 0.00977425\n",
      "Iteration 368, loss = 0.00962415\n",
      "Iteration 369, loss = 0.00958332\n",
      "Iteration 370, loss = 0.00972706\n",
      "Iteration 371, loss = 0.00951368\n",
      "Iteration 372, loss = 0.00947278\n",
      "Iteration 373, loss = 0.00944499\n",
      "Iteration 374, loss = 0.00936761\n",
      "Iteration 375, loss = 0.00949460\n",
      "Iteration 376, loss = 0.00931806\n",
      "Iteration 377, loss = 0.00922337\n",
      "Iteration 378, loss = 0.00920109\n",
      "Iteration 379, loss = 0.00914468\n",
      "Iteration 380, loss = 0.00910619\n",
      "Iteration 381, loss = 0.00907198\n",
      "Iteration 382, loss = 0.00905449\n",
      "Iteration 383, loss = 0.00898779\n",
      "Iteration 384, loss = 0.00905141\n",
      "Iteration 385, loss = 0.00891235\n",
      "Iteration 386, loss = 0.00892055\n",
      "Iteration 387, loss = 0.00881895\n",
      "Iteration 388, loss = 0.00887939\n",
      "Iteration 389, loss = 0.00883167\n",
      "Iteration 390, loss = 0.00875552\n",
      "Iteration 391, loss = 0.00874168\n",
      "Iteration 392, loss = 0.00866427\n",
      "Iteration 393, loss = 0.00861292\n",
      "Iteration 394, loss = 0.00860479\n",
      "Iteration 395, loss = 0.00860403\n",
      "Iteration 396, loss = 0.00857122\n",
      "Iteration 397, loss = 0.00858030\n",
      "Iteration 398, loss = 0.00853238\n",
      "Iteration 399, loss = 0.00841302\n",
      "Iteration 400, loss = 0.00849631\n",
      "Iteration 401, loss = 0.00835788\n",
      "Iteration 402, loss = 0.00833863\n",
      "Iteration 403, loss = 0.00828057\n",
      "Iteration 404, loss = 0.00825173\n",
      "Iteration 405, loss = 0.00828823\n",
      "Iteration 406, loss = 0.00819818\n",
      "Iteration 407, loss = 0.00820240\n",
      "Iteration 408, loss = 0.00817965\n",
      "Iteration 409, loss = 0.00812366\n",
      "Iteration 410, loss = 0.00812512\n",
      "Iteration 411, loss = 0.00805457\n",
      "Iteration 412, loss = 0.00807619\n",
      "Iteration 413, loss = 0.00802481\n",
      "Iteration 414, loss = 0.00799774\n",
      "Iteration 415, loss = 0.00811171\n",
      "Iteration 416, loss = 0.00796471\n",
      "Iteration 417, loss = 0.00795394\n",
      "Iteration 418, loss = 0.00787087\n",
      "Iteration 419, loss = 0.00788395\n",
      "Iteration 420, loss = 0.00786119\n",
      "Iteration 421, loss = 0.00785045\n",
      "Iteration 422, loss = 0.00778737\n",
      "Iteration 423, loss = 0.00782892\n",
      "Iteration 424, loss = 0.00776571\n",
      "Iteration 425, loss = 0.00767454\n",
      "Iteration 426, loss = 0.00767084\n",
      "Iteration 427, loss = 0.00765070\n",
      "Iteration 428, loss = 0.00767899\n",
      "Iteration 429, loss = 0.00761007\n",
      "Iteration 430, loss = 0.00761020\n",
      "Iteration 431, loss = 0.00758601\n",
      "Iteration 432, loss = 0.00753629\n",
      "Iteration 433, loss = 0.00756261\n",
      "Iteration 434, loss = 0.00748848\n",
      "Iteration 435, loss = 0.00753213\n",
      "Iteration 436, loss = 0.00742054\n",
      "Iteration 437, loss = 0.00739456\n",
      "Iteration 438, loss = 0.00740304\n",
      "Iteration 439, loss = 0.00739645\n",
      "Iteration 440, loss = 0.00733122\n",
      "Iteration 441, loss = 0.00730276\n",
      "Iteration 442, loss = 0.00733352\n",
      "Iteration 443, loss = 0.00733436\n",
      "Iteration 444, loss = 0.00720253\n",
      "Iteration 445, loss = 0.00722111\n",
      "Iteration 446, loss = 0.00725447\n",
      "Iteration 447, loss = 0.00716152\n",
      "Iteration 448, loss = 0.00722536\n",
      "Iteration 449, loss = 0.00720188\n",
      "Iteration 450, loss = 0.00717374\n",
      "Iteration 451, loss = 0.00712935\n",
      "Iteration 452, loss = 0.00712591\n",
      "Iteration 453, loss = 0.00712353\n",
      "Iteration 454, loss = 0.00705730\n",
      "Iteration 455, loss = 0.00713121\n",
      "Iteration 456, loss = 0.00700294\n",
      "Iteration 457, loss = 0.00711402\n",
      "Iteration 458, loss = 0.00700097\n",
      "Iteration 459, loss = 0.00694279\n",
      "Iteration 460, loss = 0.00691487\n",
      "Iteration 461, loss = 0.00700670\n",
      "Iteration 462, loss = 0.00711612\n",
      "Iteration 463, loss = 0.00697979\n",
      "Iteration 464, loss = 0.00693225\n",
      "Iteration 465, loss = 0.00686522\n",
      "Iteration 466, loss = 0.00682349\n",
      "Iteration 467, loss = 0.00682137\n",
      "Iteration 468, loss = 0.00677694\n",
      "Iteration 469, loss = 0.00682144\n",
      "Iteration 470, loss = 0.00681227\n",
      "Iteration 471, loss = 0.00679077\n",
      "Iteration 472, loss = 0.00670573\n",
      "Iteration 473, loss = 0.00671284\n",
      "Iteration 474, loss = 0.00667003\n",
      "Iteration 475, loss = 0.00668310\n",
      "Iteration 476, loss = 0.00664484\n",
      "Iteration 477, loss = 0.00664116\n",
      "Iteration 478, loss = 0.00665686\n",
      "Iteration 479, loss = 0.00658931\n",
      "Iteration 480, loss = 0.00663480\n",
      "Iteration 481, loss = 0.00657246\n",
      "Iteration 482, loss = 0.00663908\n",
      "Iteration 483, loss = 0.00655481\n",
      "Iteration 484, loss = 0.00655169\n",
      "Iteration 485, loss = 0.00654852\n",
      "Iteration 486, loss = 0.00656018\n",
      "Iteration 487, loss = 0.00644299\n",
      "Iteration 488, loss = 0.00653471\n",
      "Iteration 489, loss = 0.00646768\n",
      "Iteration 490, loss = 0.00646446\n",
      "Iteration 491, loss = 0.00642800\n",
      "Iteration 492, loss = 0.00639394\n",
      "Iteration 493, loss = 0.00643748\n",
      "Iteration 494, loss = 0.00642171\n",
      "Iteration 495, loss = 0.00641623\n",
      "Iteration 496, loss = 0.00642227\n",
      "Iteration 497, loss = 0.00632643\n",
      "Iteration 498, loss = 0.00635921\n",
      "Iteration 499, loss = 0.00632648\n",
      "Iteration 500, loss = 0.00633421\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-86102f9b4ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81      1541\n",
      "           1       0.83      0.80      0.81      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 7814 contents is represented by 1883 features (TF-IDF score of unigrams and bigrams)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "# We transform each content into a vector\n",
    "features = tfidf.fit_transform(df_bin.content).toarray()\n",
    "\n",
    "labels = df_bin.sentiment\n",
    "\n",
    "print(\"Each of the %d contents is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(features, tfidf_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7561070182241179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.66       954\n",
      "           1       0.80      0.82      0.81      1625\n",
      "\n",
      "    accuracy                           0.76      2579\n",
      "   macro avg       0.74      0.73      0.74      2579\n",
      "weighted avg       0.75      0.76      0.75      2579\n",
      "\n",
      "Accuracy for RandomForest is 0.7537805350911206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64       954\n",
      "           1       0.78      0.84      0.81      1625\n",
      "\n",
      "    accuracy                           0.75      2579\n",
      "   macro avg       0.74      0.72      0.73      2579\n",
      "weighted avg       0.75      0.75      0.75      2579\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7572702597906166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       954\n",
      "           1       0.81      0.81      0.81      1625\n",
      "\n",
      "    accuracy                           0.76      2579\n",
      "   macro avg       0.74      0.74      0.74      2579\n",
      "weighted avg       0.76      0.76      0.76      2579\n",
      "\n",
      "Accuracy for XGBoost is 0.7188832880961613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.35      0.48       954\n",
      "           1       0.71      0.93      0.81      1625\n",
      "\n",
      "    accuracy                           0.72      2579\n",
      "   macro avg       0.73      0.64      0.64      2579\n",
      "weighted avg       0.73      0.72      0.69      2579\n",
      "\n",
      "Accuracy for AdaBoost is 0.7390461419154711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.51      0.59       954\n",
      "           1       0.75      0.87      0.81      1625\n",
      "\n",
      "    accuracy                           0.74      2579\n",
      "   macro avg       0.73      0.69      0.70      2579\n",
      "weighted avg       0.73      0.74      0.73      2579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7954188892375275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80      1541\n",
      "           1       0.83      0.76      0.79      1646\n",
      "\n",
      "    accuracy                           0.80      3187\n",
      "   macro avg       0.80      0.80      0.80      3187\n",
      "weighted avg       0.80      0.80      0.80      3187\n",
      "\n",
      "Accuracy for RandomForest is 0.8035770316912457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1541\n",
      "           1       0.82      0.79      0.81      1646\n",
      "\n",
      "    accuracy                           0.80      3187\n",
      "   macro avg       0.80      0.80      0.80      3187\n",
      "weighted avg       0.80      0.80      0.80      3187\n",
      "\n",
      "Accuracy for ExtraTrees is 0.8145591465327895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81      1541\n",
      "           1       0.84      0.80      0.82      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.82      0.81      3187\n",
      "weighted avg       0.82      0.81      0.81      3187\n",
      "\n",
      "Accuracy for XGBoost is 0.7182303106369626\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.89      0.75      1541\n",
      "           1       0.85      0.55      0.67      1646\n",
      "\n",
      "    accuracy                           0.72      3187\n",
      "   macro avg       0.75      0.72      0.71      3187\n",
      "weighted avg       0.75      0.72      0.71      3187\n",
      "\n",
      "Accuracy for AdaBoost is 0.733605271415124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76      1541\n",
      "           1       0.83      0.60      0.70      1646\n",
      "\n",
      "    accuracy                           0.73      3187\n",
      "   macro avg       0.75      0.74      0.73      3187\n",
      "weighted avg       0.76      0.73      0.73      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69455927\n",
      "Iteration 2, loss = 0.69380414\n",
      "Iteration 3, loss = 0.69331817\n",
      "Iteration 4, loss = 0.69308796\n",
      "Iteration 5, loss = 0.69290478\n",
      "Iteration 6, loss = 0.69275413\n",
      "Iteration 7, loss = 0.69264821\n",
      "Iteration 8, loss = 0.69254404\n",
      "Iteration 9, loss = 0.69246154\n",
      "Iteration 10, loss = 0.69237474\n",
      "Iteration 11, loss = 0.69228882\n",
      "Iteration 12, loss = 0.69220208\n",
      "Iteration 13, loss = 0.69212750\n",
      "Iteration 14, loss = 0.69205404\n",
      "Iteration 15, loss = 0.69196811\n",
      "Iteration 16, loss = 0.69190072\n",
      "Iteration 17, loss = 0.69181147\n",
      "Iteration 18, loss = 0.69173381\n",
      "Iteration 19, loss = 0.69165412\n",
      "Iteration 20, loss = 0.69157570\n",
      "Iteration 21, loss = 0.69150438\n",
      "Iteration 22, loss = 0.69142738\n",
      "Iteration 23, loss = 0.69135592\n",
      "Iteration 24, loss = 0.69127707\n",
      "Iteration 25, loss = 0.69118567\n",
      "Iteration 26, loss = 0.69114895\n",
      "Iteration 27, loss = 0.69101803\n",
      "Iteration 28, loss = 0.69095440\n",
      "Iteration 29, loss = 0.69086587\n",
      "Iteration 30, loss = 0.69078789\n",
      "Iteration 31, loss = 0.69070208\n",
      "Iteration 32, loss = 0.69062356\n",
      "Iteration 33, loss = 0.69053865\n",
      "Iteration 34, loss = 0.69050598\n",
      "Iteration 35, loss = 0.69040120\n",
      "Iteration 36, loss = 0.69029588\n",
      "Iteration 37, loss = 0.69019497\n",
      "Iteration 38, loss = 0.69009562\n",
      "Iteration 39, loss = 0.69000353\n",
      "Iteration 40, loss = 0.68992271\n",
      "Iteration 41, loss = 0.68981871\n",
      "Iteration 42, loss = 0.68969988\n",
      "Iteration 43, loss = 0.68959711\n",
      "Iteration 44, loss = 0.68948177\n",
      "Iteration 45, loss = 0.68936886\n",
      "Iteration 46, loss = 0.68927354\n",
      "Iteration 47, loss = 0.68914344\n",
      "Iteration 48, loss = 0.68902855\n",
      "Iteration 49, loss = 0.68890177\n",
      "Iteration 50, loss = 0.68878466\n",
      "Iteration 51, loss = 0.68866388\n",
      "Iteration 52, loss = 0.68855070\n",
      "Iteration 53, loss = 0.68845248\n",
      "Iteration 54, loss = 0.68830344\n",
      "Iteration 55, loss = 0.68818382\n",
      "Iteration 56, loss = 0.68804101\n",
      "Iteration 57, loss = 0.68791819\n",
      "Iteration 58, loss = 0.68776909\n",
      "Iteration 59, loss = 0.68762940\n",
      "Iteration 60, loss = 0.68748255\n",
      "Iteration 61, loss = 0.68735056\n",
      "Iteration 62, loss = 0.68720211\n",
      "Iteration 63, loss = 0.68704217\n",
      "Iteration 64, loss = 0.68689753\n",
      "Iteration 65, loss = 0.68673159\n",
      "Iteration 66, loss = 0.68656893\n",
      "Iteration 67, loss = 0.68640644\n",
      "Iteration 68, loss = 0.68623609\n",
      "Iteration 69, loss = 0.68607816\n",
      "Iteration 70, loss = 0.68590303\n",
      "Iteration 71, loss = 0.68570926\n",
      "Iteration 72, loss = 0.68553860\n",
      "Iteration 73, loss = 0.68535527\n",
      "Iteration 74, loss = 0.68516000\n",
      "Iteration 75, loss = 0.68495897\n",
      "Iteration 76, loss = 0.68475004\n",
      "Iteration 77, loss = 0.68450135\n",
      "Iteration 78, loss = 0.68430483\n",
      "Iteration 79, loss = 0.68405142\n",
      "Iteration 80, loss = 0.68381306\n",
      "Iteration 81, loss = 0.68355790\n",
      "Iteration 82, loss = 0.68330218\n",
      "Iteration 83, loss = 0.68305698\n",
      "Iteration 84, loss = 0.68277329\n",
      "Iteration 85, loss = 0.68249084\n",
      "Iteration 86, loss = 0.68218142\n",
      "Iteration 87, loss = 0.68188448\n",
      "Iteration 88, loss = 0.68158900\n",
      "Iteration 89, loss = 0.68127803\n",
      "Iteration 90, loss = 0.68100747\n",
      "Iteration 91, loss = 0.68061048\n",
      "Iteration 92, loss = 0.68033629\n",
      "Iteration 93, loss = 0.67990631\n",
      "Iteration 94, loss = 0.67955553\n",
      "Iteration 95, loss = 0.67918823\n",
      "Iteration 96, loss = 0.67880676\n",
      "Iteration 97, loss = 0.67840722\n",
      "Iteration 98, loss = 0.67802508\n",
      "Iteration 99, loss = 0.67761434\n",
      "Iteration 100, loss = 0.67714650\n",
      "Iteration 101, loss = 0.67668254\n",
      "Iteration 102, loss = 0.67623992\n",
      "Iteration 103, loss = 0.67577829\n",
      "Iteration 104, loss = 0.67527515\n",
      "Iteration 105, loss = 0.67477385\n",
      "Iteration 106, loss = 0.67423850\n",
      "Iteration 107, loss = 0.67369886\n",
      "Iteration 108, loss = 0.67313309\n",
      "Iteration 109, loss = 0.67251073\n",
      "Iteration 110, loss = 0.67193601\n",
      "Iteration 111, loss = 0.67127458\n",
      "Iteration 112, loss = 0.67058515\n",
      "Iteration 113, loss = 0.66989993\n",
      "Iteration 114, loss = 0.66916637\n",
      "Iteration 115, loss = 0.66842302\n",
      "Iteration 116, loss = 0.66765293\n",
      "Iteration 117, loss = 0.66686813\n",
      "Iteration 118, loss = 0.66602533\n",
      "Iteration 119, loss = 0.66517043\n",
      "Iteration 120, loss = 0.66431391\n",
      "Iteration 121, loss = 0.66334873\n",
      "Iteration 122, loss = 0.66237586\n",
      "Iteration 123, loss = 0.66135098\n",
      "Iteration 124, loss = 0.66031267\n",
      "Iteration 125, loss = 0.65923846\n",
      "Iteration 126, loss = 0.65812525\n",
      "Iteration 127, loss = 0.65694744\n",
      "Iteration 128, loss = 0.65576232\n",
      "Iteration 129, loss = 0.65443086\n",
      "Iteration 130, loss = 0.65314202\n",
      "Iteration 131, loss = 0.65177100\n",
      "Iteration 132, loss = 0.65037938\n",
      "Iteration 133, loss = 0.64893568\n",
      "Iteration 134, loss = 0.64743215\n",
      "Iteration 135, loss = 0.64580820\n",
      "Iteration 136, loss = 0.64419247\n",
      "Iteration 137, loss = 0.64249463\n",
      "Iteration 138, loss = 0.64076170\n",
      "Iteration 139, loss = 0.63904028\n",
      "Iteration 140, loss = 0.63707477\n",
      "Iteration 141, loss = 0.63513995\n",
      "Iteration 142, loss = 0.63322379\n",
      "Iteration 143, loss = 0.63106028\n",
      "Iteration 144, loss = 0.62891230\n",
      "Iteration 145, loss = 0.62660459\n",
      "Iteration 146, loss = 0.62420564\n",
      "Iteration 147, loss = 0.62183649\n",
      "Iteration 148, loss = 0.61934858\n",
      "Iteration 149, loss = 0.61673059\n",
      "Iteration 150, loss = 0.61404877\n",
      "Iteration 151, loss = 0.61135021\n",
      "Iteration 152, loss = 0.60846934\n",
      "Iteration 153, loss = 0.60557049\n",
      "Iteration 154, loss = 0.60252658\n",
      "Iteration 155, loss = 0.59936780\n",
      "Iteration 156, loss = 0.59616505\n",
      "Iteration 157, loss = 0.59303731\n",
      "Iteration 158, loss = 0.58971479\n",
      "Iteration 159, loss = 0.58623821\n",
      "Iteration 160, loss = 0.58269278\n",
      "Iteration 161, loss = 0.57919225\n",
      "Iteration 162, loss = 0.57568858\n",
      "Iteration 163, loss = 0.57171565\n",
      "Iteration 164, loss = 0.56804995\n",
      "Iteration 165, loss = 0.56407509\n",
      "Iteration 166, loss = 0.56030482\n",
      "Iteration 167, loss = 0.55628999\n",
      "Iteration 168, loss = 0.55220556\n",
      "Iteration 169, loss = 0.54820761\n",
      "Iteration 170, loss = 0.54405974\n",
      "Iteration 171, loss = 0.53997291\n",
      "Iteration 172, loss = 0.53587455\n",
      "Iteration 173, loss = 0.53171729\n",
      "Iteration 174, loss = 0.52759703\n",
      "Iteration 175, loss = 0.52367633\n",
      "Iteration 176, loss = 0.51932145\n",
      "Iteration 177, loss = 0.51527512\n",
      "Iteration 178, loss = 0.51137086\n",
      "Iteration 179, loss = 0.50707656\n",
      "Iteration 180, loss = 0.50298508\n",
      "Iteration 181, loss = 0.49923050\n",
      "Iteration 182, loss = 0.49513957\n",
      "Iteration 183, loss = 0.49109917\n",
      "Iteration 184, loss = 0.48739075\n",
      "Iteration 185, loss = 0.48349231\n",
      "Iteration 186, loss = 0.48024980\n",
      "Iteration 187, loss = 0.47599645\n",
      "Iteration 188, loss = 0.47273001\n",
      "Iteration 189, loss = 0.46862127\n",
      "Iteration 190, loss = 0.46559206\n",
      "Iteration 191, loss = 0.46159420\n",
      "Iteration 192, loss = 0.45802702\n",
      "Iteration 193, loss = 0.45448406\n",
      "Iteration 194, loss = 0.45107892\n",
      "Iteration 195, loss = 0.44820076\n",
      "Iteration 196, loss = 0.44468160\n",
      "Iteration 197, loss = 0.44123739\n",
      "Iteration 198, loss = 0.43829068\n",
      "Iteration 199, loss = 0.43519776\n",
      "Iteration 200, loss = 0.43213922\n",
      "Iteration 201, loss = 0.42916427\n",
      "Iteration 202, loss = 0.42636990\n",
      "Iteration 203, loss = 0.42331450\n",
      "Iteration 204, loss = 0.41996572\n",
      "Iteration 205, loss = 0.41744790\n",
      "Iteration 206, loss = 0.41432340\n",
      "Iteration 207, loss = 0.41168237\n",
      "Iteration 208, loss = 0.40927569\n",
      "Iteration 209, loss = 0.40612922\n",
      "Iteration 210, loss = 0.40415178\n",
      "Iteration 211, loss = 0.40084748\n",
      "Iteration 212, loss = 0.39844789\n",
      "Iteration 213, loss = 0.39574588\n",
      "Iteration 214, loss = 0.39343415\n",
      "Iteration 215, loss = 0.39110538\n",
      "Iteration 216, loss = 0.38860630\n",
      "Iteration 217, loss = 0.38625518\n",
      "Iteration 218, loss = 0.38380338\n",
      "Iteration 219, loss = 0.38144985\n",
      "Iteration 220, loss = 0.37963820\n",
      "Iteration 221, loss = 0.37683451\n",
      "Iteration 222, loss = 0.37457669\n",
      "Iteration 223, loss = 0.37304560\n",
      "Iteration 224, loss = 0.37037515\n",
      "Iteration 225, loss = 0.36817510\n",
      "Iteration 226, loss = 0.36598380\n",
      "Iteration 227, loss = 0.36405466\n",
      "Iteration 228, loss = 0.36168983\n",
      "Iteration 229, loss = 0.36030758\n",
      "Iteration 230, loss = 0.35756169\n",
      "Iteration 231, loss = 0.35559843\n",
      "Iteration 232, loss = 0.35376395\n",
      "Iteration 233, loss = 0.35139523\n",
      "Iteration 234, loss = 0.35012489\n",
      "Iteration 235, loss = 0.34796115\n",
      "Iteration 236, loss = 0.34616676\n",
      "Iteration 237, loss = 0.34453075\n",
      "Iteration 238, loss = 0.34243939\n",
      "Iteration 239, loss = 0.34058062\n",
      "Iteration 240, loss = 0.33877544\n",
      "Iteration 241, loss = 0.33720771\n",
      "Iteration 242, loss = 0.33565953\n",
      "Iteration 243, loss = 0.33309333\n",
      "Iteration 244, loss = 0.33171245\n",
      "Iteration 245, loss = 0.32994424\n",
      "Iteration 246, loss = 0.32803188\n",
      "Iteration 247, loss = 0.32665068\n",
      "Iteration 248, loss = 0.32475414\n",
      "Iteration 249, loss = 0.32307286\n",
      "Iteration 250, loss = 0.32161155\n",
      "Iteration 251, loss = 0.31972772\n",
      "Iteration 252, loss = 0.31801946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.31659422\n",
      "Iteration 254, loss = 0.31484300\n",
      "Iteration 255, loss = 0.31349743\n",
      "Iteration 256, loss = 0.31203923\n",
      "Iteration 257, loss = 0.31019530\n",
      "Iteration 258, loss = 0.30820147\n",
      "Iteration 259, loss = 0.30758252\n",
      "Iteration 260, loss = 0.30605448\n",
      "Iteration 261, loss = 0.30416723\n",
      "Iteration 262, loss = 0.30285004\n",
      "Iteration 263, loss = 0.30117398\n",
      "Iteration 264, loss = 0.29998094\n",
      "Iteration 265, loss = 0.29880740\n",
      "Iteration 266, loss = 0.29670852\n",
      "Iteration 267, loss = 0.29540232\n",
      "Iteration 268, loss = 0.29414041\n",
      "Iteration 269, loss = 0.29266963\n",
      "Iteration 270, loss = 0.29091304\n",
      "Iteration 271, loss = 0.29005098\n",
      "Iteration 272, loss = 0.28866473\n",
      "Iteration 273, loss = 0.28700751\n",
      "Iteration 274, loss = 0.28587979\n",
      "Iteration 275, loss = 0.28425132\n",
      "Iteration 276, loss = 0.28321132\n",
      "Iteration 277, loss = 0.28221016\n",
      "Iteration 278, loss = 0.28033496\n",
      "Iteration 279, loss = 0.27893428\n",
      "Iteration 280, loss = 0.27774291\n",
      "Iteration 281, loss = 0.27695442\n",
      "Iteration 282, loss = 0.27513325\n",
      "Iteration 283, loss = 0.27410109\n",
      "Iteration 284, loss = 0.27278534\n",
      "Iteration 285, loss = 0.27172911\n",
      "Iteration 286, loss = 0.26977088\n",
      "Iteration 287, loss = 0.26906361\n",
      "Iteration 288, loss = 0.26726373\n",
      "Iteration 289, loss = 0.26639343\n",
      "Iteration 290, loss = 0.26530868\n",
      "Iteration 291, loss = 0.26488124\n",
      "Iteration 292, loss = 0.26294133\n",
      "Iteration 293, loss = 0.26159585\n",
      "Iteration 294, loss = 0.26029340\n",
      "Iteration 295, loss = 0.25870627\n",
      "Iteration 296, loss = 0.25808942\n",
      "Iteration 297, loss = 0.25685999\n",
      "Iteration 298, loss = 0.25537505\n",
      "Iteration 299, loss = 0.25466550\n",
      "Iteration 300, loss = 0.25293016\n",
      "Iteration 301, loss = 0.25194751\n",
      "Iteration 302, loss = 0.25074930\n",
      "Iteration 303, loss = 0.24954393\n",
      "Iteration 304, loss = 0.24845570\n",
      "Iteration 305, loss = 0.24748328\n",
      "Iteration 306, loss = 0.24648285\n",
      "Iteration 307, loss = 0.24502249\n",
      "Iteration 308, loss = 0.24378494\n",
      "Iteration 309, loss = 0.24265446\n",
      "Iteration 310, loss = 0.24221014\n",
      "Iteration 311, loss = 0.24027225\n",
      "Iteration 312, loss = 0.23982669\n",
      "Iteration 313, loss = 0.23822572\n",
      "Iteration 314, loss = 0.23747456\n",
      "Iteration 315, loss = 0.23639856\n",
      "Iteration 316, loss = 0.23494524\n",
      "Iteration 317, loss = 0.23403438\n",
      "Iteration 318, loss = 0.23304348\n",
      "Iteration 319, loss = 0.23287747\n",
      "Iteration 320, loss = 0.23102282\n",
      "Iteration 321, loss = 0.22959037\n",
      "Iteration 322, loss = 0.22837864\n",
      "Iteration 323, loss = 0.22722457\n",
      "Iteration 324, loss = 0.22667408\n",
      "Iteration 325, loss = 0.22542238\n",
      "Iteration 326, loss = 0.22409480\n",
      "Iteration 327, loss = 0.22320150\n",
      "Iteration 328, loss = 0.22199885\n",
      "Iteration 329, loss = 0.22183943\n",
      "Iteration 330, loss = 0.22028619\n",
      "Iteration 331, loss = 0.21916101\n",
      "Iteration 332, loss = 0.21886287\n",
      "Iteration 333, loss = 0.21780176\n",
      "Iteration 334, loss = 0.21586441\n",
      "Iteration 335, loss = 0.21500700\n",
      "Iteration 336, loss = 0.21388884\n",
      "Iteration 337, loss = 0.21276048\n",
      "Iteration 338, loss = 0.21242483\n",
      "Iteration 339, loss = 0.21094659\n",
      "Iteration 340, loss = 0.20978975\n",
      "Iteration 341, loss = 0.20961227\n",
      "Iteration 342, loss = 0.20823179\n",
      "Iteration 343, loss = 0.20640927\n",
      "Iteration 344, loss = 0.20564541\n",
      "Iteration 345, loss = 0.20459523\n",
      "Iteration 346, loss = 0.20421557\n",
      "Iteration 347, loss = 0.20284247\n",
      "Iteration 348, loss = 0.20166058\n",
      "Iteration 349, loss = 0.20096196\n",
      "Iteration 350, loss = 0.19975409\n",
      "Iteration 351, loss = 0.19889947\n",
      "Iteration 352, loss = 0.19795678\n",
      "Iteration 353, loss = 0.19709846\n",
      "Iteration 354, loss = 0.19605494\n",
      "Iteration 355, loss = 0.19480598\n",
      "Iteration 356, loss = 0.19377099\n",
      "Iteration 357, loss = 0.19375393\n",
      "Iteration 358, loss = 0.19268246\n",
      "Iteration 359, loss = 0.19069256\n",
      "Iteration 360, loss = 0.19038573\n",
      "Iteration 361, loss = 0.18953825\n",
      "Iteration 362, loss = 0.18798658\n",
      "Iteration 363, loss = 0.18685244\n",
      "Iteration 364, loss = 0.18592913\n",
      "Iteration 365, loss = 0.18499040\n",
      "Iteration 366, loss = 0.18424238\n",
      "Iteration 367, loss = 0.18317121\n",
      "Iteration 368, loss = 0.18230520\n",
      "Iteration 369, loss = 0.18120185\n",
      "Iteration 370, loss = 0.18042102\n",
      "Iteration 371, loss = 0.17891102\n",
      "Iteration 372, loss = 0.17878918\n",
      "Iteration 373, loss = 0.17759517\n",
      "Iteration 374, loss = 0.17637135\n",
      "Iteration 375, loss = 0.17591322\n",
      "Iteration 376, loss = 0.17463842\n",
      "Iteration 377, loss = 0.17353515\n",
      "Iteration 378, loss = 0.17335157\n",
      "Iteration 379, loss = 0.17214353\n",
      "Iteration 380, loss = 0.17100174\n",
      "Iteration 381, loss = 0.17021012\n",
      "Iteration 382, loss = 0.16908650\n",
      "Iteration 383, loss = 0.16813929\n",
      "Iteration 384, loss = 0.16765211\n",
      "Iteration 385, loss = 0.16615349\n",
      "Iteration 386, loss = 0.16564147\n",
      "Iteration 387, loss = 0.16450129\n",
      "Iteration 388, loss = 0.16323917\n",
      "Iteration 389, loss = 0.16232324\n",
      "Iteration 390, loss = 0.16163027\n",
      "Iteration 391, loss = 0.16086941\n",
      "Iteration 392, loss = 0.15979947\n",
      "Iteration 393, loss = 0.15976665\n",
      "Iteration 394, loss = 0.15740682\n",
      "Iteration 395, loss = 0.15746691\n",
      "Iteration 396, loss = 0.15615331\n",
      "Iteration 397, loss = 0.15562933\n",
      "Iteration 398, loss = 0.15474527\n",
      "Iteration 399, loss = 0.15404318\n",
      "Iteration 400, loss = 0.15306391\n",
      "Iteration 401, loss = 0.15152221\n",
      "Iteration 402, loss = 0.15060986\n",
      "Iteration 403, loss = 0.15023275\n",
      "Iteration 404, loss = 0.14890359\n",
      "Iteration 405, loss = 0.14795414\n",
      "Iteration 406, loss = 0.14732689\n",
      "Iteration 407, loss = 0.14733072\n",
      "Iteration 408, loss = 0.14522914\n",
      "Iteration 409, loss = 0.14517805\n",
      "Iteration 410, loss = 0.14400485\n",
      "Iteration 411, loss = 0.14269999\n",
      "Iteration 412, loss = 0.14190770\n",
      "Iteration 413, loss = 0.14110367\n",
      "Iteration 414, loss = 0.14001516\n",
      "Iteration 415, loss = 0.13943074\n",
      "Iteration 416, loss = 0.13825350\n",
      "Iteration 417, loss = 0.13793229\n",
      "Iteration 418, loss = 0.13659456\n",
      "Iteration 419, loss = 0.13556175\n",
      "Iteration 420, loss = 0.13510300\n",
      "Iteration 421, loss = 0.13443605\n",
      "Iteration 422, loss = 0.13324163\n",
      "Iteration 423, loss = 0.13224951\n",
      "Iteration 424, loss = 0.13151316\n",
      "Iteration 425, loss = 0.13058337\n",
      "Iteration 426, loss = 0.12988910\n",
      "Iteration 427, loss = 0.12940368\n",
      "Iteration 428, loss = 0.12812721\n",
      "Iteration 429, loss = 0.12766603\n",
      "Iteration 430, loss = 0.12697977\n",
      "Iteration 431, loss = 0.12643025\n",
      "Iteration 432, loss = 0.12476947\n",
      "Iteration 433, loss = 0.12436335\n",
      "Iteration 434, loss = 0.12330483\n",
      "Iteration 435, loss = 0.12310857\n",
      "Iteration 436, loss = 0.12223239\n",
      "Iteration 437, loss = 0.12104192\n",
      "Iteration 438, loss = 0.12018832\n",
      "Iteration 439, loss = 0.12053669\n",
      "Iteration 440, loss = 0.11866039\n",
      "Iteration 441, loss = 0.11776969\n",
      "Iteration 442, loss = 0.11722893\n",
      "Iteration 443, loss = 0.11638023\n",
      "Iteration 444, loss = 0.11596756\n",
      "Iteration 445, loss = 0.11462307\n",
      "Iteration 446, loss = 0.11432645\n",
      "Iteration 447, loss = 0.11324075\n",
      "Iteration 448, loss = 0.11265225\n",
      "Iteration 449, loss = 0.11265510\n",
      "Iteration 450, loss = 0.11132624\n",
      "Iteration 451, loss = 0.11005513\n",
      "Iteration 452, loss = 0.10988974\n",
      "Iteration 453, loss = 0.10901855\n",
      "Iteration 454, loss = 0.10828915\n",
      "Iteration 455, loss = 0.10769480\n",
      "Iteration 456, loss = 0.10710544\n",
      "Iteration 457, loss = 0.10634135\n",
      "Iteration 458, loss = 0.10571352\n",
      "Iteration 459, loss = 0.10480768\n",
      "Iteration 460, loss = 0.10429663\n",
      "Iteration 461, loss = 0.10389326\n",
      "Iteration 462, loss = 0.10292797\n",
      "Iteration 463, loss = 0.10216068\n",
      "Iteration 464, loss = 0.10235497\n",
      "Iteration 465, loss = 0.10064137\n",
      "Iteration 466, loss = 0.10028622\n",
      "Iteration 467, loss = 0.09987388\n",
      "Iteration 468, loss = 0.09904144\n",
      "Iteration 469, loss = 0.09827473\n",
      "Iteration 470, loss = 0.09819590\n",
      "Iteration 471, loss = 0.09690013\n",
      "Iteration 472, loss = 0.09654998\n",
      "Iteration 473, loss = 0.09608298\n",
      "Iteration 474, loss = 0.09510197\n",
      "Iteration 475, loss = 0.09441472\n",
      "Iteration 476, loss = 0.09367021\n",
      "Iteration 477, loss = 0.09323648\n",
      "Iteration 478, loss = 0.09258156\n",
      "Iteration 479, loss = 0.09227481\n",
      "Iteration 480, loss = 0.09166213\n",
      "Iteration 481, loss = 0.09165676\n",
      "Iteration 482, loss = 0.09038971\n",
      "Iteration 483, loss = 0.08973516\n",
      "Iteration 484, loss = 0.08915100\n",
      "Iteration 485, loss = 0.08836508\n",
      "Iteration 486, loss = 0.08770231\n",
      "Iteration 487, loss = 0.08760238\n",
      "Iteration 488, loss = 0.08712003\n",
      "Iteration 489, loss = 0.08646510\n",
      "Iteration 490, loss = 0.08594205\n",
      "Iteration 491, loss = 0.08546009\n",
      "Iteration 492, loss = 0.08524608\n",
      "Iteration 493, loss = 0.08448658\n",
      "Iteration 494, loss = 0.08374784\n",
      "Iteration 495, loss = 0.08303442\n",
      "Iteration 496, loss = 0.08268582\n",
      "Iteration 497, loss = 0.08185981\n",
      "Iteration 498, loss = 0.08229150\n",
      "Iteration 499, loss = 0.08140051\n",
      "Iteration 500, loss = 0.08064323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1541\n",
      "           1       0.82      0.80      0.81      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_ #1883 features reduced to 1040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca = pca.transform(X_train)\n",
    "test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.79102604330091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79      1541\n",
      "           1       0.82      0.76      0.79      1646\n",
      "\n",
      "    accuracy                           0.79      3187\n",
      "   macro avg       0.79      0.79      0.79      3187\n",
      "weighted avg       0.79      0.79      0.79      3187\n",
      "\n",
      "Accuracy for RandomForest is 0.7806714778788829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.72      0.76      1541\n",
      "           1       0.76      0.84      0.80      1646\n",
      "\n",
      "    accuracy                           0.78      3187\n",
      "   macro avg       0.78      0.78      0.78      3187\n",
      "weighted avg       0.78      0.78      0.78      3187\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7809852525886414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.70      0.75      1541\n",
      "           1       0.75      0.86      0.80      1646\n",
      "\n",
      "    accuracy                           0.78      3187\n",
      "   macro avg       0.79      0.78      0.78      3187\n",
      "weighted avg       0.79      0.78      0.78      3187\n",
      "\n",
      "Accuracy for XGBoost is 0.7511766551615939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75      1541\n",
      "           1       0.77      0.73      0.75      1646\n",
      "\n",
      "    accuracy                           0.75      3187\n",
      "   macro avg       0.75      0.75      0.75      3187\n",
      "weighted avg       0.75      0.75      0.75      3187\n",
      "\n",
      "Accuracy for AdaBoost is 0.7282711013492312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      1541\n",
      "           1       0.76      0.68      0.72      1646\n",
      "\n",
      "    accuracy                           0.73      3187\n",
      "   macro avg       0.73      0.73      0.73      3187\n",
      "weighted avg       0.73      0.73      0.73      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(train_pca, test_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69547283\n",
      "Iteration 2, loss = 0.69427523\n",
      "Iteration 3, loss = 0.69350550\n",
      "Iteration 4, loss = 0.69300283\n",
      "Iteration 5, loss = 0.69272158\n",
      "Iteration 6, loss = 0.69253313\n",
      "Iteration 7, loss = 0.69236173\n",
      "Iteration 8, loss = 0.69225167\n",
      "Iteration 9, loss = 0.69215999\n",
      "Iteration 10, loss = 0.69208641\n",
      "Iteration 11, loss = 0.69199714\n",
      "Iteration 12, loss = 0.69189828\n",
      "Iteration 13, loss = 0.69181375\n",
      "Iteration 14, loss = 0.69173370\n",
      "Iteration 15, loss = 0.69163528\n",
      "Iteration 16, loss = 0.69155524\n",
      "Iteration 17, loss = 0.69146031\n",
      "Iteration 18, loss = 0.69137233\n",
      "Iteration 19, loss = 0.69128859\n",
      "Iteration 20, loss = 0.69121311\n",
      "Iteration 21, loss = 0.69110840\n",
      "Iteration 22, loss = 0.69102569\n",
      "Iteration 23, loss = 0.69093538\n",
      "Iteration 24, loss = 0.69084342\n",
      "Iteration 25, loss = 0.69076838\n",
      "Iteration 26, loss = 0.69067874\n",
      "Iteration 27, loss = 0.69059908\n",
      "Iteration 28, loss = 0.69051455\n",
      "Iteration 29, loss = 0.69041906\n",
      "Iteration 30, loss = 0.69033904\n",
      "Iteration 31, loss = 0.69024716\n",
      "Iteration 32, loss = 0.69016042\n",
      "Iteration 33, loss = 0.69007230\n",
      "Iteration 34, loss = 0.68997897\n",
      "Iteration 35, loss = 0.68989034\n",
      "Iteration 36, loss = 0.68979301\n",
      "Iteration 37, loss = 0.68969485\n",
      "Iteration 38, loss = 0.68960347\n",
      "Iteration 39, loss = 0.68949842\n",
      "Iteration 40, loss = 0.68939743\n",
      "Iteration 41, loss = 0.68929731\n",
      "Iteration 42, loss = 0.68919678\n",
      "Iteration 43, loss = 0.68908602\n",
      "Iteration 44, loss = 0.68897556\n",
      "Iteration 45, loss = 0.68886720\n",
      "Iteration 46, loss = 0.68876044\n",
      "Iteration 47, loss = 0.68864276\n",
      "Iteration 48, loss = 0.68852732\n",
      "Iteration 49, loss = 0.68840710\n",
      "Iteration 50, loss = 0.68828084\n",
      "Iteration 51, loss = 0.68815482\n",
      "Iteration 52, loss = 0.68802779\n",
      "Iteration 53, loss = 0.68789594\n",
      "Iteration 54, loss = 0.68776807\n",
      "Iteration 55, loss = 0.68762759\n",
      "Iteration 56, loss = 0.68749062\n",
      "Iteration 57, loss = 0.68734893\n",
      "Iteration 58, loss = 0.68720173\n",
      "Iteration 59, loss = 0.68704258\n",
      "Iteration 60, loss = 0.68689356\n",
      "Iteration 61, loss = 0.68674439\n",
      "Iteration 62, loss = 0.68657046\n",
      "Iteration 63, loss = 0.68641187\n",
      "Iteration 64, loss = 0.68624137\n",
      "Iteration 65, loss = 0.68607900\n",
      "Iteration 66, loss = 0.68591322\n",
      "Iteration 67, loss = 0.68570629\n",
      "Iteration 68, loss = 0.68552057\n",
      "Iteration 69, loss = 0.68532955\n",
      "Iteration 70, loss = 0.68512463\n",
      "Iteration 71, loss = 0.68494156\n",
      "Iteration 72, loss = 0.68469121\n",
      "Iteration 73, loss = 0.68451104\n",
      "Iteration 74, loss = 0.68424339\n",
      "Iteration 75, loss = 0.68402638\n",
      "Iteration 76, loss = 0.68377740\n",
      "Iteration 77, loss = 0.68351704\n",
      "Iteration 78, loss = 0.68326996\n",
      "Iteration 79, loss = 0.68301278\n",
      "Iteration 80, loss = 0.68274459\n",
      "Iteration 81, loss = 0.68246146\n",
      "Iteration 82, loss = 0.68218255\n",
      "Iteration 83, loss = 0.68185946\n",
      "Iteration 84, loss = 0.68155753\n",
      "Iteration 85, loss = 0.68126061\n",
      "Iteration 86, loss = 0.68093271\n",
      "Iteration 87, loss = 0.68059503\n",
      "Iteration 88, loss = 0.68025502\n",
      "Iteration 89, loss = 0.67987682\n",
      "Iteration 90, loss = 0.67951032\n",
      "Iteration 91, loss = 0.67915045\n",
      "Iteration 92, loss = 0.67874704\n",
      "Iteration 93, loss = 0.67833972\n",
      "Iteration 94, loss = 0.67792473\n",
      "Iteration 95, loss = 0.67751826\n",
      "Iteration 96, loss = 0.67706051\n",
      "Iteration 97, loss = 0.67660693\n",
      "Iteration 98, loss = 0.67612979\n",
      "Iteration 99, loss = 0.67564141\n",
      "Iteration 100, loss = 0.67512666\n",
      "Iteration 101, loss = 0.67459904\n",
      "Iteration 102, loss = 0.67406576\n",
      "Iteration 103, loss = 0.67352591\n",
      "Iteration 104, loss = 0.67293202\n",
      "Iteration 105, loss = 0.67231142\n",
      "Iteration 106, loss = 0.67167317\n",
      "Iteration 107, loss = 0.67106259\n",
      "Iteration 108, loss = 0.67040147\n",
      "Iteration 109, loss = 0.66970912\n",
      "Iteration 110, loss = 0.66891427\n",
      "Iteration 111, loss = 0.66818361\n",
      "Iteration 112, loss = 0.66742084\n",
      "Iteration 113, loss = 0.66661504\n",
      "Iteration 114, loss = 0.66575792\n",
      "Iteration 115, loss = 0.66488868\n",
      "Iteration 116, loss = 0.66399830\n",
      "Iteration 117, loss = 0.66306692\n",
      "Iteration 118, loss = 0.66209129\n",
      "Iteration 119, loss = 0.66108430\n",
      "Iteration 120, loss = 0.66002881\n",
      "Iteration 121, loss = 0.65895797\n",
      "Iteration 122, loss = 0.65780624\n",
      "Iteration 123, loss = 0.65667625\n",
      "Iteration 124, loss = 0.65540184\n",
      "Iteration 125, loss = 0.65413310\n",
      "Iteration 126, loss = 0.65282111\n",
      "Iteration 127, loss = 0.65146736\n",
      "Iteration 128, loss = 0.65000758\n",
      "Iteration 129, loss = 0.64850921\n",
      "Iteration 130, loss = 0.64702096\n",
      "Iteration 131, loss = 0.64553264\n",
      "Iteration 132, loss = 0.64382459\n",
      "Iteration 133, loss = 0.64215190\n",
      "Iteration 134, loss = 0.64035222\n",
      "Iteration 135, loss = 0.63853778\n",
      "Iteration 136, loss = 0.63666704\n",
      "Iteration 137, loss = 0.63470093\n",
      "Iteration 138, loss = 0.63261931\n",
      "Iteration 139, loss = 0.63054228\n",
      "Iteration 140, loss = 0.62838502\n",
      "Iteration 141, loss = 0.62616247\n",
      "Iteration 142, loss = 0.62389862\n",
      "Iteration 143, loss = 0.62145538\n",
      "Iteration 144, loss = 0.61897988\n",
      "Iteration 145, loss = 0.61639693\n",
      "Iteration 146, loss = 0.61374851\n",
      "Iteration 147, loss = 0.61109427\n",
      "Iteration 148, loss = 0.60830803\n",
      "Iteration 149, loss = 0.60539442\n",
      "Iteration 150, loss = 0.60243902\n",
      "Iteration 151, loss = 0.59943299\n",
      "Iteration 152, loss = 0.59636706\n",
      "Iteration 153, loss = 0.59323633\n",
      "Iteration 154, loss = 0.59002699\n",
      "Iteration 155, loss = 0.58664276\n",
      "Iteration 156, loss = 0.58322025\n",
      "Iteration 157, loss = 0.57984027\n",
      "Iteration 158, loss = 0.57631657\n",
      "Iteration 159, loss = 0.57280971\n",
      "Iteration 160, loss = 0.56920405\n",
      "Iteration 161, loss = 0.56554264\n",
      "Iteration 162, loss = 0.56188629\n",
      "Iteration 163, loss = 0.55800099\n",
      "Iteration 164, loss = 0.55425370\n",
      "Iteration 165, loss = 0.55058913\n",
      "Iteration 166, loss = 0.54652126\n",
      "Iteration 167, loss = 0.54277118\n",
      "Iteration 168, loss = 0.53885020\n",
      "Iteration 169, loss = 0.53528015\n",
      "Iteration 170, loss = 0.53128763\n",
      "Iteration 171, loss = 0.52733731\n",
      "Iteration 172, loss = 0.52369487\n",
      "Iteration 173, loss = 0.51963734\n",
      "Iteration 174, loss = 0.51595744\n",
      "Iteration 175, loss = 0.51217996\n",
      "Iteration 176, loss = 0.50853622\n",
      "Iteration 177, loss = 0.50492690\n",
      "Iteration 178, loss = 0.50137684\n",
      "Iteration 179, loss = 0.49760047\n",
      "Iteration 180, loss = 0.49394077\n",
      "Iteration 181, loss = 0.49046923\n",
      "Iteration 182, loss = 0.48682324\n",
      "Iteration 183, loss = 0.48347643\n",
      "Iteration 184, loss = 0.48009262\n",
      "Iteration 185, loss = 0.47668509\n",
      "Iteration 186, loss = 0.47332092\n",
      "Iteration 187, loss = 0.47058798\n",
      "Iteration 188, loss = 0.46698696\n",
      "Iteration 189, loss = 0.46404752\n",
      "Iteration 190, loss = 0.46103697\n",
      "Iteration 191, loss = 0.45774736\n",
      "Iteration 192, loss = 0.45465860\n",
      "Iteration 193, loss = 0.45189405\n",
      "Iteration 194, loss = 0.44885979\n",
      "Iteration 195, loss = 0.44608724\n",
      "Iteration 196, loss = 0.44340763\n",
      "Iteration 197, loss = 0.44063156\n",
      "Iteration 198, loss = 0.43797120\n",
      "Iteration 199, loss = 0.43537943\n",
      "Iteration 200, loss = 0.43260303\n",
      "Iteration 201, loss = 0.42995360\n",
      "Iteration 202, loss = 0.42773160\n",
      "Iteration 203, loss = 0.42509563\n",
      "Iteration 204, loss = 0.42263614\n",
      "Iteration 205, loss = 0.42045010\n",
      "Iteration 206, loss = 0.41786744\n",
      "Iteration 207, loss = 0.41579286\n",
      "Iteration 208, loss = 0.41344206\n",
      "Iteration 209, loss = 0.41133081\n",
      "Iteration 210, loss = 0.40881397\n",
      "Iteration 211, loss = 0.40686129\n",
      "Iteration 212, loss = 0.40476042\n",
      "Iteration 213, loss = 0.40257050\n",
      "Iteration 214, loss = 0.40094420\n",
      "Iteration 215, loss = 0.39842788\n",
      "Iteration 216, loss = 0.39643709\n",
      "Iteration 217, loss = 0.39442264\n",
      "Iteration 218, loss = 0.39321479\n",
      "Iteration 219, loss = 0.39079371\n",
      "Iteration 220, loss = 0.38868371\n",
      "Iteration 221, loss = 0.38768967\n",
      "Iteration 222, loss = 0.38536830\n",
      "Iteration 223, loss = 0.38347287\n",
      "Iteration 224, loss = 0.38216497\n",
      "Iteration 225, loss = 0.38053726\n",
      "Iteration 226, loss = 0.37855333\n",
      "Iteration 227, loss = 0.37677104\n",
      "Iteration 228, loss = 0.37489539\n",
      "Iteration 229, loss = 0.37317475\n",
      "Iteration 230, loss = 0.37186948\n",
      "Iteration 231, loss = 0.37001371\n",
      "Iteration 232, loss = 0.36838724\n",
      "Iteration 233, loss = 0.36744475\n",
      "Iteration 234, loss = 0.36556575\n",
      "Iteration 235, loss = 0.36437539\n",
      "Iteration 236, loss = 0.36253656\n",
      "Iteration 237, loss = 0.36141027\n",
      "Iteration 238, loss = 0.35989480\n",
      "Iteration 239, loss = 0.35835605\n",
      "Iteration 240, loss = 0.35694228\n",
      "Iteration 241, loss = 0.35561728\n",
      "Iteration 242, loss = 0.35415134\n",
      "Iteration 243, loss = 0.35300734\n",
      "Iteration 244, loss = 0.35156134\n",
      "Iteration 245, loss = 0.35018030\n",
      "Iteration 246, loss = 0.34956350\n",
      "Iteration 247, loss = 0.34757395\n",
      "Iteration 248, loss = 0.34626426\n",
      "Iteration 249, loss = 0.34563365\n",
      "Iteration 250, loss = 0.34453194\n",
      "Iteration 251, loss = 0.34309330\n",
      "Iteration 252, loss = 0.34160970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.34064628\n",
      "Iteration 254, loss = 0.33937833\n",
      "Iteration 255, loss = 0.33838152\n",
      "Iteration 256, loss = 0.33733491\n",
      "Iteration 257, loss = 0.33596376\n",
      "Iteration 258, loss = 0.33467167\n",
      "Iteration 259, loss = 0.33424728\n",
      "Iteration 260, loss = 0.33310485\n",
      "Iteration 261, loss = 0.33143612\n",
      "Iteration 262, loss = 0.33004775\n",
      "Iteration 263, loss = 0.32923921\n",
      "Iteration 264, loss = 0.32823467\n",
      "Iteration 265, loss = 0.32775897\n",
      "Iteration 266, loss = 0.32641785\n",
      "Iteration 267, loss = 0.32515573\n",
      "Iteration 268, loss = 0.32473209\n",
      "Iteration 269, loss = 0.32280650\n",
      "Iteration 270, loss = 0.32295378\n",
      "Iteration 271, loss = 0.32132515\n",
      "Iteration 272, loss = 0.32016918\n",
      "Iteration 273, loss = 0.31899345\n",
      "Iteration 274, loss = 0.31790284\n",
      "Iteration 275, loss = 0.31699731\n",
      "Iteration 276, loss = 0.31610312\n",
      "Iteration 277, loss = 0.31521080\n",
      "Iteration 278, loss = 0.31421016\n",
      "Iteration 279, loss = 0.31329972\n",
      "Iteration 280, loss = 0.31314403\n",
      "Iteration 281, loss = 0.31153518\n",
      "Iteration 282, loss = 0.31104188\n",
      "Iteration 283, loss = 0.31000305\n",
      "Iteration 284, loss = 0.30909390\n",
      "Iteration 285, loss = 0.30800049\n",
      "Iteration 286, loss = 0.30676091\n",
      "Iteration 287, loss = 0.30710329\n",
      "Iteration 288, loss = 0.30504939\n",
      "Iteration 289, loss = 0.30437203\n",
      "Iteration 290, loss = 0.30390482\n",
      "Iteration 291, loss = 0.30272107\n",
      "Iteration 292, loss = 0.30308303\n",
      "Iteration 293, loss = 0.30083830\n",
      "Iteration 294, loss = 0.30014305\n",
      "Iteration 295, loss = 0.29963743\n",
      "Iteration 296, loss = 0.29882032\n",
      "Iteration 297, loss = 0.29808828\n",
      "Iteration 298, loss = 0.29638830\n",
      "Iteration 299, loss = 0.29638588\n",
      "Iteration 300, loss = 0.29518177\n",
      "Iteration 301, loss = 0.29451471\n",
      "Iteration 302, loss = 0.29340790\n",
      "Iteration 303, loss = 0.29268969\n",
      "Iteration 304, loss = 0.29245531\n",
      "Iteration 305, loss = 0.29164597\n",
      "Iteration 306, loss = 0.29033002\n",
      "Iteration 307, loss = 0.28945774\n",
      "Iteration 308, loss = 0.28870386\n",
      "Iteration 309, loss = 0.28871954\n",
      "Iteration 310, loss = 0.28755665\n",
      "Iteration 311, loss = 0.28674870\n",
      "Iteration 312, loss = 0.28560827\n",
      "Iteration 313, loss = 0.28465545\n",
      "Iteration 314, loss = 0.28382367\n",
      "Iteration 315, loss = 0.28299828\n",
      "Iteration 316, loss = 0.28211343\n",
      "Iteration 317, loss = 0.28119774\n",
      "Iteration 318, loss = 0.28069044\n",
      "Iteration 319, loss = 0.28061860\n",
      "Iteration 320, loss = 0.27963845\n",
      "Iteration 321, loss = 0.27846406\n",
      "Iteration 322, loss = 0.27823683\n",
      "Iteration 323, loss = 0.27683051\n",
      "Iteration 324, loss = 0.27647714\n",
      "Iteration 325, loss = 0.27497434\n",
      "Iteration 326, loss = 0.27530038\n",
      "Iteration 327, loss = 0.27370512\n",
      "Iteration 328, loss = 0.27313302\n",
      "Iteration 329, loss = 0.27250014\n",
      "Iteration 330, loss = 0.27190469\n",
      "Iteration 331, loss = 0.27110471\n",
      "Iteration 332, loss = 0.27024588\n",
      "Iteration 333, loss = 0.26966794\n",
      "Iteration 334, loss = 0.26806270\n",
      "Iteration 335, loss = 0.26770122\n",
      "Iteration 336, loss = 0.26707641\n",
      "Iteration 337, loss = 0.26625110\n",
      "Iteration 338, loss = 0.26507012\n",
      "Iteration 339, loss = 0.26410420\n",
      "Iteration 340, loss = 0.26372503\n",
      "Iteration 341, loss = 0.26290586\n",
      "Iteration 342, loss = 0.26223714\n",
      "Iteration 343, loss = 0.26310303\n",
      "Iteration 344, loss = 0.26033088\n",
      "Iteration 345, loss = 0.25967170\n",
      "Iteration 346, loss = 0.25932327\n",
      "Iteration 347, loss = 0.25805043\n",
      "Iteration 348, loss = 0.25755504\n",
      "Iteration 349, loss = 0.25728320\n",
      "Iteration 350, loss = 0.25597497\n",
      "Iteration 351, loss = 0.25512203\n",
      "Iteration 352, loss = 0.25450601\n",
      "Iteration 353, loss = 0.25355047\n",
      "Iteration 354, loss = 0.25314520\n",
      "Iteration 355, loss = 0.25249802\n",
      "Iteration 356, loss = 0.25194763\n",
      "Iteration 357, loss = 0.25043410\n",
      "Iteration 358, loss = 0.24960135\n",
      "Iteration 359, loss = 0.24892706\n",
      "Iteration 360, loss = 0.24791274\n",
      "Iteration 361, loss = 0.24766716\n",
      "Iteration 362, loss = 0.24687970\n",
      "Iteration 363, loss = 0.24581199\n",
      "Iteration 364, loss = 0.24501012\n",
      "Iteration 365, loss = 0.24406383\n",
      "Iteration 366, loss = 0.24374140\n",
      "Iteration 367, loss = 0.24278365\n",
      "Iteration 368, loss = 0.24182366\n",
      "Iteration 369, loss = 0.24118259\n",
      "Iteration 370, loss = 0.24013687\n",
      "Iteration 371, loss = 0.24097094\n",
      "Iteration 372, loss = 0.23896202\n",
      "Iteration 373, loss = 0.23844840\n",
      "Iteration 374, loss = 0.23707803\n",
      "Iteration 375, loss = 0.23619811\n",
      "Iteration 376, loss = 0.23518644\n",
      "Iteration 377, loss = 0.23459000\n",
      "Iteration 378, loss = 0.23377208\n",
      "Iteration 379, loss = 0.23258320\n",
      "Iteration 380, loss = 0.23197438\n",
      "Iteration 381, loss = 0.23161580\n",
      "Iteration 382, loss = 0.23053438\n",
      "Iteration 383, loss = 0.23005781\n",
      "Iteration 384, loss = 0.22933469\n",
      "Iteration 385, loss = 0.22807147\n",
      "Iteration 386, loss = 0.22715649\n",
      "Iteration 387, loss = 0.22671993\n",
      "Iteration 388, loss = 0.22582597\n",
      "Iteration 389, loss = 0.22480173\n",
      "Iteration 390, loss = 0.22351172\n",
      "Iteration 391, loss = 0.22382317\n",
      "Iteration 392, loss = 0.22251201\n",
      "Iteration 393, loss = 0.22127036\n",
      "Iteration 394, loss = 0.22039458\n",
      "Iteration 395, loss = 0.21990819\n",
      "Iteration 396, loss = 0.21895150\n",
      "Iteration 397, loss = 0.21807003\n",
      "Iteration 398, loss = 0.21738679\n",
      "Iteration 399, loss = 0.21733134\n",
      "Iteration 400, loss = 0.21545466\n",
      "Iteration 401, loss = 0.21472126\n",
      "Iteration 402, loss = 0.21381499\n",
      "Iteration 403, loss = 0.21369226\n",
      "Iteration 404, loss = 0.21282221\n",
      "Iteration 405, loss = 0.21147428\n",
      "Iteration 406, loss = 0.21062359\n",
      "Iteration 407, loss = 0.21000432\n",
      "Iteration 408, loss = 0.20886638\n",
      "Iteration 409, loss = 0.20764631\n",
      "Iteration 410, loss = 0.20704880\n",
      "Iteration 411, loss = 0.20607828\n",
      "Iteration 412, loss = 0.20507909\n",
      "Iteration 413, loss = 0.20400366\n",
      "Iteration 414, loss = 0.20370846\n",
      "Iteration 415, loss = 0.20251004\n",
      "Iteration 416, loss = 0.20195467\n",
      "Iteration 417, loss = 0.20112271\n",
      "Iteration 418, loss = 0.20003164\n",
      "Iteration 419, loss = 0.19898533\n",
      "Iteration 420, loss = 0.19837156\n",
      "Iteration 421, loss = 0.19776216\n",
      "Iteration 422, loss = 0.19682014\n",
      "Iteration 423, loss = 0.19551254\n",
      "Iteration 424, loss = 0.19440222\n",
      "Iteration 425, loss = 0.19353588\n",
      "Iteration 426, loss = 0.19285655\n",
      "Iteration 427, loss = 0.19228652\n",
      "Iteration 428, loss = 0.19148040\n",
      "Iteration 429, loss = 0.19046743\n",
      "Iteration 430, loss = 0.18943664\n",
      "Iteration 431, loss = 0.18856484\n",
      "Iteration 432, loss = 0.18770443\n",
      "Iteration 433, loss = 0.18683474\n",
      "Iteration 434, loss = 0.18573917\n",
      "Iteration 435, loss = 0.18511855\n",
      "Iteration 436, loss = 0.18438940\n",
      "Iteration 437, loss = 0.18283854\n",
      "Iteration 438, loss = 0.18215072\n",
      "Iteration 439, loss = 0.18182596\n",
      "Iteration 440, loss = 0.18009961\n",
      "Iteration 441, loss = 0.17966233\n",
      "Iteration 442, loss = 0.17861389\n",
      "Iteration 443, loss = 0.17721319\n",
      "Iteration 444, loss = 0.17640918\n",
      "Iteration 445, loss = 0.17573090\n",
      "Iteration 446, loss = 0.17465298\n",
      "Iteration 447, loss = 0.17376178\n",
      "Iteration 448, loss = 0.17300773\n",
      "Iteration 449, loss = 0.17178643\n",
      "Iteration 450, loss = 0.17111784\n",
      "Iteration 451, loss = 0.16997237\n",
      "Iteration 452, loss = 0.16952761\n",
      "Iteration 453, loss = 0.16882830\n",
      "Iteration 454, loss = 0.16729886\n",
      "Iteration 455, loss = 0.16765353\n",
      "Iteration 456, loss = 0.16559986\n",
      "Iteration 457, loss = 0.16468991\n",
      "Iteration 458, loss = 0.16452410\n",
      "Iteration 459, loss = 0.16241105\n",
      "Iteration 460, loss = 0.16179983\n",
      "Iteration 461, loss = 0.16080000\n",
      "Iteration 462, loss = 0.16025720\n",
      "Iteration 463, loss = 0.15904499\n",
      "Iteration 464, loss = 0.15869113\n",
      "Iteration 465, loss = 0.15714319\n",
      "Iteration 466, loss = 0.15639361\n",
      "Iteration 467, loss = 0.15563471\n",
      "Iteration 468, loss = 0.15489218\n",
      "Iteration 469, loss = 0.15342879\n",
      "Iteration 470, loss = 0.15293840\n",
      "Iteration 471, loss = 0.15187213\n",
      "Iteration 472, loss = 0.15108380\n",
      "Iteration 473, loss = 0.14968421\n",
      "Iteration 474, loss = 0.14886308\n",
      "Iteration 475, loss = 0.14850052\n",
      "Iteration 476, loss = 0.14746255\n",
      "Iteration 477, loss = 0.14632929\n",
      "Iteration 478, loss = 0.14586228\n",
      "Iteration 479, loss = 0.14461641\n",
      "Iteration 480, loss = 0.14379707\n",
      "Iteration 481, loss = 0.14245933\n",
      "Iteration 482, loss = 0.14192878\n",
      "Iteration 483, loss = 0.14109683\n",
      "Iteration 484, loss = 0.14010825\n",
      "Iteration 485, loss = 0.13919620\n",
      "Iteration 486, loss = 0.13872646\n",
      "Iteration 487, loss = 0.13746969\n",
      "Iteration 488, loss = 0.13689499\n",
      "Iteration 489, loss = 0.13596633\n",
      "Iteration 490, loss = 0.13514272\n",
      "Iteration 491, loss = 0.13355250\n",
      "Iteration 492, loss = 0.13315797\n",
      "Iteration 493, loss = 0.13205166\n",
      "Iteration 494, loss = 0.13094961\n",
      "Iteration 495, loss = 0.13080930\n",
      "Iteration 496, loss = 0.12989927\n",
      "Iteration 497, loss = 0.12877119\n",
      "Iteration 498, loss = 0.12818808\n",
      "Iteration 499, loss = 0.12705997\n",
      "Iteration 500, loss = 0.12656926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      1541\n",
      "           1       0.83      0.76      0.80      1646\n",
      "\n",
      "    accuracy                           0.80      3187\n",
      "   macro avg       0.80      0.80      0.80      3187\n",
      "weighted avg       0.80      0.80      0.80      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(test_pca)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        [layin, n, bed, headache, ughhhhwaitin, call]\n",
       "2                    [funeral, ceremonygloomy, friday]\n",
       "6    [sleep, im, thinking, old, friend, want, hes, ...\n",
       "8                               [charlene, love, miss]\n",
       "9                           [im, sorry, least, friday]\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_stop_removal = df_bin[\"content\"].apply(lambda x: [item for item in x.split() if item not in stop])\n",
    "w2v_stop_removal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        print(\"cannot compute similarity\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute similarity ['suuure']\n",
      "cannot compute similarity ['roughnight']\n",
      "cannot compute similarity ['grrrrrrrrrrrrrrrrrrrrrrrr']\n",
      "cannot compute similarity ['--', 'yeahhh', 'wasnt', 'thereeeeeeeeeee']\n",
      "cannot compute similarity ['yeaaaahh']\n",
      "cannot compute similarity ['hallooo', 'bayernhallooo', 'stau']\n",
      "cannot compute similarity ['awee']\n",
      "cannot compute similarity ['suchatease']\n",
      "cannot compute similarity []\n",
      "cannot compute similarity []\n",
      "cannot compute similarity ['sadface']\n",
      "cannot compute similarity ['shareeee']\n",
      "cannot compute similarity ['michelleeeeeeeeeeeemybelleeeeeeeeeeeeeeeeee', '*snif', 'snif*']\n",
      "cannot compute similarity []\n",
      "cannot compute similarity ['goooooodmorning']\n",
      "cannot compute similarity ['helloooo']\n",
      "cannot compute similarity ['chilliin']\n",
      "cannot compute similarity ['hahaaaha']\n",
      "cannot compute similarity ['tear*']\n",
      "cannot compute similarity ['goodmorning']\n",
      "cannot compute similarity ['http//twitpiccom/vfcx', 'awesomeeeeee']\n",
      "cannot compute similarity ['goodmorning']\n",
      "cannot compute similarity ['yeaaa']\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "train_tokenized = df_bin.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df_bin, test_size=0.3, random_state = 42)\n",
    "\n",
    "# test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "# train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "\n",
    "# X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "# X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(X_train_word_average, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X_train, w2v_X_test, w2v_y_train, w2v_y_test = train_test_split(X_train_word_average, w2v_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.791779759596743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70       954\n",
      "           1       0.81      0.87      0.84      1625\n",
      "\n",
      "    accuracy                           0.79      2579\n",
      "   macro avg       0.78      0.76      0.77      2579\n",
      "weighted avg       0.79      0.79      0.79      2579\n",
      "\n",
      "Accuracy for RandomForest is 0.7468010856921288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.45      0.57       954\n",
      "           1       0.74      0.92      0.82      1625\n",
      "\n",
      "    accuracy                           0.75      2579\n",
      "   macro avg       0.76      0.68      0.69      2579\n",
      "weighted avg       0.75      0.75      0.73      2579\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7409848778596355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.40      0.53       954\n",
      "           1       0.73      0.94      0.82      1625\n",
      "\n",
      "    accuracy                           0.74      2579\n",
      "   macro avg       0.76      0.67      0.68      2579\n",
      "weighted avg       0.75      0.74      0.71      2579\n",
      "\n",
      "Accuracy for XGBoost is 0.781698332687088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.58      0.66       954\n",
      "           1       0.79      0.90      0.84      1625\n",
      "\n",
      "    accuracy                           0.78      2579\n",
      "   macro avg       0.78      0.74      0.75      2579\n",
      "weighted avg       0.78      0.78      0.77      2579\n",
      "\n",
      "Accuracy for AdaBoost is 0.7355564172159752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63       954\n",
      "           1       0.78      0.81      0.79      1625\n",
      "\n",
      "    accuracy                           0.74      2579\n",
      "   macro avg       0.72      0.71      0.71      2579\n",
      "weighted avg       0.73      0.74      0.73      2579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(w2v_X_train, w2v_X_test, w2v_y_train, w2v_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7721995607154063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77      1541\n",
      "           1       0.78      0.77      0.78      1646\n",
      "\n",
      "    accuracy                           0.77      3187\n",
      "   macro avg       0.77      0.77      0.77      3187\n",
      "weighted avg       0.77      0.77      0.77      3187\n",
      "\n",
      "Accuracy for RandomForest is 0.8123627235644807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      1541\n",
      "           1       0.81      0.83      0.82      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n",
      "Accuracy for ExtraTrees is 0.8136178224035143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80      1541\n",
      "           1       0.81      0.84      0.82      1646\n",
      "\n",
      "    accuracy                           0.81      3187\n",
      "   macro avg       0.81      0.81      0.81      3187\n",
      "weighted avg       0.81      0.81      0.81      3187\n",
      "\n",
      "Accuracy for XGBoost is 0.7781612802008158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78      1541\n",
      "           1       0.80      0.76      0.78      1646\n",
      "\n",
      "    accuracy                           0.78      3187\n",
      "   macro avg       0.78      0.78      0.78      3187\n",
      "weighted avg       0.78      0.78      0.78      3187\n",
      "\n",
      "Accuracy for AdaBoost is 0.740822089739567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74      1541\n",
      "           1       0.76      0.73      0.74      1646\n",
      "\n",
      "    accuracy                           0.74      3187\n",
      "   macro avg       0.74      0.74      0.74      3187\n",
      "weighted avg       0.74      0.74      0.74      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69278984\n",
      "Iteration 2, loss = 0.69261083\n",
      "Iteration 3, loss = 0.69238648\n",
      "Iteration 4, loss = 0.69218977\n",
      "Iteration 5, loss = 0.69200117\n",
      "Iteration 6, loss = 0.69183798\n",
      "Iteration 7, loss = 0.69164686\n",
      "Iteration 8, loss = 0.69147120\n",
      "Iteration 9, loss = 0.69128926\n",
      "Iteration 10, loss = 0.69113096\n",
      "Iteration 11, loss = 0.69095970\n",
      "Iteration 12, loss = 0.69078084\n",
      "Iteration 13, loss = 0.69061634\n",
      "Iteration 14, loss = 0.69044750\n",
      "Iteration 15, loss = 0.69026286\n",
      "Iteration 16, loss = 0.69010285\n",
      "Iteration 17, loss = 0.68991228\n",
      "Iteration 18, loss = 0.68972835\n",
      "Iteration 19, loss = 0.68953690\n",
      "Iteration 20, loss = 0.68934345\n",
      "Iteration 21, loss = 0.68914665\n",
      "Iteration 22, loss = 0.68894723\n",
      "Iteration 23, loss = 0.68874080\n",
      "Iteration 24, loss = 0.68853408\n",
      "Iteration 25, loss = 0.68832190\n",
      "Iteration 26, loss = 0.68809078\n",
      "Iteration 27, loss = 0.68788854\n",
      "Iteration 28, loss = 0.68762338\n",
      "Iteration 29, loss = 0.68742279\n",
      "Iteration 30, loss = 0.68713292\n",
      "Iteration 31, loss = 0.68687091\n",
      "Iteration 32, loss = 0.68661688\n",
      "Iteration 33, loss = 0.68632269\n",
      "Iteration 34, loss = 0.68602717\n",
      "Iteration 35, loss = 0.68572212\n",
      "Iteration 36, loss = 0.68543848\n",
      "Iteration 37, loss = 0.68509150\n",
      "Iteration 38, loss = 0.68474566\n",
      "Iteration 39, loss = 0.68440100\n",
      "Iteration 40, loss = 0.68407271\n",
      "Iteration 41, loss = 0.68368153\n",
      "Iteration 42, loss = 0.68327951\n",
      "Iteration 43, loss = 0.68289311\n",
      "Iteration 44, loss = 0.68248216\n",
      "Iteration 45, loss = 0.68202487\n",
      "Iteration 46, loss = 0.68159013\n",
      "Iteration 47, loss = 0.68111314\n",
      "Iteration 48, loss = 0.68061403\n",
      "Iteration 49, loss = 0.68011852\n",
      "Iteration 50, loss = 0.67960964\n",
      "Iteration 51, loss = 0.67901101\n",
      "Iteration 52, loss = 0.67844299\n",
      "Iteration 53, loss = 0.67786067\n",
      "Iteration 54, loss = 0.67726168\n",
      "Iteration 55, loss = 0.67666634\n",
      "Iteration 56, loss = 0.67595375\n",
      "Iteration 57, loss = 0.67531191\n",
      "Iteration 58, loss = 0.67458057\n",
      "Iteration 59, loss = 0.67382776\n",
      "Iteration 60, loss = 0.67306899\n",
      "Iteration 61, loss = 0.67228269\n",
      "Iteration 62, loss = 0.67144772\n",
      "Iteration 63, loss = 0.67064442\n",
      "Iteration 64, loss = 0.66968550\n",
      "Iteration 65, loss = 0.66874067\n",
      "Iteration 66, loss = 0.66778824\n",
      "Iteration 67, loss = 0.66682799\n",
      "Iteration 68, loss = 0.66572603\n",
      "Iteration 69, loss = 0.66450763\n",
      "Iteration 70, loss = 0.66334413\n",
      "Iteration 71, loss = 0.66214969\n",
      "Iteration 72, loss = 0.66091072\n",
      "Iteration 73, loss = 0.65964340\n",
      "Iteration 74, loss = 0.65830741\n",
      "Iteration 75, loss = 0.65685259\n",
      "Iteration 76, loss = 0.65543622\n",
      "Iteration 77, loss = 0.65386511\n",
      "Iteration 78, loss = 0.65232245\n",
      "Iteration 79, loss = 0.65062924\n",
      "Iteration 80, loss = 0.64888829\n",
      "Iteration 81, loss = 0.64706374\n",
      "Iteration 82, loss = 0.64520980\n",
      "Iteration 83, loss = 0.64324909\n",
      "Iteration 84, loss = 0.64117848\n",
      "Iteration 85, loss = 0.63911586\n",
      "Iteration 86, loss = 0.63689631\n",
      "Iteration 87, loss = 0.63465593\n",
      "Iteration 88, loss = 0.63221275\n",
      "Iteration 89, loss = 0.62970302\n",
      "Iteration 90, loss = 0.62717178\n",
      "Iteration 91, loss = 0.62452223\n",
      "Iteration 92, loss = 0.62170483\n",
      "Iteration 93, loss = 0.61881505\n",
      "Iteration 94, loss = 0.61592219\n",
      "Iteration 95, loss = 0.61285717\n",
      "Iteration 96, loss = 0.60981576\n",
      "Iteration 97, loss = 0.60659149\n",
      "Iteration 98, loss = 0.60347621\n",
      "Iteration 99, loss = 0.60016458\n",
      "Iteration 100, loss = 0.59682559\n",
      "Iteration 101, loss = 0.59340705\n",
      "Iteration 102, loss = 0.58989931\n",
      "Iteration 103, loss = 0.58642076\n",
      "Iteration 104, loss = 0.58281437\n",
      "Iteration 105, loss = 0.57929051\n",
      "Iteration 106, loss = 0.57566206\n",
      "Iteration 107, loss = 0.57201744\n",
      "Iteration 108, loss = 0.56850351\n",
      "Iteration 109, loss = 0.56488549\n",
      "Iteration 110, loss = 0.56116966\n",
      "Iteration 111, loss = 0.55772923\n",
      "Iteration 112, loss = 0.55410016\n",
      "Iteration 113, loss = 0.55074375\n",
      "Iteration 114, loss = 0.54716038\n",
      "Iteration 115, loss = 0.54365717\n",
      "Iteration 116, loss = 0.54045057\n",
      "Iteration 117, loss = 0.53711735\n",
      "Iteration 118, loss = 0.53377770\n",
      "Iteration 119, loss = 0.53073912\n",
      "Iteration 120, loss = 0.52752211\n",
      "Iteration 121, loss = 0.52452069\n",
      "Iteration 122, loss = 0.52152970\n",
      "Iteration 123, loss = 0.51916882\n",
      "Iteration 124, loss = 0.51611782\n",
      "Iteration 125, loss = 0.51346913\n",
      "Iteration 126, loss = 0.51107590\n",
      "Iteration 127, loss = 0.50868136\n",
      "Iteration 128, loss = 0.50605809\n",
      "Iteration 129, loss = 0.50385223\n",
      "Iteration 130, loss = 0.50145872\n",
      "Iteration 131, loss = 0.49945565\n",
      "Iteration 132, loss = 0.49782156\n",
      "Iteration 133, loss = 0.49560489\n",
      "Iteration 134, loss = 0.49352823\n",
      "Iteration 135, loss = 0.49205127\n",
      "Iteration 136, loss = 0.49012599\n",
      "Iteration 137, loss = 0.48877729\n",
      "Iteration 138, loss = 0.48669739\n",
      "Iteration 139, loss = 0.48544391\n",
      "Iteration 140, loss = 0.48407995\n",
      "Iteration 141, loss = 0.48271529\n",
      "Iteration 142, loss = 0.48075453\n",
      "Iteration 143, loss = 0.47953640\n",
      "Iteration 144, loss = 0.47803059\n",
      "Iteration 145, loss = 0.47695173\n",
      "Iteration 146, loss = 0.47573126\n",
      "Iteration 147, loss = 0.47445762\n",
      "Iteration 148, loss = 0.47282716\n",
      "Iteration 149, loss = 0.47251710\n",
      "Iteration 150, loss = 0.47110130\n",
      "Iteration 151, loss = 0.47024907\n",
      "Iteration 152, loss = 0.46852478\n",
      "Iteration 153, loss = 0.46788420\n",
      "Iteration 154, loss = 0.46668875\n",
      "Iteration 155, loss = 0.46582774\n",
      "Iteration 156, loss = 0.46488897\n",
      "Iteration 157, loss = 0.46374770\n",
      "Iteration 158, loss = 0.46317245\n",
      "Iteration 159, loss = 0.46207060\n",
      "Iteration 160, loss = 0.46165633\n",
      "Iteration 161, loss = 0.46043699\n",
      "Iteration 162, loss = 0.45980041\n",
      "Iteration 163, loss = 0.45964007\n",
      "Iteration 164, loss = 0.45827813\n",
      "Iteration 165, loss = 0.45746619\n",
      "Iteration 166, loss = 0.45680387\n",
      "Iteration 167, loss = 0.45613810\n",
      "Iteration 168, loss = 0.45505135\n",
      "Iteration 169, loss = 0.45493507\n",
      "Iteration 170, loss = 0.45384339\n",
      "Iteration 171, loss = 0.45313373\n",
      "Iteration 172, loss = 0.45238434\n",
      "Iteration 173, loss = 0.45201887\n",
      "Iteration 174, loss = 0.45117003\n",
      "Iteration 175, loss = 0.45091003\n",
      "Iteration 176, loss = 0.44984912\n",
      "Iteration 177, loss = 0.44992285\n",
      "Iteration 178, loss = 0.44926676\n",
      "Iteration 179, loss = 0.44852555\n",
      "Iteration 180, loss = 0.44780918\n",
      "Iteration 181, loss = 0.44701553\n",
      "Iteration 182, loss = 0.44656198\n",
      "Iteration 183, loss = 0.44626687\n",
      "Iteration 184, loss = 0.44577178\n",
      "Iteration 185, loss = 0.44508481\n",
      "Iteration 186, loss = 0.44474791\n",
      "Iteration 187, loss = 0.44425941\n",
      "Iteration 188, loss = 0.44358448\n",
      "Iteration 189, loss = 0.44407519\n",
      "Iteration 190, loss = 0.44241710\n",
      "Iteration 191, loss = 0.44190095\n",
      "Iteration 192, loss = 0.44183458\n",
      "Iteration 193, loss = 0.44149221\n",
      "Iteration 194, loss = 0.44099747\n",
      "Iteration 195, loss = 0.44065927\n",
      "Iteration 196, loss = 0.43969016\n",
      "Iteration 197, loss = 0.43943501\n",
      "Iteration 198, loss = 0.43892568\n",
      "Iteration 199, loss = 0.43852761\n",
      "Iteration 200, loss = 0.43824130\n",
      "Iteration 201, loss = 0.43750588\n",
      "Iteration 202, loss = 0.43749681\n",
      "Iteration 203, loss = 0.43684938\n",
      "Iteration 204, loss = 0.43583542\n",
      "Iteration 205, loss = 0.43596913\n",
      "Iteration 206, loss = 0.43591651\n",
      "Iteration 207, loss = 0.43524084\n",
      "Iteration 208, loss = 0.43467766\n",
      "Iteration 209, loss = 0.43505786\n",
      "Iteration 210, loss = 0.43400622\n",
      "Iteration 211, loss = 0.43355003\n",
      "Iteration 212, loss = 0.43328292\n",
      "Iteration 213, loss = 0.43315955\n",
      "Iteration 214, loss = 0.43261931\n",
      "Iteration 215, loss = 0.43269076\n",
      "Iteration 216, loss = 0.43226605\n",
      "Iteration 217, loss = 0.43161847\n",
      "Iteration 218, loss = 0.43121968\n",
      "Iteration 219, loss = 0.43071671\n",
      "Iteration 220, loss = 0.43078046\n",
      "Iteration 221, loss = 0.43035370\n",
      "Iteration 222, loss = 0.42993760\n",
      "Iteration 223, loss = 0.42989216\n",
      "Iteration 224, loss = 0.42922166\n",
      "Iteration 225, loss = 0.42900475\n",
      "Iteration 226, loss = 0.42842755\n",
      "Iteration 227, loss = 0.42844503\n",
      "Iteration 228, loss = 0.42850477\n",
      "Iteration 229, loss = 0.42777791\n",
      "Iteration 230, loss = 0.42713517\n",
      "Iteration 231, loss = 0.42745996\n",
      "Iteration 232, loss = 0.42686772\n",
      "Iteration 233, loss = 0.42670812\n",
      "Iteration 234, loss = 0.42587281\n",
      "Iteration 235, loss = 0.42545704\n",
      "Iteration 236, loss = 0.42642094\n",
      "Iteration 237, loss = 0.42478716\n",
      "Iteration 238, loss = 0.42526851\n",
      "Iteration 239, loss = 0.42463622\n",
      "Iteration 240, loss = 0.42405393\n",
      "Iteration 241, loss = 0.42389080\n",
      "Iteration 242, loss = 0.42378677\n",
      "Iteration 243, loss = 0.42345302\n",
      "Iteration 244, loss = 0.42401260\n",
      "Iteration 245, loss = 0.42322976\n",
      "Iteration 246, loss = 0.42231885\n",
      "Iteration 247, loss = 0.42270892\n",
      "Iteration 248, loss = 0.42167294\n",
      "Iteration 249, loss = 0.42169307\n",
      "Iteration 250, loss = 0.42137037\n",
      "Iteration 251, loss = 0.42112793\n",
      "Iteration 252, loss = 0.42079112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.42058659\n",
      "Iteration 254, loss = 0.42067535\n",
      "Iteration 255, loss = 0.42008280\n",
      "Iteration 256, loss = 0.41966237\n",
      "Iteration 257, loss = 0.41915607\n",
      "Iteration 258, loss = 0.41931157\n",
      "Iteration 259, loss = 0.41913663\n",
      "Iteration 260, loss = 0.41867013\n",
      "Iteration 261, loss = 0.41802480\n",
      "Iteration 262, loss = 0.41793338\n",
      "Iteration 263, loss = 0.41849272\n",
      "Iteration 264, loss = 0.41771696\n",
      "Iteration 265, loss = 0.41749423\n",
      "Iteration 266, loss = 0.41781395\n",
      "Iteration 267, loss = 0.41730583\n",
      "Iteration 268, loss = 0.41631537\n",
      "Iteration 269, loss = 0.41584821\n",
      "Iteration 270, loss = 0.41590832\n",
      "Iteration 271, loss = 0.41600753\n",
      "Iteration 272, loss = 0.41521136\n",
      "Iteration 273, loss = 0.41453531\n",
      "Iteration 274, loss = 0.41539731\n",
      "Iteration 275, loss = 0.41437951\n",
      "Iteration 276, loss = 0.41423187\n",
      "Iteration 277, loss = 0.41428162\n",
      "Iteration 278, loss = 0.41350833\n",
      "Iteration 279, loss = 0.41285220\n",
      "Iteration 280, loss = 0.41370578\n",
      "Iteration 281, loss = 0.41293671\n",
      "Iteration 282, loss = 0.41212912\n",
      "Iteration 283, loss = 0.41229918\n",
      "Iteration 284, loss = 0.41223225\n",
      "Iteration 285, loss = 0.41169801\n",
      "Iteration 286, loss = 0.41210949\n",
      "Iteration 287, loss = 0.41134571\n",
      "Iteration 288, loss = 0.41132562\n",
      "Iteration 289, loss = 0.41087984\n",
      "Iteration 290, loss = 0.41078065\n",
      "Iteration 291, loss = 0.40991952\n",
      "Iteration 292, loss = 0.41021472\n",
      "Iteration 293, loss = 0.40989377\n",
      "Iteration 294, loss = 0.40960617\n",
      "Iteration 295, loss = 0.41006340\n",
      "Iteration 296, loss = 0.40885086\n",
      "Iteration 297, loss = 0.40906254\n",
      "Iteration 298, loss = 0.40857588\n",
      "Iteration 299, loss = 0.40847034\n",
      "Iteration 300, loss = 0.40924140\n",
      "Iteration 301, loss = 0.40718115\n",
      "Iteration 302, loss = 0.40817239\n",
      "Iteration 303, loss = 0.40736147\n",
      "Iteration 304, loss = 0.40736146\n",
      "Iteration 305, loss = 0.40660083\n",
      "Iteration 306, loss = 0.40658689\n",
      "Iteration 307, loss = 0.40635834\n",
      "Iteration 308, loss = 0.40682893\n",
      "Iteration 309, loss = 0.40591273\n",
      "Iteration 310, loss = 0.40516270\n",
      "Iteration 311, loss = 0.40509267\n",
      "Iteration 312, loss = 0.40532641\n",
      "Iteration 313, loss = 0.40495013\n",
      "Iteration 314, loss = 0.40462438\n",
      "Iteration 315, loss = 0.40396678\n",
      "Iteration 316, loss = 0.40383345\n",
      "Iteration 317, loss = 0.40434546\n",
      "Iteration 318, loss = 0.40382171\n",
      "Iteration 319, loss = 0.40321200\n",
      "Iteration 320, loss = 0.40311055\n",
      "Iteration 321, loss = 0.40284466\n",
      "Iteration 322, loss = 0.40230201\n",
      "Iteration 323, loss = 0.40219510\n",
      "Iteration 324, loss = 0.40253030\n",
      "Iteration 325, loss = 0.40177153\n",
      "Iteration 326, loss = 0.40141496\n",
      "Iteration 327, loss = 0.40116231\n",
      "Iteration 328, loss = 0.40138522\n",
      "Iteration 329, loss = 0.40148159\n",
      "Iteration 330, loss = 0.40144141\n",
      "Iteration 331, loss = 0.40060082\n",
      "Iteration 332, loss = 0.39979813\n",
      "Iteration 333, loss = 0.39971188\n",
      "Iteration 334, loss = 0.39935383\n",
      "Iteration 335, loss = 0.40002302\n",
      "Iteration 336, loss = 0.39939071\n",
      "Iteration 337, loss = 0.39799096\n",
      "Iteration 338, loss = 0.39832735\n",
      "Iteration 339, loss = 0.39818617\n",
      "Iteration 340, loss = 0.39795793\n",
      "Iteration 341, loss = 0.39724602\n",
      "Iteration 342, loss = 0.39828589\n",
      "Iteration 343, loss = 0.39768427\n",
      "Iteration 344, loss = 0.39667271\n",
      "Iteration 345, loss = 0.39639863\n",
      "Iteration 346, loss = 0.39622676\n",
      "Iteration 347, loss = 0.39586977\n",
      "Iteration 348, loss = 0.39606310\n",
      "Iteration 349, loss = 0.39523902\n",
      "Iteration 350, loss = 0.39531101\n",
      "Iteration 351, loss = 0.39469746\n",
      "Iteration 352, loss = 0.39488898\n",
      "Iteration 353, loss = 0.39399174\n",
      "Iteration 354, loss = 0.39393459\n",
      "Iteration 355, loss = 0.39358253\n",
      "Iteration 356, loss = 0.39419778\n",
      "Iteration 357, loss = 0.39358773\n",
      "Iteration 358, loss = 0.39266364\n",
      "Iteration 359, loss = 0.39262566\n",
      "Iteration 360, loss = 0.39219707\n",
      "Iteration 361, loss = 0.39258493\n",
      "Iteration 362, loss = 0.39182568\n",
      "Iteration 363, loss = 0.39192263\n",
      "Iteration 364, loss = 0.39155518\n",
      "Iteration 365, loss = 0.39101823\n",
      "Iteration 366, loss = 0.39120367\n",
      "Iteration 367, loss = 0.39011082\n",
      "Iteration 368, loss = 0.38999485\n",
      "Iteration 369, loss = 0.38944736\n",
      "Iteration 370, loss = 0.38936381\n",
      "Iteration 371, loss = 0.38921130\n",
      "Iteration 372, loss = 0.38924670\n",
      "Iteration 373, loss = 0.38899705\n",
      "Iteration 374, loss = 0.38861358\n",
      "Iteration 375, loss = 0.38799214\n",
      "Iteration 376, loss = 0.38752536\n",
      "Iteration 377, loss = 0.38780611\n",
      "Iteration 378, loss = 0.38715447\n",
      "Iteration 379, loss = 0.38675057\n",
      "Iteration 380, loss = 0.38702229\n",
      "Iteration 381, loss = 0.38737989\n",
      "Iteration 382, loss = 0.38653179\n",
      "Iteration 383, loss = 0.38593331\n",
      "Iteration 384, loss = 0.38536436\n",
      "Iteration 385, loss = 0.38483022\n",
      "Iteration 386, loss = 0.38473653\n",
      "Iteration 387, loss = 0.38454673\n",
      "Iteration 388, loss = 0.38469449\n",
      "Iteration 389, loss = 0.38403343\n",
      "Iteration 390, loss = 0.38370661\n",
      "Iteration 391, loss = 0.38366518\n",
      "Iteration 392, loss = 0.38300544\n",
      "Iteration 393, loss = 0.38230258\n",
      "Iteration 394, loss = 0.38252068\n",
      "Iteration 395, loss = 0.38252096\n",
      "Iteration 396, loss = 0.38304127\n",
      "Iteration 397, loss = 0.38253021\n",
      "Iteration 398, loss = 0.38155487\n",
      "Iteration 399, loss = 0.38154167\n",
      "Iteration 400, loss = 0.38086940\n",
      "Iteration 401, loss = 0.38061601\n",
      "Iteration 402, loss = 0.37973977\n",
      "Iteration 403, loss = 0.37935387\n",
      "Iteration 404, loss = 0.37950576\n",
      "Iteration 405, loss = 0.37883651\n",
      "Iteration 406, loss = 0.37847279\n",
      "Iteration 407, loss = 0.37915850\n",
      "Iteration 408, loss = 0.37799191\n",
      "Iteration 409, loss = 0.37758367\n",
      "Iteration 410, loss = 0.37730641\n",
      "Iteration 411, loss = 0.37672817\n",
      "Iteration 412, loss = 0.37668248\n",
      "Iteration 413, loss = 0.37612436\n",
      "Iteration 414, loss = 0.37602707\n",
      "Iteration 415, loss = 0.37554814\n",
      "Iteration 416, loss = 0.37608783\n",
      "Iteration 417, loss = 0.37523435\n",
      "Iteration 418, loss = 0.37509080\n",
      "Iteration 419, loss = 0.37447179\n",
      "Iteration 420, loss = 0.37360683\n",
      "Iteration 421, loss = 0.37322697\n",
      "Iteration 422, loss = 0.37299506\n",
      "Iteration 423, loss = 0.37258903\n",
      "Iteration 424, loss = 0.37266945\n",
      "Iteration 425, loss = 0.37245090\n",
      "Iteration 426, loss = 0.37173160\n",
      "Iteration 427, loss = 0.37167873\n",
      "Iteration 428, loss = 0.37135363\n",
      "Iteration 429, loss = 0.37086413\n",
      "Iteration 430, loss = 0.37016449\n",
      "Iteration 431, loss = 0.37002504\n",
      "Iteration 432, loss = 0.36962620\n",
      "Iteration 433, loss = 0.36934923\n",
      "Iteration 434, loss = 0.36959114\n",
      "Iteration 435, loss = 0.36867948\n",
      "Iteration 436, loss = 0.36800838\n",
      "Iteration 437, loss = 0.36801781\n",
      "Iteration 438, loss = 0.36766595\n",
      "Iteration 439, loss = 0.36781086\n",
      "Iteration 440, loss = 0.36658693\n",
      "Iteration 441, loss = 0.36600253\n",
      "Iteration 442, loss = 0.36564729\n",
      "Iteration 443, loss = 0.36529156\n",
      "Iteration 444, loss = 0.36511966\n",
      "Iteration 445, loss = 0.36508691\n",
      "Iteration 446, loss = 0.36463966\n",
      "Iteration 447, loss = 0.36427731\n",
      "Iteration 448, loss = 0.36393513\n",
      "Iteration 449, loss = 0.36345030\n",
      "Iteration 450, loss = 0.36279729\n",
      "Iteration 451, loss = 0.36208469\n",
      "Iteration 452, loss = 0.36131041\n",
      "Iteration 453, loss = 0.36106816\n",
      "Iteration 454, loss = 0.36099491\n",
      "Iteration 455, loss = 0.36107702\n",
      "Iteration 456, loss = 0.35958337\n",
      "Iteration 457, loss = 0.35936817\n",
      "Iteration 458, loss = 0.35912454\n",
      "Iteration 459, loss = 0.35882449\n",
      "Iteration 460, loss = 0.35873002\n",
      "Iteration 461, loss = 0.35833753\n",
      "Iteration 462, loss = 0.35786617\n",
      "Iteration 463, loss = 0.35719305\n",
      "Iteration 464, loss = 0.35683721\n",
      "Iteration 465, loss = 0.35633581\n",
      "Iteration 466, loss = 0.35567149\n",
      "Iteration 467, loss = 0.35507387\n",
      "Iteration 468, loss = 0.35575722\n",
      "Iteration 469, loss = 0.35474260\n",
      "Iteration 470, loss = 0.35365945\n",
      "Iteration 471, loss = 0.35475803\n",
      "Iteration 472, loss = 0.35338819\n",
      "Iteration 473, loss = 0.35248938\n",
      "Iteration 474, loss = 0.35236513\n",
      "Iteration 475, loss = 0.35225157\n",
      "Iteration 476, loss = 0.35094867\n",
      "Iteration 477, loss = 0.35022667\n",
      "Iteration 478, loss = 0.35033922\n",
      "Iteration 479, loss = 0.34923992\n",
      "Iteration 480, loss = 0.34918148\n",
      "Iteration 481, loss = 0.34854208\n",
      "Iteration 482, loss = 0.34860327\n",
      "Iteration 483, loss = 0.34835580\n",
      "Iteration 484, loss = 0.34834569\n",
      "Iteration 485, loss = 0.34725879\n",
      "Iteration 486, loss = 0.34611945\n",
      "Iteration 487, loss = 0.34575679\n",
      "Iteration 488, loss = 0.34503283\n",
      "Iteration 489, loss = 0.34502155\n",
      "Iteration 490, loss = 0.34596515\n",
      "Iteration 491, loss = 0.34375969\n",
      "Iteration 492, loss = 0.34354995\n",
      "Iteration 493, loss = 0.34383562\n",
      "Iteration 494, loss = 0.34215583\n",
      "Iteration 495, loss = 0.34282897\n",
      "Iteration 496, loss = 0.34154410\n",
      "Iteration 497, loss = 0.34065158\n",
      "Iteration 498, loss = 0.34087726\n",
      "Iteration 499, loss = 0.34058481\n",
      "Iteration 500, loss = 0.33946063\n"
     ]
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)\n",
    "y_pred = clf_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78      1541\n",
      "           1       0.80      0.77      0.79      1646\n",
      "\n",
      "    accuracy                           0.78      3187\n",
      "   macro avg       0.78      0.78      0.78      3187\n",
      "weighted avg       0.78      0.78      0.78      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:13, 30047.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embeddings_index = {}\n",
    "f = open('GLOVE_models/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_glove = [sent2vec(x) for x in df_bin['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_glove = np.array(features_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(features_glove, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_X_train, glove_X_test, glove_y_train, glove_y_test = train_test_split(features_glove, glove_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7413726250484683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      1625\n",
      "           1       0.67      0.60      0.63       954\n",
      "\n",
      "    accuracy                           0.74      2579\n",
      "   macro avg       0.72      0.71      0.72      2579\n",
      "weighted avg       0.74      0.74      0.74      2579\n",
      "\n",
      "Accuracy for RandomForest is 0.7227607599844901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.90      0.80      1625\n",
      "           1       0.71      0.42      0.53       954\n",
      "\n",
      "    accuracy                           0.72      2579\n",
      "   macro avg       0.72      0.66      0.67      2579\n",
      "weighted avg       0.72      0.72      0.70      2579\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7103528499418379\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80      1625\n",
      "           1       0.71      0.37      0.49       954\n",
      "\n",
      "    accuracy                           0.71      2579\n",
      "   macro avg       0.71      0.64      0.64      2579\n",
      "weighted avg       0.71      0.71      0.68      2579\n",
      "\n",
      "Accuracy for XGBoost is 0.7340054284606436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80      1625\n",
      "           1       0.68      0.54      0.60       954\n",
      "\n",
      "    accuracy                           0.73      2579\n",
      "   macro avg       0.72      0.69      0.70      2579\n",
      "weighted avg       0.73      0.73      0.73      2579\n",
      "\n",
      "Accuracy for AdaBoost is 0.7126793330748352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78      1625\n",
      "           1       0.62      0.58      0.60       954\n",
      "\n",
      "    accuracy                           0.71      2579\n",
      "   macro avg       0.69      0.69      0.69      2579\n",
      "weighted avg       0.71      0.71      0.71      2579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(glove_X_train, glove_X_test, glove_y_train, glove_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LinearSVC is 0.7345465955443992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.74      1646\n",
      "           1       0.72      0.75      0.73      1541\n",
      "\n",
      "    accuracy                           0.73      3187\n",
      "   macro avg       0.73      0.74      0.73      3187\n",
      "weighted avg       0.74      0.73      0.73      3187\n",
      "\n",
      "Accuracy for RandomForest is 0.7885158456228428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1646\n",
      "           1       0.79      0.77      0.78      1541\n",
      "\n",
      "    accuracy                           0.79      3187\n",
      "   macro avg       0.79      0.79      0.79      3187\n",
      "weighted avg       0.79      0.79      0.79      3187\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7941637903984938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1646\n",
      "           1       0.80      0.77      0.78      1541\n",
      "\n",
      "    accuracy                           0.79      3187\n",
      "   macro avg       0.79      0.79      0.79      3187\n",
      "weighted avg       0.79      0.79      0.79      3187\n",
      "\n",
      "Accuracy for XGBoost is 0.7317226231565735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73      1646\n",
      "           1       0.70      0.77      0.73      1541\n",
      "\n",
      "    accuracy                           0.73      3187\n",
      "   macro avg       0.73      0.73      0.73      3187\n",
      "weighted avg       0.73      0.73      0.73      3187\n",
      "\n",
      "Accuracy for AdaBoost is 0.700972701600251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70      1646\n",
      "           1       0.68      0.72      0.70      1541\n",
      "\n",
      "    accuracy                           0.70      3187\n",
      "   macro avg       0.70      0.70      0.70      3187\n",
      "weighted avg       0.70      0.70      0.70      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69334470\n",
      "Iteration 2, loss = 0.69284921\n",
      "Iteration 3, loss = 0.69242053\n",
      "Iteration 4, loss = 0.69210649\n",
      "Iteration 5, loss = 0.69182566\n",
      "Iteration 6, loss = 0.69155366\n",
      "Iteration 7, loss = 0.69130821\n",
      "Iteration 8, loss = 0.69104309\n",
      "Iteration 9, loss = 0.69079725\n",
      "Iteration 10, loss = 0.69057490\n",
      "Iteration 11, loss = 0.69030499\n",
      "Iteration 12, loss = 0.69006230\n",
      "Iteration 13, loss = 0.68983403\n",
      "Iteration 14, loss = 0.68959664\n",
      "Iteration 15, loss = 0.68933988\n",
      "Iteration 16, loss = 0.68907521\n",
      "Iteration 17, loss = 0.68883090\n",
      "Iteration 18, loss = 0.68856440\n",
      "Iteration 19, loss = 0.68829777\n",
      "Iteration 20, loss = 0.68802631\n",
      "Iteration 21, loss = 0.68775715\n",
      "Iteration 22, loss = 0.68746857\n",
      "Iteration 23, loss = 0.68718208\n",
      "Iteration 24, loss = 0.68690651\n",
      "Iteration 25, loss = 0.68657928\n",
      "Iteration 26, loss = 0.68627364\n",
      "Iteration 27, loss = 0.68598493\n",
      "Iteration 28, loss = 0.68563723\n",
      "Iteration 29, loss = 0.68524033\n",
      "Iteration 30, loss = 0.68488726\n",
      "Iteration 31, loss = 0.68448887\n",
      "Iteration 32, loss = 0.68411564\n",
      "Iteration 33, loss = 0.68374907\n",
      "Iteration 34, loss = 0.68327406\n",
      "Iteration 35, loss = 0.68284542\n",
      "Iteration 36, loss = 0.68238561\n",
      "Iteration 37, loss = 0.68192528\n",
      "Iteration 38, loss = 0.68145364\n",
      "Iteration 39, loss = 0.68094044\n",
      "Iteration 40, loss = 0.68042871\n",
      "Iteration 41, loss = 0.67991498\n",
      "Iteration 42, loss = 0.67936301\n",
      "Iteration 43, loss = 0.67880904\n",
      "Iteration 44, loss = 0.67823966\n",
      "Iteration 45, loss = 0.67764971\n",
      "Iteration 46, loss = 0.67696786\n",
      "Iteration 47, loss = 0.67634130\n",
      "Iteration 48, loss = 0.67567946\n",
      "Iteration 49, loss = 0.67497482\n",
      "Iteration 50, loss = 0.67423634\n",
      "Iteration 51, loss = 0.67346991\n",
      "Iteration 52, loss = 0.67269449\n",
      "Iteration 53, loss = 0.67190620\n",
      "Iteration 54, loss = 0.67100733\n",
      "Iteration 55, loss = 0.67011343\n",
      "Iteration 56, loss = 0.66921147\n",
      "Iteration 57, loss = 0.66825931\n",
      "Iteration 58, loss = 0.66724787\n",
      "Iteration 59, loss = 0.66621054\n",
      "Iteration 60, loss = 0.66514603\n",
      "Iteration 61, loss = 0.66403053\n",
      "Iteration 62, loss = 0.66286400\n",
      "Iteration 63, loss = 0.66173649\n",
      "Iteration 64, loss = 0.66044178\n",
      "Iteration 65, loss = 0.65921134\n",
      "Iteration 66, loss = 0.65788294\n",
      "Iteration 67, loss = 0.65652891\n",
      "Iteration 68, loss = 0.65508686\n",
      "Iteration 69, loss = 0.65363848\n",
      "Iteration 70, loss = 0.65221843\n",
      "Iteration 71, loss = 0.65061470\n",
      "Iteration 72, loss = 0.64892591\n",
      "Iteration 73, loss = 0.64727287\n",
      "Iteration 74, loss = 0.64557382\n",
      "Iteration 75, loss = 0.64372880\n",
      "Iteration 76, loss = 0.64188157\n",
      "Iteration 77, loss = 0.64012777\n",
      "Iteration 78, loss = 0.63799227\n",
      "Iteration 79, loss = 0.63582548\n",
      "Iteration 80, loss = 0.63373049\n",
      "Iteration 81, loss = 0.63160661\n",
      "Iteration 82, loss = 0.62931701\n",
      "Iteration 83, loss = 0.62715553\n",
      "Iteration 84, loss = 0.62473824\n",
      "Iteration 85, loss = 0.62232761\n",
      "Iteration 86, loss = 0.61985170\n",
      "Iteration 87, loss = 0.61750395\n",
      "Iteration 88, loss = 0.61494923\n",
      "Iteration 89, loss = 0.61250142\n",
      "Iteration 90, loss = 0.60997203\n",
      "Iteration 91, loss = 0.60741096\n",
      "Iteration 92, loss = 0.60486532\n",
      "Iteration 93, loss = 0.60230179\n",
      "Iteration 94, loss = 0.59977609\n",
      "Iteration 95, loss = 0.59722672\n",
      "Iteration 96, loss = 0.59466000\n",
      "Iteration 97, loss = 0.59216553\n",
      "Iteration 98, loss = 0.58959073\n",
      "Iteration 99, loss = 0.58711868\n",
      "Iteration 100, loss = 0.58461066\n",
      "Iteration 101, loss = 0.58223698\n",
      "Iteration 102, loss = 0.57986455\n",
      "Iteration 103, loss = 0.57744853\n",
      "Iteration 104, loss = 0.57505274\n",
      "Iteration 105, loss = 0.57303814\n",
      "Iteration 106, loss = 0.57062501\n",
      "Iteration 107, loss = 0.56843722\n",
      "Iteration 108, loss = 0.56632384\n",
      "Iteration 109, loss = 0.56412843\n",
      "Iteration 110, loss = 0.56226742\n",
      "Iteration 111, loss = 0.56054708\n",
      "Iteration 112, loss = 0.55854953\n",
      "Iteration 113, loss = 0.55671513\n",
      "Iteration 114, loss = 0.55526691\n",
      "Iteration 115, loss = 0.55311688\n",
      "Iteration 116, loss = 0.55195997\n",
      "Iteration 117, loss = 0.55021929\n",
      "Iteration 118, loss = 0.54868704\n",
      "Iteration 119, loss = 0.54728225\n",
      "Iteration 120, loss = 0.54549833\n",
      "Iteration 121, loss = 0.54413437\n",
      "Iteration 122, loss = 0.54318378\n",
      "Iteration 123, loss = 0.54157082\n",
      "Iteration 124, loss = 0.54036274\n",
      "Iteration 125, loss = 0.53913969\n",
      "Iteration 126, loss = 0.53812504\n",
      "Iteration 127, loss = 0.53692182\n",
      "Iteration 128, loss = 0.53612380\n",
      "Iteration 129, loss = 0.53502109\n",
      "Iteration 130, loss = 0.53384782\n",
      "Iteration 131, loss = 0.53294568\n",
      "Iteration 132, loss = 0.53211376\n",
      "Iteration 133, loss = 0.53121741\n",
      "Iteration 134, loss = 0.53056991\n",
      "Iteration 135, loss = 0.52966520\n",
      "Iteration 136, loss = 0.52877977\n",
      "Iteration 137, loss = 0.52809514\n",
      "Iteration 138, loss = 0.52724849\n",
      "Iteration 139, loss = 0.52634986\n",
      "Iteration 140, loss = 0.52572499\n",
      "Iteration 141, loss = 0.52492720\n",
      "Iteration 142, loss = 0.52428017\n",
      "Iteration 143, loss = 0.52411631\n",
      "Iteration 144, loss = 0.52299818\n",
      "Iteration 145, loss = 0.52253957\n",
      "Iteration 146, loss = 0.52160847\n",
      "Iteration 147, loss = 0.52105346\n",
      "Iteration 148, loss = 0.52074921\n",
      "Iteration 149, loss = 0.51997811\n",
      "Iteration 150, loss = 0.51943574\n",
      "Iteration 151, loss = 0.51911783\n",
      "Iteration 152, loss = 0.51828386\n",
      "Iteration 153, loss = 0.51786945\n",
      "Iteration 154, loss = 0.51721473\n",
      "Iteration 155, loss = 0.51645895\n",
      "Iteration 156, loss = 0.51691321\n",
      "Iteration 157, loss = 0.51607362\n",
      "Iteration 158, loss = 0.51553134\n",
      "Iteration 159, loss = 0.51503124\n",
      "Iteration 160, loss = 0.51455678\n",
      "Iteration 161, loss = 0.51414008\n",
      "Iteration 162, loss = 0.51356251\n",
      "Iteration 163, loss = 0.51300890\n",
      "Iteration 164, loss = 0.51272551\n",
      "Iteration 165, loss = 0.51269552\n",
      "Iteration 166, loss = 0.51201142\n",
      "Iteration 167, loss = 0.51155038\n",
      "Iteration 168, loss = 0.51140004\n",
      "Iteration 169, loss = 0.51066720\n",
      "Iteration 170, loss = 0.51044019\n",
      "Iteration 171, loss = 0.51013225\n",
      "Iteration 172, loss = 0.50984272\n",
      "Iteration 173, loss = 0.50957134\n",
      "Iteration 174, loss = 0.50904772\n",
      "Iteration 175, loss = 0.50897771\n",
      "Iteration 176, loss = 0.50831340\n",
      "Iteration 177, loss = 0.50834166\n",
      "Iteration 178, loss = 0.50822052\n",
      "Iteration 179, loss = 0.50734906\n",
      "Iteration 180, loss = 0.50726561\n",
      "Iteration 181, loss = 0.50719273\n",
      "Iteration 182, loss = 0.50662039\n",
      "Iteration 183, loss = 0.50624085\n",
      "Iteration 184, loss = 0.50604165\n",
      "Iteration 185, loss = 0.50606009\n",
      "Iteration 186, loss = 0.50547079\n",
      "Iteration 187, loss = 0.50535026\n",
      "Iteration 188, loss = 0.50522718\n",
      "Iteration 189, loss = 0.50477104\n",
      "Iteration 190, loss = 0.50460276\n",
      "Iteration 191, loss = 0.50447102\n",
      "Iteration 192, loss = 0.50404443\n",
      "Iteration 193, loss = 0.50404740\n",
      "Iteration 194, loss = 0.50339032\n",
      "Iteration 195, loss = 0.50326439\n",
      "Iteration 196, loss = 0.50296777\n",
      "Iteration 197, loss = 0.50300066\n",
      "Iteration 198, loss = 0.50261115\n",
      "Iteration 199, loss = 0.50253092\n",
      "Iteration 200, loss = 0.50232222\n",
      "Iteration 201, loss = 0.50217001\n",
      "Iteration 202, loss = 0.50140326\n",
      "Iteration 203, loss = 0.50165831\n",
      "Iteration 204, loss = 0.50122717\n",
      "Iteration 205, loss = 0.50107353\n",
      "Iteration 206, loss = 0.50084965\n",
      "Iteration 207, loss = 0.50073801\n",
      "Iteration 208, loss = 0.50050028\n",
      "Iteration 209, loss = 0.50039482\n",
      "Iteration 210, loss = 0.50037336\n",
      "Iteration 211, loss = 0.49998169\n",
      "Iteration 212, loss = 0.50021873\n",
      "Iteration 213, loss = 0.49977445\n",
      "Iteration 214, loss = 0.49916632\n",
      "Iteration 215, loss = 0.49907438\n",
      "Iteration 216, loss = 0.49907136\n",
      "Iteration 217, loss = 0.49860935\n",
      "Iteration 218, loss = 0.49853312\n",
      "Iteration 219, loss = 0.49865712\n",
      "Iteration 220, loss = 0.49832418\n",
      "Iteration 221, loss = 0.49789586\n",
      "Iteration 222, loss = 0.49774968\n",
      "Iteration 223, loss = 0.49741888\n",
      "Iteration 224, loss = 0.49724063\n",
      "Iteration 225, loss = 0.49733677\n",
      "Iteration 226, loss = 0.49764968\n",
      "Iteration 227, loss = 0.49673136\n",
      "Iteration 228, loss = 0.49671057\n",
      "Iteration 229, loss = 0.49637448\n",
      "Iteration 230, loss = 0.49697045\n",
      "Iteration 231, loss = 0.49611306\n",
      "Iteration 232, loss = 0.49588453\n",
      "Iteration 233, loss = 0.49596150\n",
      "Iteration 234, loss = 0.49590800\n",
      "Iteration 235, loss = 0.49537166\n",
      "Iteration 236, loss = 0.49513680\n",
      "Iteration 237, loss = 0.49548005\n",
      "Iteration 238, loss = 0.49513028\n",
      "Iteration 239, loss = 0.49464299\n",
      "Iteration 240, loss = 0.49476188\n",
      "Iteration 241, loss = 0.49445762\n",
      "Iteration 242, loss = 0.49403651\n",
      "Iteration 243, loss = 0.49380353\n",
      "Iteration 244, loss = 0.49452546\n",
      "Iteration 245, loss = 0.49392704\n",
      "Iteration 246, loss = 0.49375793\n",
      "Iteration 247, loss = 0.49338471\n",
      "Iteration 248, loss = 0.49327685\n",
      "Iteration 249, loss = 0.49317732\n",
      "Iteration 250, loss = 0.49312668\n",
      "Iteration 251, loss = 0.49240962\n",
      "Iteration 252, loss = 0.49316796\n",
      "Iteration 253, loss = 0.49266475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.49241162\n",
      "Iteration 255, loss = 0.49232593\n",
      "Iteration 256, loss = 0.49197606\n",
      "Iteration 257, loss = 0.49189231\n",
      "Iteration 258, loss = 0.49165263\n",
      "Iteration 259, loss = 0.49142434\n",
      "Iteration 260, loss = 0.49119415\n",
      "Iteration 261, loss = 0.49136570\n",
      "Iteration 262, loss = 0.49069245\n",
      "Iteration 263, loss = 0.49092034\n",
      "Iteration 264, loss = 0.49049738\n",
      "Iteration 265, loss = 0.49047861\n",
      "Iteration 266, loss = 0.49049684\n",
      "Iteration 267, loss = 0.49038791\n",
      "Iteration 268, loss = 0.49009741\n",
      "Iteration 269, loss = 0.48964141\n",
      "Iteration 270, loss = 0.49039518\n",
      "Iteration 271, loss = 0.48971667\n",
      "Iteration 272, loss = 0.48934500\n",
      "Iteration 273, loss = 0.48924268\n",
      "Iteration 274, loss = 0.48886024\n",
      "Iteration 275, loss = 0.48931749\n",
      "Iteration 276, loss = 0.48903328\n",
      "Iteration 277, loss = 0.48844644\n",
      "Iteration 278, loss = 0.48836540\n",
      "Iteration 279, loss = 0.48821712\n",
      "Iteration 280, loss = 0.48791495\n",
      "Iteration 281, loss = 0.48861138\n",
      "Iteration 282, loss = 0.48765460\n",
      "Iteration 283, loss = 0.48808007\n",
      "Iteration 284, loss = 0.48743693\n",
      "Iteration 285, loss = 0.48718481\n",
      "Iteration 286, loss = 0.48776244\n",
      "Iteration 287, loss = 0.48675897\n",
      "Iteration 288, loss = 0.48712615\n",
      "Iteration 289, loss = 0.48713008\n",
      "Iteration 290, loss = 0.48684211\n",
      "Iteration 291, loss = 0.48642711\n",
      "Iteration 292, loss = 0.48629828\n",
      "Iteration 293, loss = 0.48597423\n",
      "Iteration 294, loss = 0.48582755\n",
      "Iteration 295, loss = 0.48552534\n",
      "Iteration 296, loss = 0.48549142\n",
      "Iteration 297, loss = 0.48523924\n",
      "Iteration 298, loss = 0.48514122\n",
      "Iteration 299, loss = 0.48569076\n",
      "Iteration 300, loss = 0.48503634\n",
      "Iteration 301, loss = 0.48470215\n",
      "Iteration 302, loss = 0.48446180\n",
      "Iteration 303, loss = 0.48476297\n",
      "Iteration 304, loss = 0.48445541\n",
      "Iteration 305, loss = 0.48440522\n",
      "Iteration 306, loss = 0.48376656\n",
      "Iteration 307, loss = 0.48414801\n",
      "Iteration 308, loss = 0.48370552\n",
      "Iteration 309, loss = 0.48353503\n",
      "Iteration 310, loss = 0.48357999\n",
      "Iteration 311, loss = 0.48300993\n",
      "Iteration 312, loss = 0.48305325\n",
      "Iteration 313, loss = 0.48264705\n",
      "Iteration 314, loss = 0.48259769\n",
      "Iteration 315, loss = 0.48234750\n",
      "Iteration 316, loss = 0.48241175\n",
      "Iteration 317, loss = 0.48190926\n",
      "Iteration 318, loss = 0.48245611\n",
      "Iteration 319, loss = 0.48177643\n",
      "Iteration 320, loss = 0.48191478\n",
      "Iteration 321, loss = 0.48149883\n",
      "Iteration 322, loss = 0.48118873\n",
      "Iteration 323, loss = 0.48131638\n",
      "Iteration 324, loss = 0.48065516\n",
      "Iteration 325, loss = 0.48075038\n",
      "Iteration 326, loss = 0.48105195\n",
      "Iteration 327, loss = 0.48049494\n",
      "Iteration 328, loss = 0.48118263\n",
      "Iteration 329, loss = 0.48004739\n",
      "Iteration 330, loss = 0.48014716\n",
      "Iteration 331, loss = 0.47972467\n",
      "Iteration 332, loss = 0.47952502\n",
      "Iteration 333, loss = 0.47960073\n",
      "Iteration 334, loss = 0.47962993\n",
      "Iteration 335, loss = 0.47932491\n",
      "Iteration 336, loss = 0.47958602\n",
      "Iteration 337, loss = 0.47892379\n",
      "Iteration 338, loss = 0.47872651\n",
      "Iteration 339, loss = 0.47817480\n",
      "Iteration 340, loss = 0.47792660\n",
      "Iteration 341, loss = 0.47837854\n",
      "Iteration 342, loss = 0.47838456\n",
      "Iteration 343, loss = 0.47769468\n",
      "Iteration 344, loss = 0.47768857\n",
      "Iteration 345, loss = 0.47742584\n",
      "Iteration 346, loss = 0.47753052\n",
      "Iteration 347, loss = 0.47684877\n",
      "Iteration 348, loss = 0.47670884\n",
      "Iteration 349, loss = 0.47660371\n",
      "Iteration 350, loss = 0.47651174\n",
      "Iteration 351, loss = 0.47669358\n",
      "Iteration 352, loss = 0.47629806\n",
      "Iteration 353, loss = 0.47624149\n",
      "Iteration 354, loss = 0.47599878\n",
      "Iteration 355, loss = 0.47583709\n",
      "Iteration 356, loss = 0.47522213\n",
      "Iteration 357, loss = 0.47558052\n",
      "Iteration 358, loss = 0.47485381\n",
      "Iteration 359, loss = 0.47457922\n",
      "Iteration 360, loss = 0.47525251\n",
      "Iteration 361, loss = 0.47491504\n",
      "Iteration 362, loss = 0.47425066\n",
      "Iteration 363, loss = 0.47388505\n",
      "Iteration 364, loss = 0.47386453\n",
      "Iteration 365, loss = 0.47335229\n",
      "Iteration 366, loss = 0.47383234\n",
      "Iteration 367, loss = 0.47381427\n",
      "Iteration 368, loss = 0.47291177\n",
      "Iteration 369, loss = 0.47255907\n",
      "Iteration 370, loss = 0.47301911\n",
      "Iteration 371, loss = 0.47278644\n",
      "Iteration 372, loss = 0.47200945\n",
      "Iteration 373, loss = 0.47179528\n",
      "Iteration 374, loss = 0.47164362\n",
      "Iteration 375, loss = 0.47149070\n",
      "Iteration 376, loss = 0.47109748\n",
      "Iteration 377, loss = 0.47142057\n",
      "Iteration 378, loss = 0.47111864\n",
      "Iteration 379, loss = 0.47074481\n",
      "Iteration 380, loss = 0.47048411\n",
      "Iteration 381, loss = 0.47040922\n",
      "Iteration 382, loss = 0.46999880\n",
      "Iteration 383, loss = 0.46977997\n",
      "Iteration 384, loss = 0.46975024\n",
      "Iteration 385, loss = 0.47077439\n",
      "Iteration 386, loss = 0.46938500\n",
      "Iteration 387, loss = 0.46920549\n",
      "Iteration 388, loss = 0.46876976\n",
      "Iteration 389, loss = 0.46815215\n",
      "Iteration 390, loss = 0.46886787\n",
      "Iteration 391, loss = 0.46904718\n",
      "Iteration 392, loss = 0.46767801\n",
      "Iteration 393, loss = 0.46756785\n",
      "Iteration 394, loss = 0.46839584\n",
      "Iteration 395, loss = 0.46722550\n",
      "Iteration 396, loss = 0.46699113\n",
      "Iteration 397, loss = 0.46656054\n",
      "Iteration 398, loss = 0.46684931\n",
      "Iteration 399, loss = 0.46695945\n",
      "Iteration 400, loss = 0.46622500\n",
      "Iteration 401, loss = 0.46691979\n",
      "Iteration 402, loss = 0.46552000\n",
      "Iteration 403, loss = 0.46532759\n",
      "Iteration 404, loss = 0.46617970\n",
      "Iteration 405, loss = 0.46523610\n",
      "Iteration 406, loss = 0.46503896\n",
      "Iteration 407, loss = 0.46450751\n",
      "Iteration 408, loss = 0.46477641\n",
      "Iteration 409, loss = 0.46391064\n",
      "Iteration 410, loss = 0.46381937\n",
      "Iteration 411, loss = 0.46384839\n",
      "Iteration 412, loss = 0.46331618\n",
      "Iteration 413, loss = 0.46309806\n",
      "Iteration 414, loss = 0.46269636\n",
      "Iteration 415, loss = 0.46276812\n",
      "Iteration 416, loss = 0.46218608\n",
      "Iteration 417, loss = 0.46205680\n",
      "Iteration 418, loss = 0.46186749\n",
      "Iteration 419, loss = 0.46160273\n",
      "Iteration 420, loss = 0.46157307\n",
      "Iteration 421, loss = 0.46073085\n",
      "Iteration 422, loss = 0.46129664\n",
      "Iteration 423, loss = 0.46068872\n",
      "Iteration 424, loss = 0.46077369\n",
      "Iteration 425, loss = 0.45960245\n",
      "Iteration 426, loss = 0.45980556\n",
      "Iteration 427, loss = 0.45999037\n",
      "Iteration 428, loss = 0.45932364\n",
      "Iteration 429, loss = 0.45988689\n",
      "Iteration 430, loss = 0.45948059\n",
      "Iteration 431, loss = 0.45869552\n",
      "Iteration 432, loss = 0.45824895\n",
      "Iteration 433, loss = 0.45768417\n",
      "Iteration 434, loss = 0.45782734\n",
      "Iteration 435, loss = 0.45751514\n",
      "Iteration 436, loss = 0.45645178\n",
      "Iteration 437, loss = 0.45730974\n",
      "Iteration 438, loss = 0.45672321\n",
      "Iteration 439, loss = 0.45652623\n",
      "Iteration 440, loss = 0.45594870\n",
      "Iteration 441, loss = 0.45625349\n",
      "Iteration 442, loss = 0.45573849\n",
      "Iteration 443, loss = 0.45560106\n",
      "Iteration 444, loss = 0.45487897\n",
      "Iteration 445, loss = 0.45453061\n",
      "Iteration 446, loss = 0.45383439\n",
      "Iteration 447, loss = 0.45365071\n",
      "Iteration 448, loss = 0.45369344\n",
      "Iteration 449, loss = 0.45332603\n",
      "Iteration 450, loss = 0.45313902\n",
      "Iteration 451, loss = 0.45260199\n",
      "Iteration 452, loss = 0.45217983\n",
      "Iteration 453, loss = 0.45229019\n",
      "Iteration 454, loss = 0.45187233\n",
      "Iteration 455, loss = 0.45125349\n",
      "Iteration 456, loss = 0.45116496\n",
      "Iteration 457, loss = 0.45061482\n",
      "Iteration 458, loss = 0.45101260\n",
      "Iteration 459, loss = 0.44997806\n",
      "Iteration 460, loss = 0.45012105\n",
      "Iteration 461, loss = 0.44995626\n",
      "Iteration 462, loss = 0.44931260\n",
      "Iteration 463, loss = 0.45024337\n",
      "Iteration 464, loss = 0.44811216\n",
      "Iteration 465, loss = 0.44857225\n",
      "Iteration 466, loss = 0.44744315\n",
      "Iteration 467, loss = 0.44753800\n",
      "Iteration 468, loss = 0.44696313\n",
      "Iteration 469, loss = 0.44654075\n",
      "Iteration 470, loss = 0.44648878\n",
      "Iteration 471, loss = 0.44636487\n",
      "Iteration 472, loss = 0.44653143\n",
      "Iteration 473, loss = 0.44594818\n",
      "Iteration 474, loss = 0.44500318\n",
      "Iteration 475, loss = 0.44455963\n",
      "Iteration 476, loss = 0.44397750\n",
      "Iteration 477, loss = 0.44367700\n",
      "Iteration 478, loss = 0.44446878\n",
      "Iteration 479, loss = 0.44347432\n",
      "Iteration 480, loss = 0.44240908\n",
      "Iteration 481, loss = 0.44282574\n",
      "Iteration 482, loss = 0.44257882\n",
      "Iteration 483, loss = 0.44158725\n",
      "Iteration 484, loss = 0.44156061\n",
      "Iteration 485, loss = 0.44068723\n",
      "Iteration 486, loss = 0.44094159\n",
      "Iteration 487, loss = 0.44011593\n",
      "Iteration 488, loss = 0.44016376\n",
      "Iteration 489, loss = 0.43957341\n",
      "Iteration 490, loss = 0.43868109\n",
      "Iteration 491, loss = 0.43939706\n",
      "Iteration 492, loss = 0.43817089\n",
      "Iteration 493, loss = 0.43772185\n",
      "Iteration 494, loss = 0.43746556\n",
      "Iteration 495, loss = 0.43686320\n",
      "Iteration 496, loss = 0.43639177\n",
      "Iteration 497, loss = 0.43589511\n",
      "Iteration 498, loss = 0.43651966\n",
      "Iteration 499, loss = 0.43527537\n",
      "Iteration 500, loss = 0.43509175\n"
     ]
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)\n",
    "y_pred = clf_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77      1541\n",
      "           1       0.79      0.75      0.77      1646\n",
      "\n",
      "    accuracy                           0.77      3187\n",
      "   macro avg       0.77      0.77      0.77      3187\n",
      "weighted avg       0.77      0.77      0.77      3187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13</td>\n",
       "      <td>29549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>worry</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7433</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment content\n",
       "count      30000   30000\n",
       "unique        13   29549\n",
       "top        worry        \n",
       "freq        7433      60"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
