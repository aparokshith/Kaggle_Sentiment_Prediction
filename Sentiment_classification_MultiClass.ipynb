{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import words\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cv = CountVectorizer(binary=True)\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"sa-emotions/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tes):\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_U_NAME = re.compile(\"@[\\S]+\")\n",
    "    REPLACE_DIGITS = re.compile(\"\\d\")\n",
    "    REPLACE_W_SPACE = re.compile(\"_\")\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_NO_SPACE, '')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_U_NAME,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_DIGITS,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.replace(REPLACE_W_SPACE,'')\n",
    "    tes[\"content\"] = tes[\"content\"].str.lower()\n",
    "    return tes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = cleaner(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty   i know  i was listenin to bad habit earlier a...\n",
       "1     sadness  layin n bed with a headache  ughhhhwaitin on y...\n",
       "2     sadness                      funeral ceremonygloomy friday\n",
       "3  enthusiasm                wants to hang out with friends soon\n",
       "4     neutral   we want to trade with someone who has houston..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = df_train.loc[(df_train['sentiment'] == 'worry') | (df_train['sentiment'] == 'anger') | (df_train['sentiment'] == 'hate')| (df_train['sentiment'] == 'love')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worry</td>\n",
       "      <td>re-pinging  why didnt you go to prom bc my bf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worry</td>\n",
       "      <td>hmmm http//wwwdjherocom/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>worry</td>\n",
       "      <td>choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>worry</td>\n",
       "      <td>lady gaga tweeted about not being impressed b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            content\n",
       "5      worry  re-pinging  why didnt you go to prom bc my bf ...\n",
       "7      worry                   hmmm http//wwwdjherocom/ is down\n",
       "11     worry                            choked on her retainers\n",
       "16      love                                             agreed\n",
       "18     worry   lady gaga tweeted about not being impressed b..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry    7433\n",
       "love     2068\n",
       "hate     1187\n",
       "anger      98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_bin['sentiment'].value_counts().plot('bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(train_data,test_data,train_labels,test_labels):\n",
    "    lr = LogisticRegression(C=1)\n",
    "    lsvc = LinearSVC(random_state=0, tol=1e-5)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    et = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "    xgb = XGBClassifier()\n",
    "    adb = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    models = {'LogisticRegression':lr,'LinearSVC':lsvc,'RandomForest':rf,'ExtraTrees':et,'XGBoost':xgb,'AdaBoost':adb}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(train_data, train_labels)\n",
    "        print(\"Accuracy for {} is {}\".format(model_name,accuracy_score(test_labels, model.predict(test_data))))\n",
    "        predicted = model.predict(test_data)\n",
    "        print(classification_report(test_labels,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(df_bin[\"content\"])\n",
    "count_vec = cv.transform(df_bin[\"content\"])\n",
    "# X_test_vec = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worry    7433\n",
       "love     2068\n",
       "hate     1187\n",
       "anger      98\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bin[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(count_vec, df_bin['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_SMOTE.value_counts().plot('bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_enc(values):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    return(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_labels = int_enc(np.array(df_bin['sentiment']))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_X_train, cv_X_test, cv_y_train, cv_y_test = train_test_split(count_vec, cv_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_search(train_features,y_train):\n",
    "    parameters = {'C': np.linspace(1, 0.0001, 100, 20)}\n",
    "    grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "    grid_search.fit(train_features, y_train)\n",
    "\n",
    "    print('best parameters: ', grid_search.best_params_)\n",
    "    print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_search(cv_X_train, cv_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.192: 0.7525280898876404\n"
     ]
    }
   ],
   "source": [
    "#non-SMOTE\n",
    "c = 0.1920 #value from grid search\n",
    "lr = LogisticRegression(C=c)\n",
    "lr.fit(cv_X_train, cv_y_train)\n",
    "print (\"Accuracy for C=%s: %s\" \n",
    "   % (c, accuracy_score(cv_y_test, lr.predict(cv_X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.68      0.20      0.31       399\n",
      "           2       0.74      0.39      0.51       664\n",
      "           3       0.76      0.95      0.84      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.54      0.39      0.42      3560\n",
      "weighted avg       0.74      0.75      0.71      3560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(cv_X_test)\n",
    "print(classification_report(cv_y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_search(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.9798: 0.7561149612719119\n"
     ]
    }
   ],
   "source": [
    "c = 0.9798 #value from gridsearch\n",
    "lr = LogisticRegression(C=c)\n",
    "lr.fit(X_train, y_train)\n",
    "print (\"Accuracy for C=%s: %s\" \n",
    "   % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82      2508\n",
      "           1       0.75      0.65      0.70      2414\n",
      "           2       0.80      0.70      0.75      2448\n",
      "           3       0.79      0.70      0.74      2442\n",
      "\n",
      "    accuracy                           0.76      9812\n",
      "   macro avg       0.76      0.75      0.75      9812\n",
      "weighted avg       0.76      0.76      0.75      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lr.predict(X_test)\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.57      0.23      0.33       399\n",
      "           2       0.69      0.46      0.55       664\n",
      "           3       0.77      0.92      0.84      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.51      0.40      0.43      3560\n",
      "weighted avg       0.72      0.75      0.72      3560\n",
      "\n",
      "Accuracy for LinearSVC is 0.7154494382022472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.39      0.26      0.31       399\n",
      "           2       0.58      0.50      0.53       664\n",
      "           3       0.78      0.86      0.82      2460\n",
      "\n",
      "    accuracy                           0.72      3560\n",
      "   macro avg       0.44      0.40      0.42      3560\n",
      "weighted avg       0.69      0.72      0.70      3560\n",
      "\n",
      "Accuracy for RandomForest is 0.7480337078651685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.72      0.16      0.26       399\n",
      "           2       0.69      0.39      0.50       664\n",
      "           3       0.76      0.95      0.84      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.54      0.37      0.40      3560\n",
      "weighted avg       0.73      0.75      0.70      3560\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7544943820224719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.64      0.17      0.27       399\n",
      "           2       0.70      0.43      0.53       664\n",
      "           3       0.77      0.95      0.85      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.53      0.39      0.41      3560\n",
      "weighted avg       0.73      0.75      0.72      3560\n",
      "\n",
      "Accuracy for XGBoost is 0.7384831460674157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.67      0.17      0.27       399\n",
      "           2       0.75      0.28      0.40       664\n",
      "           3       0.74      0.97      0.84      2460\n",
      "\n",
      "    accuracy                           0.74      3560\n",
      "   macro avg       0.54      0.35      0.38      3560\n",
      "weighted avg       0.73      0.74      0.68      3560\n",
      "\n",
      "Accuracy for AdaBoost is 0.7289325842696629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.59      0.15      0.23       399\n",
      "           2       0.71      0.28      0.41       664\n",
      "           3       0.74      0.95      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.51      0.35      0.37      3560\n",
      "weighted avg       0.71      0.73      0.68      3560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(cv_X_train, cv_X_test, cv_y_train, cv_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7562168772931105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.97      0.82      2508\n",
      "           1       0.75      0.65      0.70      2414\n",
      "           2       0.81      0.70      0.75      2448\n",
      "           3       0.79      0.70      0.74      2442\n",
      "\n",
      "    accuracy                           0.76      9812\n",
      "   macro avg       0.76      0.75      0.75      9812\n",
      "weighted avg       0.76      0.76      0.75      9812\n",
      "\n",
      "Accuracy for LinearSVC is 0.751528740317978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.82      2508\n",
      "           1       0.74      0.64      0.69      2414\n",
      "           2       0.79      0.71      0.74      2448\n",
      "           3       0.78      0.68      0.73      2442\n",
      "\n",
      "    accuracy                           0.75      9812\n",
      "   macro avg       0.76      0.75      0.75      9812\n",
      "weighted avg       0.76      0.75      0.75      9812\n",
      "\n",
      "Accuracy for RandomForest is 0.8032001630656339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86      2508\n",
      "           1       0.89      0.67      0.77      2414\n",
      "           2       0.85      0.72      0.78      2448\n",
      "           3       0.76      0.82      0.79      2442\n",
      "\n",
      "    accuracy                           0.80      9812\n",
      "   macro avg       0.81      0.80      0.80      9812\n",
      "weighted avg       0.81      0.80      0.80      9812\n",
      "\n",
      "Accuracy for ExtraTrees is 0.807990216061965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86      2508\n",
      "           1       0.91      0.67      0.77      2414\n",
      "           2       0.87      0.70      0.78      2448\n",
      "           3       0.75      0.87      0.80      2442\n",
      "\n",
      "    accuracy                           0.81      9812\n",
      "   macro avg       0.82      0.81      0.80      9812\n",
      "weighted avg       0.82      0.81      0.80      9812\n",
      "\n",
      "Accuracy for XGBoost is 0.8527313493681207\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      2508\n",
      "           1       0.95      0.76      0.84      2414\n",
      "           2       0.85      0.72      0.78      2448\n",
      "           3       0.69      0.94      0.80      2442\n",
      "\n",
      "    accuracy                           0.85      9812\n",
      "   macro avg       0.87      0.85      0.85      9812\n",
      "weighted avg       0.87      0.85      0.85      9812\n",
      "\n",
      "Accuracy for AdaBoost is 0.672645739910314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.96      0.76      2508\n",
      "           1       0.67      0.50      0.57      2414\n",
      "           2       0.74      0.54      0.63      2448\n",
      "           3       0.70      0.67      0.68      2442\n",
      "\n",
      "    accuracy                           0.67      9812\n",
      "   macro avg       0.68      0.67      0.66      9812\n",
      "weighted avg       0.68      0.67      0.66      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=500, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38418821\n",
      "Iteration 2, loss = 1.37501847\n",
      "Iteration 3, loss = 1.36659376\n",
      "Iteration 4, loss = 1.35661452\n",
      "Iteration 5, loss = 1.34423442\n",
      "Iteration 6, loss = 1.32829374\n",
      "Iteration 7, loss = 1.30641871\n",
      "Iteration 8, loss = 1.27748281\n",
      "Iteration 9, loss = 1.24217168\n",
      "Iteration 10, loss = 1.20223802\n",
      "Iteration 11, loss = 1.16255847\n",
      "Iteration 12, loss = 1.12786283\n",
      "Iteration 13, loss = 1.09792756\n",
      "Iteration 14, loss = 1.07170036\n",
      "Iteration 15, loss = 1.04749262\n",
      "Iteration 16, loss = 1.02467692\n",
      "Iteration 17, loss = 1.00277869\n",
      "Iteration 18, loss = 0.98125053\n",
      "Iteration 19, loss = 0.96009286\n",
      "Iteration 20, loss = 0.93911542\n",
      "Iteration 21, loss = 0.91831546\n",
      "Iteration 22, loss = 0.89803335\n",
      "Iteration 23, loss = 0.87835004\n",
      "Iteration 24, loss = 0.85956052\n",
      "Iteration 25, loss = 0.84165486\n",
      "Iteration 26, loss = 0.82500263\n",
      "Iteration 27, loss = 0.80935426\n",
      "Iteration 28, loss = 0.79460685\n",
      "Iteration 29, loss = 0.78103656\n",
      "Iteration 30, loss = 0.76849390\n",
      "Iteration 31, loss = 0.75676797\n",
      "Iteration 32, loss = 0.74601539\n",
      "Iteration 33, loss = 0.73566552\n",
      "Iteration 34, loss = 0.72606842\n",
      "Iteration 35, loss = 0.71690762\n",
      "Iteration 36, loss = 0.70824837\n",
      "Iteration 37, loss = 0.70028697\n",
      "Iteration 38, loss = 0.69236527\n",
      "Iteration 39, loss = 0.68503046\n",
      "Iteration 40, loss = 0.67763751\n",
      "Iteration 41, loss = 0.67064175\n",
      "Iteration 42, loss = 0.66411157\n",
      "Iteration 43, loss = 0.65793835\n",
      "Iteration 44, loss = 0.65131973\n",
      "Iteration 45, loss = 0.64522035\n",
      "Iteration 46, loss = 0.63946882\n",
      "Iteration 47, loss = 0.63388832\n",
      "Iteration 48, loss = 0.62822951\n",
      "Iteration 49, loss = 0.62280497\n",
      "Iteration 50, loss = 0.61743516\n",
      "Iteration 51, loss = 0.61218725\n",
      "Iteration 52, loss = 0.60704357\n",
      "Iteration 53, loss = 0.60202436\n",
      "Iteration 54, loss = 0.59695438\n",
      "Iteration 55, loss = 0.59226834\n",
      "Iteration 56, loss = 0.58746695\n",
      "Iteration 57, loss = 0.58255878\n",
      "Iteration 58, loss = 0.57769684\n",
      "Iteration 59, loss = 0.57302543\n",
      "Iteration 60, loss = 0.56862236\n",
      "Iteration 61, loss = 0.56395729\n",
      "Iteration 62, loss = 0.55932490\n",
      "Iteration 63, loss = 0.55500783\n",
      "Iteration 64, loss = 0.55038554\n",
      "Iteration 65, loss = 0.54575239\n",
      "Iteration 66, loss = 0.54151151\n",
      "Iteration 67, loss = 0.53693701\n",
      "Iteration 68, loss = 0.53274773\n",
      "Iteration 69, loss = 0.52800968\n",
      "Iteration 70, loss = 0.52365122\n",
      "Iteration 71, loss = 0.51907974\n",
      "Iteration 72, loss = 0.51465163\n",
      "Iteration 73, loss = 0.51016825\n",
      "Iteration 74, loss = 0.50584846\n",
      "Iteration 75, loss = 0.50101618\n",
      "Iteration 76, loss = 0.49671648\n",
      "Iteration 77, loss = 0.49260675\n",
      "Iteration 78, loss = 0.48808134\n",
      "Iteration 79, loss = 0.48343422\n",
      "Iteration 80, loss = 0.47880920\n",
      "Iteration 81, loss = 0.47435512\n",
      "Iteration 82, loss = 0.46998633\n",
      "Iteration 83, loss = 0.46529754\n",
      "Iteration 84, loss = 0.46104596\n",
      "Iteration 85, loss = 0.45636574\n",
      "Iteration 86, loss = 0.45176576\n",
      "Iteration 87, loss = 0.44742881\n",
      "Iteration 88, loss = 0.44326021\n",
      "Iteration 89, loss = 0.43852249\n",
      "Iteration 90, loss = 0.43405947\n",
      "Iteration 91, loss = 0.42976311\n",
      "Iteration 92, loss = 0.42502383\n",
      "Iteration 93, loss = 0.42061295\n",
      "Iteration 94, loss = 0.41641478\n",
      "Iteration 95, loss = 0.41211152\n",
      "Iteration 96, loss = 0.40790970\n",
      "Iteration 97, loss = 0.40352473\n",
      "Iteration 98, loss = 0.39902866\n",
      "Iteration 99, loss = 0.39488378\n",
      "Iteration 100, loss = 0.39089246\n",
      "Iteration 101, loss = 0.38661245\n",
      "Iteration 102, loss = 0.38302122\n",
      "Iteration 103, loss = 0.37860424\n",
      "Iteration 104, loss = 0.37470682\n",
      "Iteration 105, loss = 0.37088722\n",
      "Iteration 106, loss = 0.36710869\n",
      "Iteration 107, loss = 0.36334977\n",
      "Iteration 108, loss = 0.35974209\n",
      "Iteration 109, loss = 0.35607408\n",
      "Iteration 110, loss = 0.35274335\n",
      "Iteration 111, loss = 0.34921650\n",
      "Iteration 112, loss = 0.34618473\n",
      "Iteration 113, loss = 0.34280624\n",
      "Iteration 114, loss = 0.34001010\n",
      "Iteration 115, loss = 0.33662242\n",
      "Iteration 116, loss = 0.33384482\n",
      "Iteration 117, loss = 0.33102169\n",
      "Iteration 118, loss = 0.32818679\n",
      "Iteration 119, loss = 0.32581799\n",
      "Iteration 120, loss = 0.32284627\n",
      "Iteration 121, loss = 0.32047481\n",
      "Iteration 122, loss = 0.31815262\n",
      "Iteration 123, loss = 0.31529003\n",
      "Iteration 124, loss = 0.31310901\n",
      "Iteration 125, loss = 0.31073649\n",
      "Iteration 126, loss = 0.30863493\n",
      "Iteration 127, loss = 0.30658765\n",
      "Iteration 128, loss = 0.30481538\n",
      "Iteration 129, loss = 0.30292987\n",
      "Iteration 130, loss = 0.30082153\n",
      "Iteration 131, loss = 0.29909518\n",
      "Iteration 132, loss = 0.29718687\n",
      "Iteration 133, loss = 0.29572525\n",
      "Iteration 134, loss = 0.29419863\n",
      "Iteration 135, loss = 0.29270818\n",
      "Iteration 136, loss = 0.29125692\n",
      "Iteration 137, loss = 0.28958240\n",
      "Iteration 138, loss = 0.28805580\n",
      "Iteration 139, loss = 0.28681437\n",
      "Iteration 140, loss = 0.28552993\n",
      "Iteration 141, loss = 0.28408305\n",
      "Iteration 142, loss = 0.28281156\n",
      "Iteration 143, loss = 0.28152893\n",
      "Iteration 144, loss = 0.28050602\n",
      "Iteration 145, loss = 0.27914756\n",
      "Iteration 146, loss = 0.27774758\n",
      "Iteration 147, loss = 0.27749430\n",
      "Iteration 148, loss = 0.27618882\n",
      "Iteration 149, loss = 0.27543969\n",
      "Iteration 150, loss = 0.27396057\n",
      "Iteration 151, loss = 0.27272269\n",
      "Iteration 152, loss = 0.27252106\n",
      "Iteration 153, loss = 0.27183955\n",
      "Iteration 154, loss = 0.27111787\n",
      "Iteration 155, loss = 0.26967583\n",
      "Iteration 156, loss = 0.26869008\n",
      "Iteration 157, loss = 0.26811750\n",
      "Iteration 158, loss = 0.26801784\n",
      "Iteration 159, loss = 0.26699781\n",
      "Iteration 160, loss = 0.26685111\n",
      "Iteration 161, loss = 0.26544791\n",
      "Iteration 162, loss = 0.26525811\n",
      "Iteration 163, loss = 0.26396690\n",
      "Iteration 164, loss = 0.26366679\n",
      "Iteration 165, loss = 0.26325875\n",
      "Iteration 166, loss = 0.26237457\n",
      "Iteration 167, loss = 0.26200541\n",
      "Iteration 168, loss = 0.26147660\n",
      "Iteration 169, loss = 0.26119144\n",
      "Iteration 170, loss = 0.26003056\n",
      "Iteration 171, loss = 0.25997808\n",
      "Iteration 172, loss = 0.26024118\n",
      "Iteration 173, loss = 0.25943739\n",
      "Iteration 174, loss = 0.25847844\n",
      "Iteration 175, loss = 0.25842581\n",
      "Iteration 176, loss = 0.25762466\n",
      "Iteration 177, loss = 0.25749916\n",
      "Iteration 178, loss = 0.25724844\n",
      "Iteration 179, loss = 0.25620857\n",
      "Iteration 180, loss = 0.25635325\n",
      "Iteration 181, loss = 0.25604626\n",
      "Iteration 182, loss = 0.25572512\n",
      "Iteration 183, loss = 0.25556719\n",
      "Iteration 184, loss = 0.25491497\n",
      "Iteration 185, loss = 0.25431586\n",
      "Iteration 186, loss = 0.25380174\n",
      "Iteration 187, loss = 0.25382898\n",
      "Iteration 188, loss = 0.25355376\n",
      "Iteration 189, loss = 0.25324914\n",
      "Iteration 190, loss = 0.25302889\n",
      "Iteration 191, loss = 0.25256221\n",
      "Iteration 192, loss = 0.25220522\n",
      "Iteration 193, loss = 0.25180878\n",
      "Iteration 194, loss = 0.25227839\n",
      "Iteration 195, loss = 0.25140704\n",
      "Iteration 196, loss = 0.25164697\n",
      "Iteration 197, loss = 0.25075552\n",
      "Iteration 198, loss = 0.25089537\n",
      "Iteration 199, loss = 0.25042970\n",
      "Iteration 200, loss = 0.25053014\n",
      "Iteration 201, loss = 0.24983445\n",
      "Iteration 202, loss = 0.25029161\n",
      "Iteration 203, loss = 0.24992961\n",
      "Iteration 204, loss = 0.24964961\n",
      "Iteration 205, loss = 0.24912439\n",
      "Iteration 206, loss = 0.24906060\n",
      "Iteration 207, loss = 0.24853318\n",
      "Iteration 208, loss = 0.24895050\n",
      "Iteration 209, loss = 0.24863619\n",
      "Iteration 210, loss = 0.24858933\n",
      "Iteration 211, loss = 0.24837215\n",
      "Iteration 212, loss = 0.24808605\n",
      "Iteration 213, loss = 0.24782035\n",
      "Iteration 214, loss = 0.24795831\n",
      "Iteration 215, loss = 0.24747901\n",
      "Iteration 216, loss = 0.24748195\n",
      "Iteration 217, loss = 0.24714060\n",
      "Iteration 218, loss = 0.24714350\n",
      "Iteration 219, loss = 0.24709457\n",
      "Iteration 220, loss = 0.24692705\n",
      "Iteration 221, loss = 0.24661190\n",
      "Iteration 222, loss = 0.24724206\n",
      "Iteration 223, loss = 0.24653928\n",
      "Iteration 224, loss = 0.24647795\n",
      "Iteration 225, loss = 0.24579063\n",
      "Iteration 226, loss = 0.24570073\n",
      "Iteration 227, loss = 0.24593899\n",
      "Iteration 228, loss = 0.24546787\n",
      "Iteration 229, loss = 0.24608196\n",
      "Iteration 230, loss = 0.24625856\n",
      "Iteration 231, loss = 0.24533820\n",
      "Iteration 232, loss = 0.24537200\n",
      "Iteration 233, loss = 0.24496632\n",
      "Iteration 234, loss = 0.24522768\n",
      "Iteration 235, loss = 0.24550374\n",
      "Iteration 236, loss = 0.24526046\n",
      "Iteration 237, loss = 0.24471406\n",
      "Iteration 238, loss = 0.24455021\n",
      "Iteration 239, loss = 0.24483342\n",
      "Iteration 240, loss = 0.24415066\n",
      "Iteration 241, loss = 0.24427769\n",
      "Iteration 242, loss = 0.24424609\n",
      "Iteration 243, loss = 0.24460945\n",
      "Iteration 244, loss = 0.24470025\n",
      "Iteration 245, loss = 0.24369473\n",
      "Iteration 246, loss = 0.24368468\n",
      "Iteration 247, loss = 0.24403995\n",
      "Iteration 248, loss = 0.24360948\n",
      "Iteration 249, loss = 0.24396891\n",
      "Iteration 250, loss = 0.24305523\n",
      "Iteration 251, loss = 0.24358143\n",
      "Iteration 252, loss = 0.24408237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.24322446\n",
      "Iteration 254, loss = 0.24318908\n",
      "Iteration 255, loss = 0.24361512\n",
      "Iteration 256, loss = 0.24396340\n",
      "Iteration 257, loss = 0.24325452\n",
      "Iteration 258, loss = 0.24347768\n",
      "Iteration 259, loss = 0.24306825\n",
      "Iteration 260, loss = 0.24302950\n",
      "Iteration 261, loss = 0.24297399\n",
      "Iteration 262, loss = 0.24316413\n",
      "Iteration 263, loss = 0.24297319\n",
      "Iteration 264, loss = 0.24345396\n",
      "Iteration 265, loss = 0.24303273\n",
      "Iteration 266, loss = 0.24315563\n",
      "Iteration 267, loss = 0.24241035\n",
      "Iteration 268, loss = 0.24227604\n",
      "Iteration 269, loss = 0.24262064\n",
      "Iteration 270, loss = 0.24308234\n",
      "Iteration 271, loss = 0.24292003\n",
      "Iteration 272, loss = 0.24224358\n",
      "Iteration 273, loss = 0.24217918\n",
      "Iteration 274, loss = 0.24326783\n",
      "Iteration 275, loss = 0.24238224\n",
      "Iteration 276, loss = 0.24220617\n",
      "Iteration 277, loss = 0.24231727\n",
      "Iteration 278, loss = 0.24279564\n",
      "Iteration 279, loss = 0.24222145\n",
      "Iteration 280, loss = 0.24296636\n",
      "Iteration 281, loss = 0.24232342\n",
      "Iteration 282, loss = 0.24198594\n",
      "Iteration 283, loss = 0.24213992\n",
      "Iteration 284, loss = 0.24147299\n",
      "Iteration 285, loss = 0.24174832\n",
      "Iteration 286, loss = 0.24169791\n",
      "Iteration 287, loss = 0.24214693\n",
      "Iteration 288, loss = 0.24115276\n",
      "Iteration 289, loss = 0.24199332\n",
      "Iteration 290, loss = 0.24204339\n",
      "Iteration 291, loss = 0.24193683\n",
      "Iteration 292, loss = 0.24177528\n",
      "Iteration 293, loss = 0.24144102\n",
      "Iteration 294, loss = 0.24168980\n",
      "Iteration 295, loss = 0.24139925\n",
      "Iteration 296, loss = 0.24162217\n",
      "Iteration 297, loss = 0.24135407\n",
      "Iteration 298, loss = 0.24143556\n",
      "Iteration 299, loss = 0.24180928\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.86      2508\n",
      "           1       0.85      0.69      0.76      2414\n",
      "           2       0.84      0.76      0.80      2448\n",
      "           3       0.79      0.78      0.79      2442\n",
      "\n",
      "    accuracy                           0.81      9812\n",
      "   macro avg       0.81      0.80      0.80      9812\n",
      "weighted avg       0.81      0.81      0.80      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 10786 contents is represented by 2607 features (TF-IDF score of unigrams and bigrams)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "# We transform each complaint into a vector\n",
    "features = tfidf.fit_transform(df_bin.content).toarray()\n",
    "\n",
    "labels = df_bin.sentiment\n",
    "\n",
    "print(\"Each of the %d contents is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(features, tfidf_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.748314606741573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.63      0.13      0.22       399\n",
      "           2       0.75      0.39      0.51       664\n",
      "           3       0.75      0.96      0.84      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.53      0.37      0.39      3560\n",
      "weighted avg       0.73      0.75      0.70      3560\n",
      "\n",
      "Accuracy for LinearSVC is 0.7264044943820225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.48      0.21      0.29       399\n",
      "           2       0.60      0.47      0.52       664\n",
      "           3       0.77      0.89      0.82      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.46      0.39      0.41      3560\n",
      "weighted avg       0.69      0.73      0.70      3560\n",
      "\n",
      "Accuracy for RandomForest is 0.7289325842696629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.47      0.21      0.29       399\n",
      "           2       0.61      0.45      0.51       664\n",
      "           3       0.77      0.90      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.46      0.39      0.41      3560\n",
      "weighted avg       0.70      0.73      0.70      3560\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7106741573033708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.43      0.23      0.30       399\n",
      "           2       0.55      0.48      0.51       664\n",
      "           3       0.77      0.86      0.81      2460\n",
      "\n",
      "    accuracy                           0.71      3560\n",
      "   macro avg       0.44      0.39      0.41      3560\n",
      "weighted avg       0.68      0.71      0.69      3560\n",
      "\n",
      "Accuracy for XGBoost is 0.7317415730337079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.66      0.15      0.24       399\n",
      "           2       0.74      0.25      0.37       664\n",
      "           3       0.73      0.97      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.53      0.34      0.36      3560\n",
      "weighted avg       0.72      0.73      0.67      3560\n",
      "\n",
      "Accuracy for AdaBoost is 0.7303370786516854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.56      0.21      0.30       399\n",
      "           2       0.64      0.36      0.46       664\n",
      "           3       0.75      0.93      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.49      0.37      0.40      3560\n",
      "weighted avg       0.70      0.73      0.69      3560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7796575621687729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      2508\n",
      "           1       0.77      0.77      0.77      2414\n",
      "           2       0.81      0.71      0.76      2448\n",
      "           3       0.75      0.63      0.68      2442\n",
      "\n",
      "    accuracy                           0.78      9812\n",
      "   macro avg       0.78      0.78      0.77      9812\n",
      "weighted avg       0.78      0.78      0.77      9812\n",
      "\n",
      "Accuracy for LinearSVC is 0.7973909498573176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89      2508\n",
      "           1       0.79      0.80      0.80      2414\n",
      "           2       0.81      0.76      0.78      2448\n",
      "           3       0.77      0.62      0.69      2442\n",
      "\n",
      "    accuracy                           0.80      9812\n",
      "   macro avg       0.80      0.80      0.79      9812\n",
      "weighted avg       0.80      0.80      0.79      9812\n",
      "\n",
      "Accuracy for RandomForest is 0.8814716673461068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94      2508\n",
      "           1       0.87      0.93      0.90      2414\n",
      "           2       0.87      0.86      0.87      2448\n",
      "           3       0.88      0.73      0.80      2442\n",
      "\n",
      "    accuracy                           0.88      9812\n",
      "   macro avg       0.88      0.88      0.88      9812\n",
      "weighted avg       0.88      0.88      0.88      9812\n",
      "\n",
      "Accuracy for ExtraTrees is 0.8948226661231146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95      2508\n",
      "           1       0.89      0.96      0.92      2414\n",
      "           2       0.88      0.88      0.88      2448\n",
      "           3       0.92      0.74      0.82      2442\n",
      "\n",
      "    accuracy                           0.89      9812\n",
      "   macro avg       0.90      0.89      0.89      9812\n",
      "weighted avg       0.90      0.89      0.89      9812\n",
      "\n",
      "Accuracy for XGBoost is 0.6641867101508357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.97      0.72      2508\n",
      "           1       0.84      0.48      0.61      2414\n",
      "           2       0.83      0.59      0.69      2448\n",
      "           3       0.62      0.60      0.61      2442\n",
      "\n",
      "    accuracy                           0.66      9812\n",
      "   macro avg       0.71      0.66      0.66      9812\n",
      "weighted avg       0.71      0.66      0.66      9812\n",
      "\n",
      "Accuracy for AdaBoost is 0.6119037912759886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78      2508\n",
      "           1       0.63      0.43      0.51      2414\n",
      "           2       0.76      0.50      0.60      2448\n",
      "           3       0.44      0.74      0.55      2442\n",
      "\n",
      "    accuracy                           0.61      9812\n",
      "   macro avg       0.66      0.61      0.61      9812\n",
      "weighted avg       0.66      0.61      0.61      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=150, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38683229\n",
      "Iteration 2, loss = 1.38548566\n",
      "Iteration 3, loss = 1.38481602\n",
      "Iteration 4, loss = 1.38425853\n",
      "Iteration 5, loss = 1.38369731\n",
      "Iteration 6, loss = 1.38315093\n",
      "Iteration 7, loss = 1.38263112\n",
      "Iteration 8, loss = 1.38209840\n",
      "Iteration 9, loss = 1.38156324\n",
      "Iteration 10, loss = 1.38104404\n",
      "Iteration 11, loss = 1.38048782\n",
      "Iteration 12, loss = 1.37992768\n",
      "Iteration 13, loss = 1.37936974\n",
      "Iteration 14, loss = 1.37877929\n",
      "Iteration 15, loss = 1.37816659\n",
      "Iteration 16, loss = 1.37758355\n",
      "Iteration 17, loss = 1.37689486\n",
      "Iteration 18, loss = 1.37615925\n",
      "Iteration 19, loss = 1.37542426\n",
      "Iteration 20, loss = 1.37460070\n",
      "Iteration 21, loss = 1.37372091\n",
      "Iteration 22, loss = 1.37280087\n",
      "Iteration 23, loss = 1.37185648\n",
      "Iteration 24, loss = 1.37084153\n",
      "Iteration 25, loss = 1.36982374\n",
      "Iteration 26, loss = 1.36866837\n",
      "Iteration 27, loss = 1.36748957\n",
      "Iteration 28, loss = 1.36619568\n",
      "Iteration 29, loss = 1.36483213\n",
      "Iteration 30, loss = 1.36335773\n",
      "Iteration 31, loss = 1.36178012\n",
      "Iteration 32, loss = 1.36004225\n",
      "Iteration 33, loss = 1.35816636\n",
      "Iteration 34, loss = 1.35610759\n",
      "Iteration 35, loss = 1.35391081\n",
      "Iteration 36, loss = 1.35154461\n",
      "Iteration 37, loss = 1.34889897\n",
      "Iteration 38, loss = 1.34599080\n",
      "Iteration 39, loss = 1.34274009\n",
      "Iteration 40, loss = 1.33932569\n",
      "Iteration 41, loss = 1.33547320\n",
      "Iteration 42, loss = 1.33129724\n",
      "Iteration 43, loss = 1.32669174\n",
      "Iteration 44, loss = 1.32163499\n",
      "Iteration 45, loss = 1.31591063\n",
      "Iteration 46, loss = 1.30970974\n",
      "Iteration 47, loss = 1.30272542\n",
      "Iteration 48, loss = 1.29503507\n",
      "Iteration 49, loss = 1.28658482\n",
      "Iteration 50, loss = 1.27716246\n",
      "Iteration 51, loss = 1.26675483\n",
      "Iteration 52, loss = 1.25538744\n",
      "Iteration 53, loss = 1.24299560\n",
      "Iteration 54, loss = 1.22947282\n",
      "Iteration 55, loss = 1.21486590\n",
      "Iteration 56, loss = 1.19919138\n",
      "Iteration 57, loss = 1.18266895\n",
      "Iteration 58, loss = 1.16528256\n",
      "Iteration 59, loss = 1.14718675\n",
      "Iteration 60, loss = 1.12844448\n",
      "Iteration 61, loss = 1.10944490\n",
      "Iteration 62, loss = 1.09018395\n",
      "Iteration 63, loss = 1.07079456\n",
      "Iteration 64, loss = 1.05128903\n",
      "Iteration 65, loss = 1.03190884\n",
      "Iteration 66, loss = 1.01266581\n",
      "Iteration 67, loss = 0.99387807\n",
      "Iteration 68, loss = 0.97490892\n",
      "Iteration 69, loss = 0.95576664\n",
      "Iteration 70, loss = 0.93691242\n",
      "Iteration 71, loss = 0.91799456\n",
      "Iteration 72, loss = 0.89927837\n",
      "Iteration 73, loss = 0.88050449\n",
      "Iteration 74, loss = 0.86224017\n",
      "Iteration 75, loss = 0.84413988\n",
      "Iteration 76, loss = 0.82629589\n",
      "Iteration 77, loss = 0.80891768\n",
      "Iteration 78, loss = 0.79206062\n",
      "Iteration 79, loss = 0.77545083\n",
      "Iteration 80, loss = 0.75926759\n",
      "Iteration 81, loss = 0.74380108\n",
      "Iteration 82, loss = 0.72926084\n",
      "Iteration 83, loss = 0.71430378\n",
      "Iteration 84, loss = 0.70052894\n",
      "Iteration 85, loss = 0.68741107\n",
      "Iteration 86, loss = 0.67478574\n",
      "Iteration 87, loss = 0.66244426\n",
      "Iteration 88, loss = 0.65132349\n",
      "Iteration 89, loss = 0.63990891\n",
      "Iteration 90, loss = 0.62905363\n",
      "Iteration 91, loss = 0.61900630\n",
      "Iteration 92, loss = 0.60880848\n",
      "Iteration 93, loss = 0.59978106\n",
      "Iteration 94, loss = 0.59072730\n",
      "Iteration 95, loss = 0.58200407\n",
      "Iteration 96, loss = 0.57348396\n",
      "Iteration 97, loss = 0.56533981\n",
      "Iteration 98, loss = 0.55673641\n",
      "Iteration 99, loss = 0.54920148\n",
      "Iteration 100, loss = 0.54199860\n",
      "Iteration 101, loss = 0.53466929\n",
      "Iteration 102, loss = 0.52725428\n",
      "Iteration 103, loss = 0.52047004\n",
      "Iteration 104, loss = 0.51375321\n",
      "Iteration 105, loss = 0.50692510\n",
      "Iteration 106, loss = 0.50144328\n",
      "Iteration 107, loss = 0.49392628\n",
      "Iteration 108, loss = 0.48765995\n",
      "Iteration 109, loss = 0.48197895\n",
      "Iteration 110, loss = 0.47582745\n",
      "Iteration 111, loss = 0.46991943\n",
      "Iteration 112, loss = 0.46431326\n",
      "Iteration 113, loss = 0.45849057\n",
      "Iteration 114, loss = 0.45340802\n",
      "Iteration 115, loss = 0.44770053\n",
      "Iteration 116, loss = 0.44294183\n",
      "Iteration 117, loss = 0.43689326\n",
      "Iteration 118, loss = 0.43194807\n",
      "Iteration 119, loss = 0.42644027\n",
      "Iteration 120, loss = 0.42136928\n",
      "Iteration 121, loss = 0.41695577\n",
      "Iteration 122, loss = 0.41247920\n",
      "Iteration 123, loss = 0.40698145\n",
      "Iteration 124, loss = 0.40188509\n",
      "Iteration 125, loss = 0.39703571\n",
      "Iteration 126, loss = 0.39248905\n",
      "Iteration 127, loss = 0.38792130\n",
      "Iteration 128, loss = 0.38320366\n",
      "Iteration 129, loss = 0.37819515\n",
      "Iteration 130, loss = 0.37418188\n",
      "Iteration 131, loss = 0.36909096\n",
      "Iteration 132, loss = 0.36520576\n",
      "Iteration 133, loss = 0.36122269\n",
      "Iteration 134, loss = 0.35644071\n",
      "Iteration 135, loss = 0.35197452\n",
      "Iteration 136, loss = 0.34789350\n",
      "Iteration 137, loss = 0.34459667\n",
      "Iteration 138, loss = 0.33996957\n",
      "Iteration 139, loss = 0.33631078\n",
      "Iteration 140, loss = 0.33201540\n",
      "Iteration 141, loss = 0.32798709\n",
      "Iteration 142, loss = 0.32423307\n",
      "Iteration 143, loss = 0.32018442\n",
      "Iteration 144, loss = 0.31604792\n",
      "Iteration 145, loss = 0.31244157\n",
      "Iteration 146, loss = 0.30879569\n",
      "Iteration 147, loss = 0.30529810\n",
      "Iteration 148, loss = 0.30143570\n",
      "Iteration 149, loss = 0.29775488\n",
      "Iteration 150, loss = 0.29421563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=150,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      2508\n",
      "           1       0.83      0.83      0.83      2414\n",
      "           2       0.84      0.81      0.83      2448\n",
      "           3       0.81      0.72      0.76      2442\n",
      "\n",
      "    accuracy                           0.84      9812\n",
      "   macro avg       0.84      0.84      0.84      9812\n",
      "weighted avg       0.84      0.84      0.84      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca = pca.transform(X_train)\n",
    "test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7590705258866693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87      2508\n",
      "           1       0.74      0.73      0.74      2414\n",
      "           2       0.80      0.69      0.74      2448\n",
      "           3       0.71      0.61      0.66      2442\n",
      "\n",
      "    accuracy                           0.76      9812\n",
      "   macro avg       0.76      0.76      0.75      9812\n",
      "weighted avg       0.76      0.76      0.75      9812\n",
      "\n",
      "Accuracy for LinearSVC is 0.7703832042397065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      2508\n",
      "           1       0.75      0.75      0.75      2414\n",
      "           2       0.80      0.72      0.76      2448\n",
      "           3       0.73      0.61      0.66      2442\n",
      "\n",
      "    accuracy                           0.77      9812\n",
      "   macro avg       0.77      0.77      0.76      9812\n",
      "weighted avg       0.77      0.77      0.76      9812\n",
      "\n",
      "Accuracy for RandomForest is 0.9012433754586221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      2508\n",
      "           1       0.89      0.93      0.91      2414\n",
      "           2       0.92      0.77      0.84      2448\n",
      "           3       0.85      0.90      0.87      2442\n",
      "\n",
      "    accuracy                           0.90      9812\n",
      "   macro avg       0.90      0.90      0.90      9812\n",
      "weighted avg       0.90      0.90      0.90      9812\n",
      "\n",
      "Accuracy for ExtraTrees is 0.9015491235222177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      2508\n",
      "           1       0.89      0.93      0.91      2414\n",
      "           2       0.92      0.77      0.84      2448\n",
      "           3       0.85      0.90      0.87      2442\n",
      "\n",
      "    accuracy                           0.90      9812\n",
      "   macro avg       0.90      0.90      0.90      9812\n",
      "weighted avg       0.90      0.90      0.90      9812\n",
      "\n",
      "Accuracy for XGBoost is 0.7778230737871994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      2508\n",
      "           1       0.75      0.70      0.73      2414\n",
      "           2       0.78      0.64      0.70      2448\n",
      "           3       0.68      0.77      0.72      2442\n",
      "\n",
      "    accuracy                           0.78      9812\n",
      "   macro avg       0.78      0.78      0.77      9812\n",
      "weighted avg       0.78      0.78      0.77      9812\n",
      "\n",
      "Accuracy for AdaBoost is 0.6433958418263351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      2508\n",
      "           1       0.58      0.51      0.54      2414\n",
      "           2       0.60      0.35      0.45      2448\n",
      "           3       0.48      0.71      0.57      2442\n",
      "\n",
      "    accuracy                           0.64      9812\n",
      "   macro avg       0.65      0.64      0.63      9812\n",
      "weighted avg       0.65      0.64      0.63      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(train_pca, test_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=300, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39163230\n",
      "Iteration 2, loss = 1.38741436\n",
      "Iteration 3, loss = 1.38593391\n",
      "Iteration 4, loss = 1.38516352\n",
      "Iteration 5, loss = 1.38452518\n",
      "Iteration 6, loss = 1.38399908\n",
      "Iteration 7, loss = 1.38346097\n",
      "Iteration 8, loss = 1.38296464\n",
      "Iteration 9, loss = 1.38245654\n",
      "Iteration 10, loss = 1.38194506\n",
      "Iteration 11, loss = 1.38146196\n",
      "Iteration 12, loss = 1.38096402\n",
      "Iteration 13, loss = 1.38047810\n",
      "Iteration 14, loss = 1.37993540\n",
      "Iteration 15, loss = 1.37937498\n",
      "Iteration 16, loss = 1.37879251\n",
      "Iteration 17, loss = 1.37819480\n",
      "Iteration 18, loss = 1.37760282\n",
      "Iteration 19, loss = 1.37693741\n",
      "Iteration 20, loss = 1.37625040\n",
      "Iteration 21, loss = 1.37551684\n",
      "Iteration 22, loss = 1.37471739\n",
      "Iteration 23, loss = 1.37387469\n",
      "Iteration 24, loss = 1.37300630\n",
      "Iteration 25, loss = 1.37206840\n",
      "Iteration 26, loss = 1.37108797\n",
      "Iteration 27, loss = 1.36999689\n",
      "Iteration 28, loss = 1.36881903\n",
      "Iteration 29, loss = 1.36753528\n",
      "Iteration 30, loss = 1.36598478\n",
      "Iteration 31, loss = 1.36428178\n",
      "Iteration 32, loss = 1.36263663\n",
      "Iteration 33, loss = 1.36081598\n",
      "Iteration 34, loss = 1.35891034\n",
      "Iteration 35, loss = 1.35671703\n",
      "Iteration 36, loss = 1.35397998\n",
      "Iteration 37, loss = 1.35117215\n",
      "Iteration 38, loss = 1.34838011\n",
      "Iteration 39, loss = 1.34541077\n",
      "Iteration 40, loss = 1.34204922\n",
      "Iteration 41, loss = 1.33812429\n",
      "Iteration 42, loss = 1.33382607\n",
      "Iteration 43, loss = 1.32936387\n",
      "Iteration 44, loss = 1.32442912\n",
      "Iteration 45, loss = 1.31911619\n",
      "Iteration 46, loss = 1.31309016\n",
      "Iteration 47, loss = 1.30658585\n",
      "Iteration 48, loss = 1.29917456\n",
      "Iteration 49, loss = 1.29094305\n",
      "Iteration 50, loss = 1.28186684\n",
      "Iteration 51, loss = 1.27181875\n",
      "Iteration 52, loss = 1.26092645\n",
      "Iteration 53, loss = 1.24888479\n",
      "Iteration 54, loss = 1.23590220\n",
      "Iteration 55, loss = 1.22148591\n",
      "Iteration 56, loss = 1.20618008\n",
      "Iteration 57, loss = 1.18961381\n",
      "Iteration 58, loss = 1.17236825\n",
      "Iteration 59, loss = 1.15403121\n",
      "Iteration 60, loss = 1.13540510\n",
      "Iteration 61, loss = 1.11555879\n",
      "Iteration 62, loss = 1.09576441\n",
      "Iteration 63, loss = 1.07592118\n",
      "Iteration 64, loss = 1.05588098\n",
      "Iteration 65, loss = 1.03659801\n",
      "Iteration 66, loss = 1.01791386\n",
      "Iteration 67, loss = 0.99883309\n",
      "Iteration 68, loss = 0.98041935\n",
      "Iteration 69, loss = 0.96275748\n",
      "Iteration 70, loss = 0.94538742\n",
      "Iteration 71, loss = 0.92858922\n",
      "Iteration 72, loss = 0.91255603\n",
      "Iteration 73, loss = 0.89687513\n",
      "Iteration 74, loss = 0.88185393\n",
      "Iteration 75, loss = 0.86742612\n",
      "Iteration 76, loss = 0.85328461\n",
      "Iteration 77, loss = 0.83996311\n",
      "Iteration 78, loss = 0.82703400\n",
      "Iteration 79, loss = 0.81483379\n",
      "Iteration 80, loss = 0.80270705\n",
      "Iteration 81, loss = 0.79084357\n",
      "Iteration 82, loss = 0.77922468\n",
      "Iteration 83, loss = 0.76829453\n",
      "Iteration 84, loss = 0.75760685\n",
      "Iteration 85, loss = 0.74714382\n",
      "Iteration 86, loss = 0.73698464\n",
      "Iteration 87, loss = 0.72680168\n",
      "Iteration 88, loss = 0.71724984\n",
      "Iteration 89, loss = 0.70764786\n",
      "Iteration 90, loss = 0.69812800\n",
      "Iteration 91, loss = 0.68894891\n",
      "Iteration 92, loss = 0.68048784\n",
      "Iteration 93, loss = 0.67159701\n",
      "Iteration 94, loss = 0.66351692\n",
      "Iteration 95, loss = 0.65520993\n",
      "Iteration 96, loss = 0.64782803\n",
      "Iteration 97, loss = 0.63966932\n",
      "Iteration 98, loss = 0.63241429\n",
      "Iteration 99, loss = 0.62503430\n",
      "Iteration 100, loss = 0.61779918\n",
      "Iteration 101, loss = 0.61180203\n",
      "Iteration 102, loss = 0.60465814\n",
      "Iteration 103, loss = 0.59810148\n",
      "Iteration 104, loss = 0.59187579\n",
      "Iteration 105, loss = 0.58607064\n",
      "Iteration 106, loss = 0.58064741\n",
      "Iteration 107, loss = 0.57495536\n",
      "Iteration 108, loss = 0.56884951\n",
      "Iteration 109, loss = 0.56327180\n",
      "Iteration 110, loss = 0.55808639\n",
      "Iteration 111, loss = 0.55295526\n",
      "Iteration 112, loss = 0.54810758\n",
      "Iteration 113, loss = 0.54279341\n",
      "Iteration 114, loss = 0.53817608\n",
      "Iteration 115, loss = 0.53386940\n",
      "Iteration 116, loss = 0.52856613\n",
      "Iteration 117, loss = 0.52388377\n",
      "Iteration 118, loss = 0.51972415\n",
      "Iteration 119, loss = 0.51549988\n",
      "Iteration 120, loss = 0.51085333\n",
      "Iteration 121, loss = 0.50669261\n",
      "Iteration 122, loss = 0.50301044\n",
      "Iteration 123, loss = 0.49819884\n",
      "Iteration 124, loss = 0.49437741\n",
      "Iteration 125, loss = 0.49060695\n",
      "Iteration 126, loss = 0.48624186\n",
      "Iteration 127, loss = 0.48228798\n",
      "Iteration 128, loss = 0.47934554\n",
      "Iteration 129, loss = 0.47483564\n",
      "Iteration 130, loss = 0.47094834\n",
      "Iteration 131, loss = 0.46727646\n",
      "Iteration 132, loss = 0.46388815\n",
      "Iteration 133, loss = 0.46022479\n",
      "Iteration 134, loss = 0.45665270\n",
      "Iteration 135, loss = 0.45281125\n",
      "Iteration 136, loss = 0.44949887\n",
      "Iteration 137, loss = 0.44629539\n",
      "Iteration 138, loss = 0.44255147\n",
      "Iteration 139, loss = 0.43942474\n",
      "Iteration 140, loss = 0.43633893\n",
      "Iteration 141, loss = 0.43317757\n",
      "Iteration 142, loss = 0.43032368\n",
      "Iteration 143, loss = 0.42597554\n",
      "Iteration 144, loss = 0.42324755\n",
      "Iteration 145, loss = 0.42006044\n",
      "Iteration 146, loss = 0.41681971\n",
      "Iteration 147, loss = 0.41409016\n",
      "Iteration 148, loss = 0.41069480\n",
      "Iteration 149, loss = 0.40743748\n",
      "Iteration 150, loss = 0.40492474\n",
      "Iteration 151, loss = 0.40144674\n",
      "Iteration 152, loss = 0.39900008\n",
      "Iteration 153, loss = 0.39574890\n",
      "Iteration 154, loss = 0.39255734\n",
      "Iteration 155, loss = 0.39024290\n",
      "Iteration 156, loss = 0.38751001\n",
      "Iteration 157, loss = 0.38433893\n",
      "Iteration 158, loss = 0.38122028\n",
      "Iteration 159, loss = 0.37861479\n",
      "Iteration 160, loss = 0.37592834\n",
      "Iteration 161, loss = 0.37309416\n",
      "Iteration 162, loss = 0.37047943\n",
      "Iteration 163, loss = 0.36781899\n",
      "Iteration 164, loss = 0.36497615\n",
      "Iteration 165, loss = 0.36231828\n",
      "Iteration 166, loss = 0.35992698\n",
      "Iteration 167, loss = 0.35694934\n",
      "Iteration 168, loss = 0.35430136\n",
      "Iteration 169, loss = 0.35194061\n",
      "Iteration 170, loss = 0.34995741\n",
      "Iteration 171, loss = 0.34683379\n",
      "Iteration 172, loss = 0.34422807\n",
      "Iteration 173, loss = 0.34163481\n",
      "Iteration 174, loss = 0.33920249\n",
      "Iteration 175, loss = 0.33633395\n",
      "Iteration 176, loss = 0.33464247\n",
      "Iteration 177, loss = 0.33152564\n",
      "Iteration 178, loss = 0.32932204\n",
      "Iteration 179, loss = 0.32743682\n",
      "Iteration 180, loss = 0.32426733\n",
      "Iteration 181, loss = 0.32201702\n",
      "Iteration 182, loss = 0.31925633\n",
      "Iteration 183, loss = 0.31760953\n",
      "Iteration 184, loss = 0.31481194\n",
      "Iteration 185, loss = 0.31255928\n",
      "Iteration 186, loss = 0.30992436\n",
      "Iteration 187, loss = 0.30770778\n",
      "Iteration 188, loss = 0.30580847\n",
      "Iteration 189, loss = 0.30389862\n",
      "Iteration 190, loss = 0.30099415\n",
      "Iteration 191, loss = 0.29875794\n",
      "Iteration 192, loss = 0.29690255\n",
      "Iteration 193, loss = 0.29386962\n",
      "Iteration 194, loss = 0.29266264\n",
      "Iteration 195, loss = 0.28991726\n",
      "Iteration 196, loss = 0.28765812\n",
      "Iteration 197, loss = 0.28505507\n",
      "Iteration 198, loss = 0.28312571\n",
      "Iteration 199, loss = 0.28101222\n",
      "Iteration 200, loss = 0.27924056\n",
      "Iteration 201, loss = 0.27657867\n",
      "Iteration 202, loss = 0.27454790\n",
      "Iteration 203, loss = 0.27229743\n",
      "Iteration 204, loss = 0.26979964\n",
      "Iteration 205, loss = 0.26791199\n",
      "Iteration 206, loss = 0.26670685\n",
      "Iteration 207, loss = 0.26367521\n",
      "Iteration 208, loss = 0.26222547\n",
      "Iteration 209, loss = 0.25987503\n",
      "Iteration 210, loss = 0.25784740\n",
      "Iteration 211, loss = 0.25535326\n",
      "Iteration 212, loss = 0.25447122\n",
      "Iteration 213, loss = 0.25095592\n",
      "Iteration 214, loss = 0.24983825\n",
      "Iteration 215, loss = 0.24771216\n",
      "Iteration 216, loss = 0.24529381\n",
      "Iteration 217, loss = 0.24353277\n",
      "Iteration 218, loss = 0.24181830\n",
      "Iteration 219, loss = 0.23975443\n",
      "Iteration 220, loss = 0.23828278\n",
      "Iteration 221, loss = 0.23546717\n",
      "Iteration 222, loss = 0.23437362\n",
      "Iteration 223, loss = 0.23165178\n",
      "Iteration 224, loss = 0.23007776\n",
      "Iteration 225, loss = 0.22854533\n",
      "Iteration 226, loss = 0.22608548\n",
      "Iteration 227, loss = 0.22403295\n",
      "Iteration 228, loss = 0.22344917\n",
      "Iteration 229, loss = 0.22082778\n",
      "Iteration 230, loss = 0.21878826\n",
      "Iteration 231, loss = 0.21796683\n",
      "Iteration 232, loss = 0.21520017\n",
      "Iteration 233, loss = 0.21505756\n",
      "Iteration 234, loss = 0.21206745\n",
      "Iteration 235, loss = 0.21100048\n",
      "Iteration 236, loss = 0.20832544\n",
      "Iteration 237, loss = 0.20696461\n",
      "Iteration 238, loss = 0.20520739\n",
      "Iteration 239, loss = 0.20411999\n",
      "Iteration 240, loss = 0.20259917\n",
      "Iteration 241, loss = 0.20026762\n",
      "Iteration 242, loss = 0.19882271\n",
      "Iteration 243, loss = 0.19698846\n",
      "Iteration 244, loss = 0.19526726\n",
      "Iteration 245, loss = 0.19426653\n",
      "Iteration 246, loss = 0.19206182\n",
      "Iteration 247, loss = 0.19104880\n",
      "Iteration 248, loss = 0.18978448\n",
      "Iteration 249, loss = 0.18844875\n",
      "Iteration 250, loss = 0.18667032\n",
      "Iteration 251, loss = 0.18528279\n",
      "Iteration 252, loss = 0.18353313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.18244267\n",
      "Iteration 254, loss = 0.18032235\n",
      "Iteration 255, loss = 0.17909463\n",
      "Iteration 256, loss = 0.17868164\n",
      "Iteration 257, loss = 0.17716105\n",
      "Iteration 258, loss = 0.17602839\n",
      "Iteration 259, loss = 0.17369049\n",
      "Iteration 260, loss = 0.17372375\n",
      "Iteration 261, loss = 0.17180910\n",
      "Iteration 262, loss = 0.17044515\n",
      "Iteration 263, loss = 0.16875684\n",
      "Iteration 264, loss = 0.16835440\n",
      "Iteration 265, loss = 0.16641094\n",
      "Iteration 266, loss = 0.16504644\n",
      "Iteration 267, loss = 0.16389779\n",
      "Iteration 268, loss = 0.16274740\n",
      "Iteration 269, loss = 0.16208864\n",
      "Iteration 270, loss = 0.16100400\n",
      "Iteration 271, loss = 0.15984732\n",
      "Iteration 272, loss = 0.15850927\n",
      "Iteration 273, loss = 0.15823319\n",
      "Iteration 274, loss = 0.15641857\n",
      "Iteration 275, loss = 0.15565141\n",
      "Iteration 276, loss = 0.15451496\n",
      "Iteration 277, loss = 0.15502141\n",
      "Iteration 278, loss = 0.15274393\n",
      "Iteration 279, loss = 0.15110387\n",
      "Iteration 280, loss = 0.15043195\n",
      "Iteration 281, loss = 0.14920781\n",
      "Iteration 282, loss = 0.14881320\n",
      "Iteration 283, loss = 0.14798224\n",
      "Iteration 284, loss = 0.14664744\n",
      "Iteration 285, loss = 0.14577527\n",
      "Iteration 286, loss = 0.14455521\n",
      "Iteration 287, loss = 0.14310405\n",
      "Iteration 288, loss = 0.14200745\n",
      "Iteration 289, loss = 0.14187268\n",
      "Iteration 290, loss = 0.14047817\n",
      "Iteration 291, loss = 0.14020335\n",
      "Iteration 292, loss = 0.13884975\n",
      "Iteration 293, loss = 0.13913851\n",
      "Iteration 294, loss = 0.13836781\n",
      "Iteration 295, loss = 0.13571106\n",
      "Iteration 296, loss = 0.13626625\n",
      "Iteration 297, loss = 0.13514199\n",
      "Iteration 298, loss = 0.13434230\n",
      "Iteration 299, loss = 0.13338916\n",
      "Iteration 300, loss = 0.13232862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=300,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      2508\n",
      "           1       0.85      0.91      0.88      2414\n",
      "           2       0.84      0.88      0.86      2448\n",
      "           3       0.86      0.70      0.77      2442\n",
      "\n",
      "    accuracy                           0.87      9812\n",
      "   macro avg       0.87      0.87      0.87      9812\n",
      "weighted avg       0.87      0.87      0.87      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(test_pca)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     [re-pinging, didnt, go, prom, bc, bf, didnt, l...\n",
       "7                           [hmmm, http//wwwdjherocom/]\n",
       "11                                  [choked, retainers]\n",
       "16                                             [agreed]\n",
       "18    [lady, gaga, tweeted, impressed, video, leakin...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_stop_removal = df_bin[\"content\"].apply(lambda x: [item for item in x.split() if item not in stop])\n",
    "w2v_stop_removal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        print(\"cannot compute similarity %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute similarity %s ['youstinkatrespondingtotexts']\n",
      "cannot compute similarity %s ['fuckinm', 'transtelecom']\n",
      "cannot compute similarity %s ['confuzzled']\n",
      "cannot compute similarity %s ['doesntfeelwell']\n",
      "cannot compute similarity %s ['ughhhhhhhhhhhh']\n",
      "cannot compute similarity %s ['aaaaaaaaaaa', 'mcfly']\n",
      "cannot compute similarity %s []\n",
      "cannot compute similarity %s ['motherfuck', 'qw']\n",
      "cannot compute similarity %s []\n",
      "cannot compute similarity %s ['*hug*']\n",
      "cannot compute similarity %s ['awwwwww', '*hugs*']\n",
      "cannot compute similarity %s ['soreeeee', 'throattttttt']\n",
      "cannot compute similarity %s ['*hugs*']\n",
      "cannot compute similarity %s ['homeworkboring']\n",
      "cannot compute similarity %s ['grrrrrrrr']\n",
      "cannot compute similarity %s ['shower/traffic/factory', 'yaaaay']\n",
      "cannot compute similarity %s ['lllooovvveee', 'icarly']\n",
      "cannot compute similarity %s []\n",
      "cannot compute similarity %s []\n",
      "cannot compute similarity %s ['xloveyoutoo']\n",
      "cannot compute similarity %s []\n",
      "cannot compute similarity %s ['goodmorning']\n",
      "cannot compute similarity %s ['subway=yum']\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "train_tokenized = df_bin.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(df_bin, test_size=0.3, random_state = 42)\n",
    "\n",
    "# test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "# train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['content']), axis=1).values\n",
    "\n",
    "# X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "# X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(X_train_word_average, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X_train, w2v_X_test, w2v_y_train, w2v_y_test = train_test_split(X_train_word_average, w2v_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7528089887640449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.75      0.12      0.21       399\n",
      "           2       0.76      0.39      0.51       664\n",
      "           3       0.75      0.97      0.85      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.56      0.37      0.39      3560\n",
      "weighted avg       0.75      0.75      0.70      3560\n",
      "\n",
      "Accuracy for LinearSVC is 0.7514044943820225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.72      0.13      0.22       399\n",
      "           2       0.71      0.42      0.53       664\n",
      "           3       0.76      0.95      0.84      2460\n",
      "\n",
      "    accuracy                           0.75      3560\n",
      "   macro avg       0.55      0.38      0.40      3560\n",
      "weighted avg       0.74      0.75      0.71      3560\n",
      "\n",
      "Accuracy for RandomForest is 0.7196629213483146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       1.00      0.02      0.03       399\n",
      "           2       0.82      0.18      0.29       664\n",
      "           3       0.71      0.99      0.83      2460\n",
      "\n",
      "    accuracy                           0.72      3560\n",
      "   macro avg       0.63      0.30      0.29      3560\n",
      "weighted avg       0.76      0.72      0.63      3560\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7129213483146067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.86      0.02      0.03       399\n",
      "           2       0.82      0.14      0.23       664\n",
      "           3       0.71      0.99      0.83      2460\n",
      "\n",
      "    accuracy                           0.71      3560\n",
      "   macro avg       0.60      0.29      0.27      3560\n",
      "weighted avg       0.74      0.71      0.62      3560\n",
      "\n",
      "Accuracy for XGBoost is 0.7432584269662922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.74      0.06      0.12       399\n",
      "           2       0.78      0.33      0.46       664\n",
      "           3       0.74      0.98      0.84      2460\n",
      "\n",
      "    accuracy                           0.74      3560\n",
      "   macro avg       0.57      0.34      0.36      3560\n",
      "weighted avg       0.74      0.74      0.68      3560\n",
      "\n",
      "Accuracy for AdaBoost is 0.6884831460674158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.51      0.06      0.10       399\n",
      "           2       0.46      0.33      0.38       664\n",
      "           3       0.73      0.90      0.80      2460\n",
      "\n",
      "    accuracy                           0.69      3560\n",
      "   macro avg       0.42      0.32      0.32      3560\n",
      "weighted avg       0.65      0.69      0.64      3560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(w2v_X_train, w2v_X_test, w2v_y_train, w2v_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7175907052588667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      2508\n",
      "           1       0.71      0.71      0.71      2414\n",
      "           2       0.76      0.76      0.76      2448\n",
      "           3       0.60      0.51      0.55      2442\n",
      "\n",
      "    accuracy                           0.72      9812\n",
      "   macro avg       0.71      0.72      0.71      9812\n",
      "weighted avg       0.71      0.72      0.71      9812\n",
      "\n",
      "Accuracy for LinearSVC is 0.7309417040358744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85      2508\n",
      "           1       0.73      0.74      0.73      2414\n",
      "           2       0.75      0.78      0.77      2448\n",
      "           3       0.63      0.47      0.54      2442\n",
      "\n",
      "    accuracy                           0.73      9812\n",
      "   macro avg       0.72      0.73      0.72      9812\n",
      "weighted avg       0.72      0.73      0.72      9812\n",
      "\n",
      "Accuracy for RandomForest is 0.9331430900937627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2508\n",
      "           1       0.96      0.96      0.96      2414\n",
      "           2       0.91      0.89      0.90      2448\n",
      "           3       0.86      0.88      0.87      2442\n",
      "\n",
      "    accuracy                           0.93      9812\n",
      "   macro avg       0.93      0.93      0.93      9812\n",
      "weighted avg       0.93      0.93      0.93      9812\n",
      "\n",
      "Accuracy for ExtraTrees is 0.9453730126375867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2508\n",
      "           1       0.98      0.97      0.97      2414\n",
      "           2       0.94      0.89      0.91      2448\n",
      "           3       0.87      0.93      0.90      2442\n",
      "\n",
      "    accuracy                           0.95      9812\n",
      "   macro avg       0.95      0.95      0.95      9812\n",
      "weighted avg       0.95      0.95      0.95      9812\n",
      "\n",
      "Accuracy for XGBoost is 0.795352629433347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      2508\n",
      "           1       0.77      0.79      0.78      2414\n",
      "           2       0.76      0.77      0.77      2448\n",
      "           3       0.70      0.63      0.67      2442\n",
      "\n",
      "    accuracy                           0.80      9812\n",
      "   macro avg       0.79      0.79      0.79      9812\n",
      "weighted avg       0.79      0.80      0.79      9812\n",
      "\n",
      "Accuracy for AdaBoost is 0.6576640847941296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85      2508\n",
      "           1       0.58      0.65      0.61      2414\n",
      "           2       0.66      0.70      0.68      2448\n",
      "           3       0.52      0.47      0.50      2442\n",
      "\n",
      "    accuracy                           0.66      9812\n",
      "   macro avg       0.66      0.66      0.66      9812\n",
      "weighted avg       0.66      0.66      0.66      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=300, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38711418\n",
      "Iteration 2, loss = 1.38418324\n",
      "Iteration 3, loss = 1.38265377\n",
      "Iteration 4, loss = 1.38150920\n",
      "Iteration 5, loss = 1.38040974\n",
      "Iteration 6, loss = 1.37937956\n",
      "Iteration 7, loss = 1.37833591\n",
      "Iteration 8, loss = 1.37724345\n",
      "Iteration 9, loss = 1.37608267\n",
      "Iteration 10, loss = 1.37483476\n",
      "Iteration 11, loss = 1.37349226\n",
      "Iteration 12, loss = 1.37208307\n",
      "Iteration 13, loss = 1.37050434\n",
      "Iteration 14, loss = 1.36878815\n",
      "Iteration 15, loss = 1.36680183\n",
      "Iteration 16, loss = 1.36461017\n",
      "Iteration 17, loss = 1.36219699\n",
      "Iteration 18, loss = 1.35951470\n",
      "Iteration 19, loss = 1.35652550\n",
      "Iteration 20, loss = 1.35313536\n",
      "Iteration 21, loss = 1.34933386\n",
      "Iteration 22, loss = 1.34502133\n",
      "Iteration 23, loss = 1.34010111\n",
      "Iteration 24, loss = 1.33455402\n",
      "Iteration 25, loss = 1.32826091\n",
      "Iteration 26, loss = 1.32094330\n",
      "Iteration 27, loss = 1.31275140\n",
      "Iteration 28, loss = 1.30331976\n",
      "Iteration 29, loss = 1.29236773\n",
      "Iteration 30, loss = 1.28015509\n",
      "Iteration 31, loss = 1.26706726\n",
      "Iteration 32, loss = 1.25294679\n",
      "Iteration 33, loss = 1.23825880\n",
      "Iteration 34, loss = 1.22291253\n",
      "Iteration 35, loss = 1.20740426\n",
      "Iteration 36, loss = 1.19201344\n",
      "Iteration 37, loss = 1.17714308\n",
      "Iteration 38, loss = 1.16281818\n",
      "Iteration 39, loss = 1.14927624\n",
      "Iteration 40, loss = 1.13657940\n",
      "Iteration 41, loss = 1.12475261\n",
      "Iteration 42, loss = 1.11352925\n",
      "Iteration 43, loss = 1.10308815\n",
      "Iteration 44, loss = 1.09292383\n",
      "Iteration 45, loss = 1.08366320\n",
      "Iteration 46, loss = 1.07417238\n",
      "Iteration 47, loss = 1.06539512\n",
      "Iteration 48, loss = 1.05632643\n",
      "Iteration 49, loss = 1.04735934\n",
      "Iteration 50, loss = 1.03843332\n",
      "Iteration 51, loss = 1.02984345\n",
      "Iteration 52, loss = 1.02078982\n",
      "Iteration 53, loss = 1.01148890\n",
      "Iteration 54, loss = 1.00267079\n",
      "Iteration 55, loss = 0.99333177\n",
      "Iteration 56, loss = 0.98421670\n",
      "Iteration 57, loss = 0.97471929\n",
      "Iteration 58, loss = 0.96507240\n",
      "Iteration 59, loss = 0.95550319\n",
      "Iteration 60, loss = 0.94607383\n",
      "Iteration 61, loss = 0.93561814\n",
      "Iteration 62, loss = 0.92611840\n",
      "Iteration 63, loss = 0.91588954\n",
      "Iteration 64, loss = 0.90574285\n",
      "Iteration 65, loss = 0.89566983\n",
      "Iteration 66, loss = 0.88598548\n",
      "Iteration 67, loss = 0.87585987\n",
      "Iteration 68, loss = 0.86590162\n",
      "Iteration 69, loss = 0.85594933\n",
      "Iteration 70, loss = 0.84656733\n",
      "Iteration 71, loss = 0.83679466\n",
      "Iteration 72, loss = 0.82745321\n",
      "Iteration 73, loss = 0.81776630\n",
      "Iteration 74, loss = 0.80836597\n",
      "Iteration 75, loss = 0.80031798\n",
      "Iteration 76, loss = 0.79093856\n",
      "Iteration 77, loss = 0.78207422\n",
      "Iteration 78, loss = 0.77423626\n",
      "Iteration 79, loss = 0.76586244\n",
      "Iteration 80, loss = 0.75762240\n",
      "Iteration 81, loss = 0.74963630\n",
      "Iteration 82, loss = 0.74240746\n",
      "Iteration 83, loss = 0.73511606\n",
      "Iteration 84, loss = 0.72738573\n",
      "Iteration 85, loss = 0.72052712\n",
      "Iteration 86, loss = 0.71311438\n",
      "Iteration 87, loss = 0.70652617\n",
      "Iteration 88, loss = 0.70005160\n",
      "Iteration 89, loss = 0.69329733\n",
      "Iteration 90, loss = 0.68672979\n",
      "Iteration 91, loss = 0.68082349\n",
      "Iteration 92, loss = 0.67452357\n",
      "Iteration 93, loss = 0.66888243\n",
      "Iteration 94, loss = 0.66276641\n",
      "Iteration 95, loss = 0.65737435\n",
      "Iteration 96, loss = 0.65217241\n",
      "Iteration 97, loss = 0.64654759\n",
      "Iteration 98, loss = 0.64112094\n",
      "Iteration 99, loss = 0.63576756\n",
      "Iteration 100, loss = 0.63093272\n",
      "Iteration 101, loss = 0.62564532\n",
      "Iteration 102, loss = 0.62118010\n",
      "Iteration 103, loss = 0.61619114\n",
      "Iteration 104, loss = 0.61212528\n",
      "Iteration 105, loss = 0.60682861\n",
      "Iteration 106, loss = 0.60234332\n",
      "Iteration 107, loss = 0.59762802\n",
      "Iteration 108, loss = 0.59340268\n",
      "Iteration 109, loss = 0.58899375\n",
      "Iteration 110, loss = 0.58502604\n",
      "Iteration 111, loss = 0.58090497\n",
      "Iteration 112, loss = 0.57682441\n",
      "Iteration 113, loss = 0.57309912\n",
      "Iteration 114, loss = 0.56890901\n",
      "Iteration 115, loss = 0.56469286\n",
      "Iteration 116, loss = 0.56153139\n",
      "Iteration 117, loss = 0.55767458\n",
      "Iteration 118, loss = 0.55358127\n",
      "Iteration 119, loss = 0.55028455\n",
      "Iteration 120, loss = 0.54674182\n",
      "Iteration 121, loss = 0.54375641\n",
      "Iteration 122, loss = 0.53993674\n",
      "Iteration 123, loss = 0.53718707\n",
      "Iteration 124, loss = 0.53333574\n",
      "Iteration 125, loss = 0.53040725\n",
      "Iteration 126, loss = 0.52736422\n",
      "Iteration 127, loss = 0.52419856\n",
      "Iteration 128, loss = 0.52118132\n",
      "Iteration 129, loss = 0.51818072\n",
      "Iteration 130, loss = 0.51517682\n",
      "Iteration 131, loss = 0.51206997\n",
      "Iteration 132, loss = 0.50948605\n",
      "Iteration 133, loss = 0.50676694\n",
      "Iteration 134, loss = 0.50441805\n",
      "Iteration 135, loss = 0.50129568\n",
      "Iteration 136, loss = 0.49893309\n",
      "Iteration 137, loss = 0.49714398\n",
      "Iteration 138, loss = 0.49401674\n",
      "Iteration 139, loss = 0.49193069\n",
      "Iteration 140, loss = 0.48943202\n",
      "Iteration 141, loss = 0.48690193\n",
      "Iteration 142, loss = 0.48435767\n",
      "Iteration 143, loss = 0.48226317\n",
      "Iteration 144, loss = 0.47978104\n",
      "Iteration 145, loss = 0.47801909\n",
      "Iteration 146, loss = 0.47613321\n",
      "Iteration 147, loss = 0.47376177\n",
      "Iteration 148, loss = 0.47109473\n",
      "Iteration 149, loss = 0.46883598\n",
      "Iteration 150, loss = 0.46739696\n",
      "Iteration 151, loss = 0.46496739\n",
      "Iteration 152, loss = 0.46298617\n",
      "Iteration 153, loss = 0.46121665\n",
      "Iteration 154, loss = 0.45896008\n",
      "Iteration 155, loss = 0.45703250\n",
      "Iteration 156, loss = 0.45493638\n",
      "Iteration 157, loss = 0.45300960\n",
      "Iteration 158, loss = 0.45139528\n",
      "Iteration 159, loss = 0.44963225\n",
      "Iteration 160, loss = 0.44807088\n",
      "Iteration 161, loss = 0.44519349\n",
      "Iteration 162, loss = 0.44300342\n",
      "Iteration 163, loss = 0.44187483\n",
      "Iteration 164, loss = 0.43962755\n",
      "Iteration 165, loss = 0.43780483\n",
      "Iteration 166, loss = 0.43634559\n",
      "Iteration 167, loss = 0.43445026\n",
      "Iteration 168, loss = 0.43230165\n",
      "Iteration 169, loss = 0.43058756\n",
      "Iteration 170, loss = 0.42834317\n",
      "Iteration 171, loss = 0.42737353\n",
      "Iteration 172, loss = 0.42554137\n",
      "Iteration 173, loss = 0.42397801\n",
      "Iteration 174, loss = 0.42138081\n",
      "Iteration 175, loss = 0.41965019\n",
      "Iteration 176, loss = 0.41808330\n",
      "Iteration 177, loss = 0.41593028\n",
      "Iteration 178, loss = 0.41448510\n",
      "Iteration 179, loss = 0.41228079\n",
      "Iteration 180, loss = 0.41068139\n",
      "Iteration 181, loss = 0.40863754\n",
      "Iteration 182, loss = 0.40742074\n",
      "Iteration 183, loss = 0.40530940\n",
      "Iteration 184, loss = 0.40337087\n",
      "Iteration 185, loss = 0.40113099\n",
      "Iteration 186, loss = 0.39943979\n",
      "Iteration 187, loss = 0.39786324\n",
      "Iteration 188, loss = 0.39602756\n",
      "Iteration 189, loss = 0.39412028\n",
      "Iteration 190, loss = 0.39235200\n",
      "Iteration 191, loss = 0.39041127\n",
      "Iteration 192, loss = 0.38810857\n",
      "Iteration 193, loss = 0.38697158\n",
      "Iteration 194, loss = 0.38479262\n",
      "Iteration 195, loss = 0.38296064\n",
      "Iteration 196, loss = 0.38077677\n",
      "Iteration 197, loss = 0.37865185\n",
      "Iteration 198, loss = 0.37677124\n",
      "Iteration 199, loss = 0.37534792\n",
      "Iteration 200, loss = 0.37321785\n",
      "Iteration 201, loss = 0.37148765\n",
      "Iteration 202, loss = 0.36930881\n",
      "Iteration 203, loss = 0.36764178\n",
      "Iteration 204, loss = 0.36509461\n",
      "Iteration 205, loss = 0.36294990\n",
      "Iteration 206, loss = 0.36156629\n",
      "Iteration 207, loss = 0.35970776\n",
      "Iteration 208, loss = 0.35739684\n",
      "Iteration 209, loss = 0.35582204\n",
      "Iteration 210, loss = 0.35355574\n",
      "Iteration 211, loss = 0.35144100\n",
      "Iteration 212, loss = 0.34905895\n",
      "Iteration 213, loss = 0.34728341\n",
      "Iteration 214, loss = 0.34565269\n",
      "Iteration 215, loss = 0.34289557\n",
      "Iteration 216, loss = 0.34067052\n",
      "Iteration 217, loss = 0.33930306\n",
      "Iteration 218, loss = 0.33723915\n",
      "Iteration 219, loss = 0.33512042\n",
      "Iteration 220, loss = 0.33300857\n",
      "Iteration 221, loss = 0.33050633\n",
      "Iteration 222, loss = 0.32864324\n",
      "Iteration 223, loss = 0.32629449\n",
      "Iteration 224, loss = 0.32419887\n",
      "Iteration 225, loss = 0.32213277\n",
      "Iteration 226, loss = 0.32043187\n",
      "Iteration 227, loss = 0.31760385\n",
      "Iteration 228, loss = 0.31553986\n",
      "Iteration 229, loss = 0.31324989\n",
      "Iteration 230, loss = 0.31141260\n",
      "Iteration 231, loss = 0.30934640\n",
      "Iteration 232, loss = 0.30693704\n",
      "Iteration 233, loss = 0.30546125\n",
      "Iteration 234, loss = 0.30278091\n",
      "Iteration 235, loss = 0.30045171\n",
      "Iteration 236, loss = 0.29853669\n",
      "Iteration 237, loss = 0.29651329\n",
      "Iteration 238, loss = 0.29449250\n",
      "Iteration 239, loss = 0.29199333\n",
      "Iteration 240, loss = 0.29041490\n",
      "Iteration 241, loss = 0.28799281\n",
      "Iteration 242, loss = 0.28555372\n",
      "Iteration 243, loss = 0.28329644\n",
      "Iteration 244, loss = 0.28196367\n",
      "Iteration 245, loss = 0.27916738\n",
      "Iteration 246, loss = 0.27766905\n",
      "Iteration 247, loss = 0.27510183\n",
      "Iteration 248, loss = 0.27290000\n",
      "Iteration 249, loss = 0.27057731\n",
      "Iteration 250, loss = 0.26856943\n",
      "Iteration 251, loss = 0.26643124\n",
      "Iteration 252, loss = 0.26475289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.26307860\n",
      "Iteration 254, loss = 0.25990779\n",
      "Iteration 255, loss = 0.25874163\n",
      "Iteration 256, loss = 0.25604178\n",
      "Iteration 257, loss = 0.25365725\n",
      "Iteration 258, loss = 0.25137568\n",
      "Iteration 259, loss = 0.25028532\n",
      "Iteration 260, loss = 0.24715535\n",
      "Iteration 261, loss = 0.24551922\n",
      "Iteration 262, loss = 0.24311047\n",
      "Iteration 263, loss = 0.24163648\n",
      "Iteration 264, loss = 0.23873945\n",
      "Iteration 265, loss = 0.23764249\n",
      "Iteration 266, loss = 0.23524417\n",
      "Iteration 267, loss = 0.23413484\n",
      "Iteration 268, loss = 0.23105872\n",
      "Iteration 269, loss = 0.22914959\n",
      "Iteration 270, loss = 0.22697583\n",
      "Iteration 271, loss = 0.22569390\n",
      "Iteration 272, loss = 0.22306679\n",
      "Iteration 273, loss = 0.22117603\n",
      "Iteration 274, loss = 0.21899285\n",
      "Iteration 275, loss = 0.21681347\n",
      "Iteration 276, loss = 0.21514824\n",
      "Iteration 277, loss = 0.21302823\n",
      "Iteration 278, loss = 0.21198050\n",
      "Iteration 279, loss = 0.20902134\n",
      "Iteration 280, loss = 0.20687399\n",
      "Iteration 281, loss = 0.20612868\n",
      "Iteration 282, loss = 0.20339226\n",
      "Iteration 283, loss = 0.20141173\n",
      "Iteration 284, loss = 0.20007091\n",
      "Iteration 285, loss = 0.19781288\n",
      "Iteration 286, loss = 0.19590216\n",
      "Iteration 287, loss = 0.19405089\n",
      "Iteration 288, loss = 0.19255205\n",
      "Iteration 289, loss = 0.19038233\n",
      "Iteration 290, loss = 0.18855462\n",
      "Iteration 291, loss = 0.18725944\n",
      "Iteration 292, loss = 0.18453522\n",
      "Iteration 293, loss = 0.18386039\n",
      "Iteration 294, loss = 0.18185913\n",
      "Iteration 295, loss = 0.17895659\n",
      "Iteration 296, loss = 0.17786676\n",
      "Iteration 297, loss = 0.17608898\n",
      "Iteration 298, loss = 0.17478304\n",
      "Iteration 299, loss = 0.17276732\n",
      "Iteration 300, loss = 0.17168235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=300,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2508\n",
      "           1       0.85      0.93      0.89      2414\n",
      "           2       0.83      0.88      0.85      2448\n",
      "           3       0.84      0.70      0.76      2442\n",
      "\n",
      "    accuracy                           0.88      9812\n",
      "   macro avg       0.87      0.88      0.87      9812\n",
      "weighted avg       0.88      0.88      0.87      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:12, 31888.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embeddings_index = {}\n",
    "f = open('GLOVE_models/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_glove = [sent2vec(x) for x in df_bin['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_glove = np.array(features_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "X_SMOTE, y_SMOTE = smt.fit_sample(features_glove, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_labels = int_enc(np.array(labels))\n",
    "y_labels = int_enc(np.array(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_X_train, glove_X_test, glove_y_train, glove_y_test = train_test_split(features_glove, glove_labels, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_SMOTE, y_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.7300561797752809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.67      0.08      0.15       399\n",
      "           2       0.67      0.33      0.44       664\n",
      "           3       0.74      0.95      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.52      0.34      0.35      3560\n",
      "weighted avg       0.71      0.73      0.67      3560\n",
      "\n",
      "Accuracy for LinearSVC is 0.7280898876404495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.66      0.07      0.12       399\n",
      "           2       0.67      0.31      0.43       664\n",
      "           3       0.74      0.96      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.51      0.34      0.35      3560\n",
      "weighted avg       0.71      0.73      0.67      3560\n",
      "\n",
      "Accuracy for RandomForest is 0.7185393258426966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.75      0.02      0.03       399\n",
      "           2       0.76      0.19      0.30       664\n",
      "           3       0.72      0.99      0.83      2460\n",
      "\n",
      "    accuracy                           0.72      3560\n",
      "   macro avg       0.56      0.30      0.29      3560\n",
      "weighted avg       0.72      0.72      0.63      3560\n",
      "\n",
      "Accuracy for ExtraTrees is 0.7103932584269663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.83      0.01      0.02       399\n",
      "           2       0.73      0.14      0.24       664\n",
      "           3       0.71      0.99      0.83      2460\n",
      "\n",
      "    accuracy                           0.71      3560\n",
      "   macro avg       0.57      0.29      0.27      3560\n",
      "weighted avg       0.72      0.71      0.62      3560\n",
      "\n",
      "Accuracy for XGBoost is 0.7300561797752809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.70      0.04      0.07       399\n",
      "           2       0.73      0.30      0.42       664\n",
      "           3       0.73      0.97      0.83      2460\n",
      "\n",
      "    accuracy                           0.73      3560\n",
      "   macro avg       0.54      0.33      0.33      3560\n",
      "weighted avg       0.72      0.73      0.66      3560\n",
      "\n",
      "Accuracy for AdaBoost is 0.6823033707865168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        37\n",
      "           1       0.50      0.05      0.09       399\n",
      "           2       0.45      0.41      0.43       664\n",
      "           3       0.74      0.87      0.80      2460\n",
      "\n",
      "    accuracy                           0.68      3560\n",
      "   macro avg       0.42      0.33      0.33      3560\n",
      "weighted avg       0.65      0.68      0.64      3560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(glove_X_train, glove_X_test, glove_y_train, glove_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LogisticRegression is 0.5962087240114146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.66      2508\n",
      "           1       0.60      0.60      0.60      2414\n",
      "           2       0.68      0.72      0.70      2448\n",
      "           3       0.44      0.36      0.40      2442\n",
      "\n",
      "    accuracy                           0.60      9812\n",
      "   macro avg       0.59      0.60      0.59      9812\n",
      "weighted avg       0.59      0.60      0.59      9812\n",
      "\n",
      "Accuracy for LinearSVC is 0.600489196901753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      2508\n",
      "           1       0.59      0.61      0.60      2414\n",
      "           2       0.66      0.75      0.70      2448\n",
      "           3       0.48      0.30      0.37      2442\n",
      "\n",
      "    accuracy                           0.60      9812\n",
      "   macro avg       0.59      0.60      0.59      9812\n",
      "weighted avg       0.59      0.60      0.59      9812\n",
      "\n",
      "Accuracy for RandomForest is 0.9253974724826742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2508\n",
      "           1       0.96      0.95      0.96      2414\n",
      "           2       0.89      0.89      0.89      2448\n",
      "           3       0.86      0.86      0.86      2442\n",
      "\n",
      "    accuracy                           0.93      9812\n",
      "   macro avg       0.92      0.92      0.92      9812\n",
      "weighted avg       0.93      0.93      0.93      9812\n",
      "\n",
      "Accuracy for ExtraTrees is 0.9388503872808805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2508\n",
      "           1       0.98      0.96      0.97      2414\n",
      "           2       0.91      0.90      0.91      2448\n",
      "           3       0.88      0.90      0.89      2442\n",
      "\n",
      "    accuracy                           0.94      9812\n",
      "   macro avg       0.94      0.94      0.94      9812\n",
      "weighted avg       0.94      0.94      0.94      9812\n",
      "\n",
      "Accuracy for XGBoost is 0.7329800244598451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90      2508\n",
      "           1       0.71      0.70      0.71      2414\n",
      "           2       0.72      0.73      0.72      2448\n",
      "           3       0.62      0.52      0.57      2442\n",
      "\n",
      "    accuracy                           0.73      9812\n",
      "   macro avg       0.72      0.73      0.72      9812\n",
      "weighted avg       0.72      0.73      0.73      9812\n",
      "\n",
      "Accuracy for AdaBoost is 0.605279249898084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      2508\n",
      "           1       0.54      0.56      0.55      2414\n",
      "           2       0.61      0.68      0.64      2448\n",
      "           3       0.49      0.44      0.46      2442\n",
      "\n",
      "    accuracy                           0.61      9812\n",
      "   macro avg       0.60      0.60      0.60      9812\n",
      "weighted avg       0.60      0.61      0.60      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_report(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(300,100,200), max_iter=300, alpha=0.0001,learning_rate = 'constant',\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38581375\n",
      "Iteration 2, loss = 1.38290393\n",
      "Iteration 3, loss = 1.38117720\n",
      "Iteration 4, loss = 1.37944552\n",
      "Iteration 5, loss = 1.37774424\n",
      "Iteration 6, loss = 1.37593049\n",
      "Iteration 7, loss = 1.37391378\n",
      "Iteration 8, loss = 1.37169039\n",
      "Iteration 9, loss = 1.36928716\n",
      "Iteration 10, loss = 1.36657073\n",
      "Iteration 11, loss = 1.36350703\n",
      "Iteration 12, loss = 1.36002996\n",
      "Iteration 13, loss = 1.35598497\n",
      "Iteration 14, loss = 1.35136380\n",
      "Iteration 15, loss = 1.34604898\n",
      "Iteration 16, loss = 1.33988928\n",
      "Iteration 17, loss = 1.33272577\n",
      "Iteration 18, loss = 1.32444251\n",
      "Iteration 19, loss = 1.31485540\n",
      "Iteration 20, loss = 1.30381167\n",
      "Iteration 21, loss = 1.29145320\n",
      "Iteration 22, loss = 1.27781268\n",
      "Iteration 23, loss = 1.26295852\n",
      "Iteration 24, loss = 1.24746971\n",
      "Iteration 25, loss = 1.23189332\n",
      "Iteration 26, loss = 1.21647974\n",
      "Iteration 27, loss = 1.20253517\n",
      "Iteration 28, loss = 1.18948048\n",
      "Iteration 29, loss = 1.17832651\n",
      "Iteration 30, loss = 1.16851406\n",
      "Iteration 31, loss = 1.15993078\n",
      "Iteration 32, loss = 1.15251742\n",
      "Iteration 33, loss = 1.14585931\n",
      "Iteration 34, loss = 1.13964744\n",
      "Iteration 35, loss = 1.13408831\n",
      "Iteration 36, loss = 1.12867721\n",
      "Iteration 37, loss = 1.12406115\n",
      "Iteration 38, loss = 1.11904949\n",
      "Iteration 39, loss = 1.11431486\n",
      "Iteration 40, loss = 1.10951270\n",
      "Iteration 41, loss = 1.10493777\n",
      "Iteration 42, loss = 1.10030164\n",
      "Iteration 43, loss = 1.09585133\n",
      "Iteration 44, loss = 1.09117587\n",
      "Iteration 45, loss = 1.08686750\n",
      "Iteration 46, loss = 1.08201774\n",
      "Iteration 47, loss = 1.07736557\n",
      "Iteration 48, loss = 1.07233954\n",
      "Iteration 49, loss = 1.06759165\n",
      "Iteration 50, loss = 1.06310113\n",
      "Iteration 51, loss = 1.05803566\n",
      "Iteration 52, loss = 1.05284866\n",
      "Iteration 53, loss = 1.04778487\n",
      "Iteration 54, loss = 1.04293984\n",
      "Iteration 55, loss = 1.03755194\n",
      "Iteration 56, loss = 1.03272943\n",
      "Iteration 57, loss = 1.02766345\n",
      "Iteration 58, loss = 1.02254290\n",
      "Iteration 59, loss = 1.01778826\n",
      "Iteration 60, loss = 1.01272692\n",
      "Iteration 61, loss = 1.00746827\n",
      "Iteration 62, loss = 1.00259516\n",
      "Iteration 63, loss = 0.99762848\n",
      "Iteration 64, loss = 0.99284807\n",
      "Iteration 65, loss = 0.98791027\n",
      "Iteration 66, loss = 0.98289415\n",
      "Iteration 67, loss = 0.97793433\n",
      "Iteration 68, loss = 0.97310880\n",
      "Iteration 69, loss = 0.96844220\n",
      "Iteration 70, loss = 0.96390062\n",
      "Iteration 71, loss = 0.95835268\n",
      "Iteration 72, loss = 0.95341125\n",
      "Iteration 73, loss = 0.94901743\n",
      "Iteration 74, loss = 0.94380121\n",
      "Iteration 75, loss = 0.93874452\n",
      "Iteration 76, loss = 0.93381234\n",
      "Iteration 77, loss = 0.92955924\n",
      "Iteration 78, loss = 0.92390885\n",
      "Iteration 79, loss = 0.91942358\n",
      "Iteration 80, loss = 0.91403396\n",
      "Iteration 81, loss = 0.90924152\n",
      "Iteration 82, loss = 0.90422280\n",
      "Iteration 83, loss = 0.89953607\n",
      "Iteration 84, loss = 0.89431610\n",
      "Iteration 85, loss = 0.88937430\n",
      "Iteration 86, loss = 0.88380988\n",
      "Iteration 87, loss = 0.87929267\n",
      "Iteration 88, loss = 0.87374430\n",
      "Iteration 89, loss = 0.86837652\n",
      "Iteration 90, loss = 0.86287391\n",
      "Iteration 91, loss = 0.85783871\n",
      "Iteration 92, loss = 0.85270745\n",
      "Iteration 93, loss = 0.84739960\n",
      "Iteration 94, loss = 0.84181119\n",
      "Iteration 95, loss = 0.83628556\n",
      "Iteration 96, loss = 0.83117204\n",
      "Iteration 97, loss = 0.82569928\n",
      "Iteration 98, loss = 0.81982516\n",
      "Iteration 99, loss = 0.81463895\n",
      "Iteration 100, loss = 0.80839735\n",
      "Iteration 101, loss = 0.80268422\n",
      "Iteration 102, loss = 0.79701432\n",
      "Iteration 103, loss = 0.79151169\n",
      "Iteration 104, loss = 0.78602696\n",
      "Iteration 105, loss = 0.77986600\n",
      "Iteration 106, loss = 0.77447441\n",
      "Iteration 107, loss = 0.76879811\n",
      "Iteration 108, loss = 0.76273685\n",
      "Iteration 109, loss = 0.75727439\n",
      "Iteration 110, loss = 0.75139612\n",
      "Iteration 111, loss = 0.74560194\n",
      "Iteration 112, loss = 0.73996275\n",
      "Iteration 113, loss = 0.73451707\n",
      "Iteration 114, loss = 0.72890691\n",
      "Iteration 115, loss = 0.72339921\n",
      "Iteration 116, loss = 0.71792438\n",
      "Iteration 117, loss = 0.71192066\n",
      "Iteration 118, loss = 0.70712987\n",
      "Iteration 119, loss = 0.70149510\n",
      "Iteration 120, loss = 0.69618942\n",
      "Iteration 121, loss = 0.69100057\n",
      "Iteration 122, loss = 0.68517788\n",
      "Iteration 123, loss = 0.68026395\n",
      "Iteration 124, loss = 0.67507177\n",
      "Iteration 125, loss = 0.67023406\n",
      "Iteration 126, loss = 0.66572941\n",
      "Iteration 127, loss = 0.66091067\n",
      "Iteration 128, loss = 0.65642296\n",
      "Iteration 129, loss = 0.65163721\n",
      "Iteration 130, loss = 0.64742784\n",
      "Iteration 131, loss = 0.64286120\n",
      "Iteration 132, loss = 0.63888434\n",
      "Iteration 133, loss = 0.63499746\n",
      "Iteration 134, loss = 0.63038176\n",
      "Iteration 135, loss = 0.62681504\n",
      "Iteration 136, loss = 0.62297727\n",
      "Iteration 137, loss = 0.61943864\n",
      "Iteration 138, loss = 0.61575664\n",
      "Iteration 139, loss = 0.61253779\n",
      "Iteration 140, loss = 0.60887543\n",
      "Iteration 141, loss = 0.60588856\n",
      "Iteration 142, loss = 0.60204802\n",
      "Iteration 143, loss = 0.59893142\n",
      "Iteration 144, loss = 0.59624146\n",
      "Iteration 145, loss = 0.59291492\n",
      "Iteration 146, loss = 0.58998195\n",
      "Iteration 147, loss = 0.58710588\n",
      "Iteration 148, loss = 0.58463687\n",
      "Iteration 149, loss = 0.58220393\n",
      "Iteration 150, loss = 0.57926989\n",
      "Iteration 151, loss = 0.57669055\n",
      "Iteration 152, loss = 0.57442456\n",
      "Iteration 153, loss = 0.57170269\n",
      "Iteration 154, loss = 0.56943888\n",
      "Iteration 155, loss = 0.56697908\n",
      "Iteration 156, loss = 0.56519869\n",
      "Iteration 157, loss = 0.56244727\n",
      "Iteration 158, loss = 0.56021583\n",
      "Iteration 159, loss = 0.55827294\n",
      "Iteration 160, loss = 0.55618606\n",
      "Iteration 161, loss = 0.55389381\n",
      "Iteration 162, loss = 0.55206455\n",
      "Iteration 163, loss = 0.55006814\n",
      "Iteration 164, loss = 0.54777695\n",
      "Iteration 165, loss = 0.54588653\n",
      "Iteration 166, loss = 0.54420196\n",
      "Iteration 167, loss = 0.54225788\n",
      "Iteration 168, loss = 0.54011144\n",
      "Iteration 169, loss = 0.53840794\n",
      "Iteration 170, loss = 0.53665233\n",
      "Iteration 171, loss = 0.53477284\n",
      "Iteration 172, loss = 0.53292271\n",
      "Iteration 173, loss = 0.53092800\n",
      "Iteration 174, loss = 0.52900459\n",
      "Iteration 175, loss = 0.52733033\n",
      "Iteration 176, loss = 0.52564153\n",
      "Iteration 177, loss = 0.52386861\n",
      "Iteration 178, loss = 0.52185943\n",
      "Iteration 179, loss = 0.52056152\n",
      "Iteration 180, loss = 0.51855919\n",
      "Iteration 181, loss = 0.51718219\n",
      "Iteration 182, loss = 0.51531172\n",
      "Iteration 183, loss = 0.51332942\n",
      "Iteration 184, loss = 0.51174387\n",
      "Iteration 185, loss = 0.50985922\n",
      "Iteration 186, loss = 0.50837527\n",
      "Iteration 187, loss = 0.50634420\n",
      "Iteration 188, loss = 0.50464542\n",
      "Iteration 189, loss = 0.50285654\n",
      "Iteration 190, loss = 0.50114628\n",
      "Iteration 191, loss = 0.49980020\n",
      "Iteration 192, loss = 0.49793006\n",
      "Iteration 193, loss = 0.49603390\n",
      "Iteration 194, loss = 0.49434072\n",
      "Iteration 195, loss = 0.49250521\n",
      "Iteration 196, loss = 0.49076505\n",
      "Iteration 197, loss = 0.48881986\n",
      "Iteration 198, loss = 0.48689668\n",
      "Iteration 199, loss = 0.48519678\n",
      "Iteration 200, loss = 0.48343463\n",
      "Iteration 201, loss = 0.48222312\n",
      "Iteration 202, loss = 0.47968799\n",
      "Iteration 203, loss = 0.47826155\n",
      "Iteration 204, loss = 0.47670831\n",
      "Iteration 205, loss = 0.47445871\n",
      "Iteration 206, loss = 0.47274775\n",
      "Iteration 207, loss = 0.47075841\n",
      "Iteration 208, loss = 0.46920881\n",
      "Iteration 209, loss = 0.46705002\n",
      "Iteration 210, loss = 0.46514766\n",
      "Iteration 211, loss = 0.46344196\n",
      "Iteration 212, loss = 0.46152627\n",
      "Iteration 213, loss = 0.46034137\n",
      "Iteration 214, loss = 0.45802957\n",
      "Iteration 215, loss = 0.45605868\n",
      "Iteration 216, loss = 0.45391160\n",
      "Iteration 217, loss = 0.45211786\n",
      "Iteration 218, loss = 0.45034066\n",
      "Iteration 219, loss = 0.44815648\n",
      "Iteration 220, loss = 0.44709026\n",
      "Iteration 221, loss = 0.44422835\n",
      "Iteration 222, loss = 0.44324504\n",
      "Iteration 223, loss = 0.44106635\n",
      "Iteration 224, loss = 0.43918724\n",
      "Iteration 225, loss = 0.43789865\n",
      "Iteration 226, loss = 0.43542811\n",
      "Iteration 227, loss = 0.43321958\n",
      "Iteration 228, loss = 0.43086377\n",
      "Iteration 229, loss = 0.42961882\n",
      "Iteration 230, loss = 0.42759198\n",
      "Iteration 231, loss = 0.42549389\n",
      "Iteration 232, loss = 0.42337054\n",
      "Iteration 233, loss = 0.42150420\n",
      "Iteration 234, loss = 0.42004351\n",
      "Iteration 235, loss = 0.41787382\n",
      "Iteration 236, loss = 0.41572410\n",
      "Iteration 237, loss = 0.41307954\n",
      "Iteration 238, loss = 0.41298901\n",
      "Iteration 239, loss = 0.41001253\n",
      "Iteration 240, loss = 0.40753391\n",
      "Iteration 241, loss = 0.40628992\n",
      "Iteration 242, loss = 0.40438845\n",
      "Iteration 243, loss = 0.40199528\n",
      "Iteration 244, loss = 0.40039307\n",
      "Iteration 245, loss = 0.39844415\n",
      "Iteration 246, loss = 0.39685531\n",
      "Iteration 247, loss = 0.39481421\n",
      "Iteration 248, loss = 0.39237482\n",
      "Iteration 249, loss = 0.39049847\n",
      "Iteration 250, loss = 0.38828067\n",
      "Iteration 251, loss = 0.38630638\n",
      "Iteration 252, loss = 0.38449050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.38236496\n",
      "Iteration 254, loss = 0.38077340\n",
      "Iteration 255, loss = 0.37849685\n",
      "Iteration 256, loss = 0.37684210\n",
      "Iteration 257, loss = 0.37496497\n",
      "Iteration 258, loss = 0.37331555\n",
      "Iteration 259, loss = 0.37083588\n",
      "Iteration 260, loss = 0.36883356\n",
      "Iteration 261, loss = 0.36764453\n",
      "Iteration 262, loss = 0.36520963\n",
      "Iteration 263, loss = 0.36299954\n",
      "Iteration 264, loss = 0.36120347\n",
      "Iteration 265, loss = 0.35954837\n",
      "Iteration 266, loss = 0.35793632\n",
      "Iteration 267, loss = 0.35517984\n",
      "Iteration 268, loss = 0.35384704\n",
      "Iteration 269, loss = 0.35206347\n",
      "Iteration 270, loss = 0.34956893\n",
      "Iteration 271, loss = 0.34874851\n",
      "Iteration 272, loss = 0.34639500\n",
      "Iteration 273, loss = 0.34448750\n",
      "Iteration 274, loss = 0.34334178\n",
      "Iteration 275, loss = 0.34126347\n",
      "Iteration 276, loss = 0.33952872\n",
      "Iteration 277, loss = 0.33697084\n",
      "Iteration 278, loss = 0.33503079\n",
      "Iteration 279, loss = 0.33391990\n",
      "Iteration 280, loss = 0.33120940\n",
      "Iteration 281, loss = 0.32973022\n",
      "Iteration 282, loss = 0.32775333\n",
      "Iteration 283, loss = 0.32570650\n",
      "Iteration 284, loss = 0.32380165\n",
      "Iteration 285, loss = 0.32382756\n",
      "Iteration 286, loss = 0.32180113\n",
      "Iteration 287, loss = 0.31856890\n",
      "Iteration 288, loss = 0.31765514\n",
      "Iteration 289, loss = 0.31512305\n",
      "Iteration 290, loss = 0.31317937\n",
      "Iteration 291, loss = 0.31143476\n",
      "Iteration 292, loss = 0.30994314\n",
      "Iteration 293, loss = 0.30831611\n",
      "Iteration 294, loss = 0.30658581\n",
      "Iteration 295, loss = 0.30598496\n",
      "Iteration 296, loss = 0.30349008\n",
      "Iteration 297, loss = 0.30192226\n",
      "Iteration 298, loss = 0.30011191\n",
      "Iteration 299, loss = 0.29786622\n",
      "Iteration 300, loss = 0.29632427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300, 100, 200), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=300,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      2508\n",
      "           1       0.80      0.88      0.84      2414\n",
      "           2       0.79      0.81      0.80      2448\n",
      "           3       0.76      0.65      0.70      2442\n",
      "\n",
      "    accuracy                           0.84      9812\n",
      "   macro avg       0.83      0.84      0.83      9812\n",
      "weighted avg       0.83      0.84      0.83      9812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mlp.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
